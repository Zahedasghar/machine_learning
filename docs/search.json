[
  {
    "objectID": "psx.html",
    "href": "psx.html",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "",
    "text": "In this tutorial, we will explore several machine learning classification models to predict the direction of stock movement for the Pakistan Stock Exchange (PSX) using historical data. We will use models such as Logistic Regression, Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), K-Nearest Neighbors (KNN), and Decision Trees (CART).\nThe goal is to understand some machine learning techniques and how they can be applied to real-world financial data. We will use historical stock data from the PSX. We can understand that predicting movements in stock prices is a challenging task due to the complex nature of financial markets. This is a simplified example to demonstrate how machine learning models can be used in real financial applications. We have created confusion matrix and calculated accuracy, precision, recall, and F1 score to evaluate the performance of each model. We predict whether the stock price will go “Up” or “Down” based on historical changes in stock price and trading volume."
  },
  {
    "objectID": "psx.html#introduction",
    "href": "psx.html#introduction",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "",
    "text": "In this tutorial, we will explore several machine learning classification models to predict the direction of stock movement for the Pakistan Stock Exchange (PSX) using historical data. We will use models such as Logistic Regression, Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), K-Nearest Neighbors (KNN), and Decision Trees (CART).\nThe goal is to understand some machine learning techniques and how they can be applied to real-world financial data. We will use historical stock data from the PSX. We can understand that predicting movements in stock prices is a challenging task due to the complex nature of financial markets. This is a simplified example to demonstrate how machine learning models can be used in real financial applications. We have created confusion matrix and calculated accuracy, precision, recall, and F1 score to evaluate the performance of each model. We predict whether the stock price will go “Up” or “Down” based on historical changes in stock price and trading volume."
  },
  {
    "objectID": "psx.html#loading-and-preparing-the-data",
    "href": "psx.html#loading-and-preparing-the-data",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "1. Loading and Preparing the Data",
    "text": "1. Loading and Preparing the Data\nThe first step is to load the PSX data, clean it, and perform necessary transformations for feature engineering.\n\nlibrary(caret)      # For model evaluation and cross-validation\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(rpart)      # For Decision Trees\nlibrary(tidyverse)  # For data manipulation and visualization\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ purrr::lift()   masks caret::lift()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(Metrics)    # For additional evaluation metrics\n\nWarning: package 'Metrics' was built under R version 4.4.2\n\n\n\nAttaching package: 'Metrics'\n\nThe following objects are masked from 'package:caret':\n\n    precision, recall\n\nlibrary(janitor)    # For cleaning column names\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(class)    # For KNN"
  },
  {
    "objectID": "psx.html#load-data",
    "href": "psx.html#load-data",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "Load data",
    "text": "Load data\nWe have psx data in csv format. We will load the data and inspect its structure.\n\npsx &lt;- read_csv(\"data/psx.csv\")\n\nRows: 1827 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Date, Vol., Change %\ndbl (4): Price, Open, High, Low\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Inspect the data structure (rows, columns, and types)\ndim(psx)\n\n[1] 1827    7\n\nglimpse(psx)\n\nRows: 1,827\nColumns: 7\n$ Date       &lt;chr&gt; \"11/15/2024\", \"11/14/2024\", \"11/13/2024\", \"11/12/2024\", \"11…\n$ Price      &lt;dbl&gt; 17.42, 17.19, 16.78, 16.86, 17.02, 17.22, 17.14, 17.31, 16.…\n$ Open       &lt;dbl&gt; 17.40, 16.70, 16.95, 17.00, 17.46, 17.14, 17.24, 17.24, 17.…\n$ High       &lt;dbl&gt; 17.60, 17.60, 16.95, 17.20, 17.46, 17.49, 17.60, 17.50, 17.…\n$ Low        &lt;dbl&gt; 16.90, 16.55, 16.50, 16.70, 16.97, 17.01, 17.00, 16.60, 16.…\n$ Vol.       &lt;chr&gt; \"896.83K\", \"1.01M\", \"239.44K\", \"179.95K\", \"364.92K\", \"208.0…\n$ `Change %` &lt;chr&gt; \"1.34%\", \"2.44%\", \"-0.47%\", \"-0.94%\", \"-1.16%\", \"0.47%\", \"-…"
  },
  {
    "objectID": "psx.html#data-cleaning-and-feature-engineering",
    "href": "psx.html#data-cleaning-and-feature-engineering",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "Data cleaning and feature engineering",
    "text": "Data cleaning and feature engineering\nWe will clean the data, convert columns to appropriate types, create new features, and handle missing values.\n\n# Convert the `Date` column to a proper Date format\npsx &lt;- psx %&gt;% \n  mutate(date = as.Date(Date, format = \"%m/%d/%Y\")) |&gt; dplyr::select(-Date)\n# Clean column names to make them consistent and readable\npsx &lt;- psx %&gt;% clean_names()\n\n# Convert `change_percent` to numeric and create a new column `direction` for the stock movement (Up/Down)\npsx &lt;- psx %&gt;% \n  mutate(\n    change_percent = as.numeric(str_remove(change_percent, \"%\")),\n    direction = if_else(change_percent &gt; 0, \"Up\", \"Down\")\n  )\n\n# Arrange data by ascending date to maintain temporal order\npsx &lt;- psx %&gt;% arrange(date)\n\n# Create lag variables for `change_percent` (lags of 1 to 5 days)\npsx &lt;- psx %&gt;% \n  mutate(\n    lag1 = lag(change_percent, 1),\n    lag2 = lag(change_percent, 2),\n    lag3 = lag(change_percent, 3),\n    lag4 = lag(change_percent, 4),\n    lag5 = lag(change_percent, 5)\n  )\n\n# Convert `vol` (volume) to numeric, handling the M and K suffixes\npsx &lt;- psx %&gt;% mutate(vol = as.numeric(str_replace_all(vol, c(\"M\" = \"e6\", \"K\" = \"e3\"))))\n\n# Drop rows with missing values (NA)\npsx_clean &lt;- psx %&gt;% drop_na()\n\n# Convert `direction` to numeric for binary classification (1 for \"Up\", 0 for \"Down\")\npsx_clean &lt;- psx_clean %&gt;%\n  mutate(direction = if_else(direction == \"Up\", 1, 0))\n\nThere are 1822 rows in the cleaned dataset after data cleaning and feature engineering.\nExplanation:\n\nWe loaded the dataset and cleaned column names.\nWe transformed the change_percent column from character to numeric.\nWe created a direction column to classify stock movements as “Up” or “Down”.\nLag features are created to provide past data (1-day lag to 5-day lag) as predictors for stock movement.\nWe converted the volume (vol) column to numeric format for use in the model.\nRows with missing values were dropped, and the target variable direction was converted to binary numeric values for classification."
  },
  {
    "objectID": "psx.html#logistic-regression-glm",
    "href": "psx.html#logistic-regression-glm",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "2. Logistic Regression (GLM)",
    "text": "2. Logistic Regression (GLM)\nLogistic regression will model the probability that the stock will go “Up” based on the lag features and volume.\n\n# Fit logistic regression model\n# glm_fit &lt;- glm(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol, \n#                data = psx_clean, family = binomial)\n# \n# # Model summary\n# summary(glm_fit)\n# \n# # Add predicted probabilities to the dataset\n# psx_clean &lt;- psx_clean %&gt;%\n#   mutate(\n#     glm_probs = predict(glm_fit, type = \"response\"),\n#     glm_pred = if_else(glm_probs &gt; 0.5, 1, 0)\n#   )\n# \n# # Evaluate logistic regression performance using confusion matrix and accuracy\n# confusion_matrix &lt;- table(glm_pred = psx_clean$glm_pred, actual = psx_clean$direction)\n# accuracy &lt;- mean(psx_clean$glm_pred == psx_clean$direction)\n\n# Display results\n#print(confusion_matrix)\n#print(paste(\"Overall Accuracy:\", round(accuracy, 3)))\n\nExplanation: 1. The logistic regression model is fit with the lag features and volume as predictors. 2. Predictions are made for each observation using predict(), and the predicted probabilities are added to the dataset. 3. We then classify the probability as “Up” or “Down” based on a threshold of 0.5. 4. The performance is evaluated by comparing predicted values against actual values using a confusion matrix and accuracy."
  },
  {
    "objectID": "psx.html#train-test-split",
    "href": "psx.html#train-test-split",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "3. Train-Test Split",
    "text": "3. Train-Test Split\nWe split the data into training and testing datasets to evaluate the performance of our models.\n\n# Train-test split based on the date\ntrain_data &lt;- psx_clean %&gt;%\n  filter(date &lt; as.Date(\"2024-01-01\"))\n\ntest_data &lt;- psx_clean %&gt;%\n  filter(date &gt;= as.Date(\"2024-01-01\"))\n# \n# # Ensure direction is numeric (0 for Down, 1 for Up)\n# train_data &lt;- train_data %&gt;%\n#   mutate(direction = as.factor(direction))\n# \n# test_data &lt;- test_data %&gt;%\n#   mutate(direction = as.factor(direction))\n# \n# # Ensure consistent factor levels for direction in both training and test data\n# train_data$direction &lt;- factor(train_data$direction, levels = c(0, 1))  # 0 = Down, 1 = Up\n# test_data$direction &lt;- factor(test_data$direction, levels = c(0, 1))"
  },
  {
    "objectID": "psx.html#linear-discriminant-analysis-lda",
    "href": "psx.html#linear-discriminant-analysis-lda",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "Linear Discriminant Analysis (LDA)",
    "text": "Linear Discriminant Analysis (LDA)\n\nlda_fit &lt;- lda(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol,\n               data = train_data)\n\n# Predict on test data\n\nlda_pred &lt;- predict(lda_fit, newdata = test_data)\n\n# Evaluate LDA model\n\nlda_accuracy &lt;- mean(lda_pred$class == test_data$direction)\n\nExplanation: 1. We split the data based on the date (all data before January 1, 2024 for training, and the rest for testing). 2. The target variable direction is converted into a factor to comply with classification model requirements."
  },
  {
    "objectID": "psx.html#linear-discriminant-analysis-lda-1",
    "href": "psx.html#linear-discriminant-analysis-lda-1",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "4. Linear Discriminant Analysis (LDA)",
    "text": "4. Linear Discriminant Analysis (LDA)\nLDA is another classification technique that assumes normality in the predictor variables and tries to separate the classes (Up/Down) by maximizing the ratio of between-class variance to within-class variance.\nError in Ops.factor(lda_pred, test_data$direction) : level sets of factors are different &gt;\nExplanation: 1. We fit the LDA model using the training data. 2. Predictions are made on the test set using the fitted model. 3. The model accuracy is evaluated by comparing predicted and actual values."
  },
  {
    "objectID": "psx.html#quadratic-discriminant-analysis-qda",
    "href": "psx.html#quadratic-discriminant-analysis-qda",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "5. Quadratic Discriminant Analysis (QDA)",
    "text": "5. Quadratic Discriminant Analysis (QDA)\nQDA is similar to LDA but allows for each class to have its own covariance matrix, making it more flexible when the data distribution is not the same across classes.\n\n# Fit QDA model\nqda_fit &lt;- qda(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol, data = train_data)\n\n# Predict on test data\nqda_pred &lt;- predict(qda_fit, newdata = test_data)$class\n\n# Evaluate QDA model performance\nqda_accuracy &lt;- mean(qda_pred == test_data$direction)\n\n# Display QDA results\nprint(paste(\"QDA Accuracy:\", round(qda_accuracy, 3)))\n\n[1] \"QDA Accuracy: 0.605\"\n\n\nExplanation: 1. We fit the QDA model using the training data. 2. Predictions are made on the test set, and the model’s accuracy is evaluated."
  },
  {
    "objectID": "psx.html#k-nearest-neighbors-knn",
    "href": "psx.html#k-nearest-neighbors-knn",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "6. K-Nearest Neighbors (KNN)",
    "text": "6. K-Nearest Neighbors (KNN)\nKNN is a non-parametric method that assigns the class of an observation based on the majority class of its k nearest neighbors in the feature space.\n\n# Fit KNN model\n\nknn_fit &lt;- knn(train = train_data[, c(\"lag1\", \"lag2\", \"lag3\", \"lag4\", \"lag5\", \"vol\")],\n               test = test_data[, c(\"lag1\", \"lag2\", \"lag3\", \"lag4\", \"lag5\", \"vol\")],\n               cl = train_data$direction,\n               k = 5)\n\n# Evaluate KNN model\n\nknn_accuracy &lt;- mean(knn_fit == test_data$direction)\n\nExplanation: 1. Data is normalized to ensure all features have the same scale for KNN. 2. We fit the KNN model with k = 5 and evaluate its accuracy."
  },
  {
    "objectID": "psx.html#decision-trees-cart",
    "href": "psx.html#decision-trees-cart",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "7. Decision Trees (CART)",
    "text": "7. Decision Trees (CART)\nThe Decision Tree algorithm creates a tree-like model of decisions and their possible consequences.\n\n# Fit Decision Tree model (CART)\ndt_fit &lt;- rpart(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol,\n                 data = train_data,\n                 method = \"class\")\n\n# Predict on test data\n\ndt_pred &lt;- predict(dt_fit, newdata = test_data, type = \"class\")\n\n\n# Evaluate Decision Tree model\n\ndt_accuracy &lt;- mean(dt_pred == test_data$direction)\n\nExplanation: 1. We fit the Decision Tree (CART) model using the training data. 2. Predictions are made on the test set, and the accuracy is calculated.\n\n# Train logistic regression on train data\nglm_train &lt;- glm(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol,\n                 data = train_data, family = binomial)\n\n# Predict on train data\ntrain_data &lt;- train_data %&gt;%\n  mutate(\n    glm_probs = predict(glm_train, newdata = train_data, type = \"response\"),\n    glm_pred = if_else(glm_probs &gt; 0.5, \"Up\", \"Down\")\n  )\n\n# Calculate confusion matrix and accuracy for train data\ntrain_confusion_matrix &lt;- table(glm_pred = train_data$glm_pred, actual = train_data$direction)\nglm_train_accuracy &lt;- mean(train_data$glm_pred == train_data$direction)\n\n# Predict on test data\ntest_data &lt;- test_data %&gt;%\n  mutate(\n    glm_probs = predict(glm_train, newdata = test_data, type = \"response\"),\n    glm_pred = if_else(glm_probs &gt; 0.5, \"Up\", \"Down\")\n  )\n\n# Calculate confusion matrix and accuracy for test data\ntest_confusion_matrix &lt;- table(glm_pred = test_data$glm_pred, actual = test_data$direction)\nglm_test_accuracy &lt;- mean(test_data$glm_pred == test_data$direction)\n\n# Display results\nlist(\n  train_confusion_matrix = train_confusion_matrix,\n  glm_train_accuracy = glm_train_accuracy,\n  test_confusion_matrix = test_confusion_matrix,\n  glm_test_accuracy = glm_test_accuracy\n)\n\n$train_confusion_matrix\n        actual\nglm_pred   0   1\n    Down 756 510\n    Up   119 222\n\n$glm_train_accuracy\n[1] 0\n\n$test_confusion_matrix\n        actual\nglm_pred  0  1\n    Down 69 37\n    Up   46 63\n\n$glm_test_accuracy\n[1] 0\n\n\n\n# Logistic regression model\nglm_fit &lt;- glm(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol,\n               data = psx_clean, family = binomial)\n\n# Model summary\nsummary(glm_fit)\n\n\nCall:\nglm(formula = direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + \n    vol, family = binomial, data = psx_clean)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.237e-01  6.095e-02  -8.592  &lt; 2e-16 ***\nlag1        -6.414e-02  1.678e-02  -3.822 0.000132 ***\nlag2        -1.150e-02  1.549e-02  -0.742 0.458036    \nlag3        -4.223e-03  1.573e-02  -0.268 0.788338    \nlag4        -2.171e-02  1.515e-02  -1.433 0.151943    \nlag5        -4.423e-02  1.526e-02  -2.898 0.003758 ** \nvol          4.184e-07  4.901e-08   8.536  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2512.1  on 1821  degrees of freedom\nResidual deviance: 2396.3  on 1815  degrees of freedom\nAIC: 2410.3\n\nNumber of Fisher Scoring iterations: 4\n\n# Predict probabilities and classify directions\npsx_clean1 &lt;- psx_clean %&gt;%\n  mutate(\n    glm_probs = predict(glm_fit, type = \"response\"),\n    glm_pred = if_else(glm_probs &gt; 0.5, \"Up\", \"Down\")\n  )\n\n# Evaluate model accuracy\nconfusion_matrix &lt;- table(glm_pred = psx_clean1$glm_pred, actual = psx_clean1$direction)\naccuracy &lt;- mean(psx_clean1$glm_pred == psx_clean1$direction)\n\n# Split data into train and test sets (based on date &lt; 2024)\ntrain &lt;- psx_clean1 %&gt;%\n  filter(date &lt; as.Date(\"2024-01-01\"))\n\ntest &lt;- psx_clean1 %&gt;%\n  filter(date &gt;= as.Date(\"2024-01-01\"))\n\n# Train logistic regression on train data\nglm_train &lt;- glm(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol,\n                 data = train, family = binomial)\n\n# Predict on test data\ntest &lt;- test %&gt;%\n  mutate(\n    glm_probs = predict(glm_train, newdata = ., type = \"response\"),\n    glm_pred = if_else(glm_probs &gt; 0.5, \"Up\", \"Down\")\n  )\n\n# Convert predictions and true labels to factors with the same levels\ntest$glm_pred &lt;- factor(test$glm_pred, levels = c(\"Down\", \"Up\"))\ntest$direction &lt;- factor(test$direction, levels = c(\"Down\", \"Up\"))\n\n# Evaluate test accuracy\nconfusion_matrix_test &lt;- table(glm_pred = test$glm_pred, actual = test$direction)\ntest_accuracy &lt;- mean(test$glm_pred == test$direction)\n\n# Output results\nlist(\n  train_accuracy = accuracy,\n  test_accuracy = test_accuracy,\n  train_confusion_matrix = confusion_matrix,\n  test_confusion_matrix = confusion_matrix_test\n)\n\n$train_accuracy\n[1] 0\n\n$test_accuracy\n[1] NA\n\n$train_confusion_matrix\n        actual\nglm_pred   0   1\n    Down 848 562\n    Up   142 270\n\n$test_confusion_matrix\n        actual\nglm_pred Down Up\n    Down    0  0\n    Up      0  0\n\n# Confusion Matrix and Accuracy using caret\nglm_cm &lt;- confusionMatrix(test$glm_pred, test$direction)\n\n# Extract Accuracy\nglm_accuracy &lt;- glm_cm$overall['Accuracy']\n\n\n# Confusion Matrix and Accuracy using caret\nglm_cm &lt;- confusionMatrix(test$glm_pred, test$direction)\n\n# Extract Accuracy\nglm_accuracy &lt;- glm_cm$overall['Accuracy']\n\nglm_accuracy\n\nAccuracy \n     NaN"
  },
  {
    "objectID": "psx.html#model-evaluation",
    "href": "psx.html#model-evaluation",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "8. Model Evaluation",
    "text": "8. Model Evaluation\nWe evaluate the performance of each model using accuracy, which is the proportion of correct predictions over the total number of predictions."
  },
  {
    "objectID": "psx.html#output-results",
    "href": "psx.html#output-results",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "Output results",
    "text": "Output results\n\nlist( glm_accuracy = glm_accuracy,\n  lda_accuracy = lda_accuracy,\n  qda_accuracy = qda_accuracy,\n  knn_accuracy = knn_accuracy,\n  dt_accuracy = dt_accuracy\n)\n\n$glm_accuracy\nAccuracy \n     NaN \n\n$lda_accuracy\n[1] 0.6325581\n\n$qda_accuracy\n[1] 0.6046512\n\n$knn_accuracy\n[1] 0.5488372\n\n$dt_accuracy\n[1] 0.5813953"
  },
  {
    "objectID": "psx.html#conclusion",
    "href": "psx.html#conclusion",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "Conclusion",
    "text": "Conclusion\nWe have successfully implemented various machine learning classification models to predict stock movement direction. The models were evaluated using accuracy, and further improvements can be made by tuning hyperparameters, adding more features, or using ensemble methods.\nEach model has its strengths and weaknesses, and the results from this tutorial can be used to guide further research or model enhancement."
  },
  {
    "objectID": "gapminder_map_fn.html",
    "href": "gapminder_map_fn.html",
    "title": "gapminder_map_function",
    "section": "",
    "text": "The provided code chunks are in R and make use of tidyverse packages to work with the gapminder dataset and apply a linear model (lm) to analyze life expectancy as a function of year for each continent.\nHere’s a breakdown of what the code does:\n\nLoad necessary libraries:\n\n\n   library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nLoad and clean the gapminder data:\n\n\n   dat_gapminder &lt;- gapminder::gapminder |&gt;\n     janitor::clean_names()\n\n\nThis creates a data frame dat_gapminder from the gapminder dataset and standardizes column names using janitor::clean_names().\n\n\nApply a linear model using map:\n\n\n   dat_gapminder |&gt;\n     nest(.by = continent) |&gt;\n     mutate(\n       lin_mod = map(\n         data,\n         ~ lm(life_exp ~ year, data = .x)\n       )\n     )\n\n# A tibble: 5 × 3\n  continent data               lin_mod\n  &lt;fct&gt;     &lt;list&gt;             &lt;list&gt; \n1 Asia      &lt;tibble [396 × 5]&gt; &lt;lm&gt;   \n2 Europe    &lt;tibble [360 × 5]&gt; &lt;lm&gt;   \n3 Africa    &lt;tibble [624 × 5]&gt; &lt;lm&gt;   \n4 Americas  &lt;tibble [300 × 5]&gt; &lt;lm&gt;   \n5 Oceania   &lt;tibble [24 × 5]&gt;  &lt;lm&gt;   \n\n\n\nnest(.by = continent): Groups and nests the data by the continent column, creating a list-column with data frames for each continent.\nmutate() creates a new column, lin_mod, where the map() function is applied.\nmap() iterates over each nested data frame (data) and applies the lm(life_exp ~ year, data = .x) function, fitting a linear model for life expectancy as a function of year.\n\nThis process results in a data frame with a column that stores the fitted linear model for each continent. To extract coefficients (intercept and slope) from these models, an additional step using map() combined with broom::tidy() or coef() can be added:\n\ndat_gapminder |&gt;\n  nest(.by = continent) |&gt;\n  mutate(\n    lin_mod = map(\n      data,\n      \\(x) lm(life_exp ~ year, data = x)\n    ),\n    coeffs = map(\n      lin_mod,\n      coefficients\n    ),\n    slope = map_dbl(coeffs, \\(x) x[2]),\n    intercept = map_dbl(coeffs, \\(x) x[1])\n  )\n\n# A tibble: 5 × 6\n  continent data               lin_mod coeffs    slope intercept\n  &lt;fct&gt;     &lt;list&gt;             &lt;list&gt;  &lt;list&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 Asia      &lt;tibble [396 × 5]&gt; &lt;lm&gt;    &lt;dbl [2]&gt; 0.453     -837.\n2 Europe    &lt;tibble [360 × 5]&gt; &lt;lm&gt;    &lt;dbl [2]&gt; 0.222     -367.\n3 Africa    &lt;tibble [624 × 5]&gt; &lt;lm&gt;    &lt;dbl [2]&gt; 0.290     -524.\n4 Americas  &lt;tibble [300 × 5]&gt; &lt;lm&gt;    &lt;dbl [2]&gt; 0.368     -663.\n5 Oceania   &lt;tibble [24 × 5]&gt;  &lt;lm&gt;    &lt;dbl [2]&gt; 0.210     -342."
  },
  {
    "objectID": "Ch1.html",
    "href": "Ch1.html",
    "title": "ISLR Ch.1",
    "section": "",
    "text": "library(dplyr) library(ggplot2) install.packages(“ISLR”) library(ISLR)"
  },
  {
    "objectID": "ch02_10.html",
    "href": "ch02_10.html",
    "title": "Chapter 2 - Exercise 10",
    "section": "",
    "text": "This exercise involves the Boston housing data set. (a) To begin, load in the Boston data set. The Boston data set is part of the ISLR2 library. Now the data set is contained in the object Boston.\nRead about the data set:\n\np_01\n\nlibrary(ISLR2)\nlibrary(tidyverse)\nlibrary(MASS)\n\ndata(\"Boston\")\n#?Boston\n\n\n\np_0b\nHow many rows are in this data set? How many columns? What do the rows and columns represent?\n\ndim(Boston)\n\n[1] 506  14\n\nglimpse(Boston)\n\nRows: 506\nColumns: 14\n$ crim    &lt;dbl&gt; 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829,…\n$ zn      &lt;dbl&gt; 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 12.5, 1…\n$ indus   &lt;dbl&gt; 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87, 7.…\n$ chas    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ nox     &lt;dbl&gt; 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524,…\n$ rm      &lt;dbl&gt; 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, 6.012, 6.172, 5.631,…\n$ age     &lt;dbl&gt; 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9, 9…\n$ dis     &lt;dbl&gt; 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5605, 5.9505…\n$ rad     &lt;int&gt; 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ tax     &lt;dbl&gt; 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 311, 31…\n$ ptratio &lt;dbl&gt; 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2, 15…\n$ black   &lt;dbl&gt; 396.90, 396.90, 392.83, 394.63, 396.90, 394.12, 395.60, 396.90…\n$ lstat   &lt;dbl&gt; 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.10…\n$ medv    &lt;dbl&gt; 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15…\n\n\n\n\np_1b\nMake some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.\n\npairs(Boston)\n\n\n\n\n\n\n\n\nAlso have correlation matrix in heatmap with labels\n\ncor(Boston)\n\n               crim          zn       indus         chas         nox\ncrim     1.00000000 -0.20046922  0.40658341 -0.055891582  0.42097171\nzn      -0.20046922  1.00000000 -0.53382819 -0.042696719 -0.51660371\nindus    0.40658341 -0.53382819  1.00000000  0.062938027  0.76365145\nchas    -0.05589158 -0.04269672  0.06293803  1.000000000  0.09120281\nnox      0.42097171 -0.51660371  0.76365145  0.091202807  1.00000000\nrm      -0.21924670  0.31199059 -0.39167585  0.091251225 -0.30218819\nage      0.35273425 -0.56953734  0.64477851  0.086517774  0.73147010\ndis     -0.37967009  0.66440822 -0.70802699 -0.099175780 -0.76923011\nrad      0.62550515 -0.31194783  0.59512927 -0.007368241  0.61144056\ntax      0.58276431 -0.31456332  0.72076018 -0.035586518  0.66802320\nptratio  0.28994558 -0.39167855  0.38324756 -0.121515174  0.18893268\nblack   -0.38506394  0.17552032 -0.35697654  0.048788485 -0.38005064\nlstat    0.45562148 -0.41299457  0.60379972 -0.053929298  0.59087892\nmedv    -0.38830461  0.36044534 -0.48372516  0.175260177 -0.42732077\n                 rm         age         dis          rad         tax    ptratio\ncrim    -0.21924670  0.35273425 -0.37967009  0.625505145  0.58276431  0.2899456\nzn       0.31199059 -0.56953734  0.66440822 -0.311947826 -0.31456332 -0.3916785\nindus   -0.39167585  0.64477851 -0.70802699  0.595129275  0.72076018  0.3832476\nchas     0.09125123  0.08651777 -0.09917578 -0.007368241 -0.03558652 -0.1215152\nnox     -0.30218819  0.73147010 -0.76923011  0.611440563  0.66802320  0.1889327\nrm       1.00000000 -0.24026493  0.20524621 -0.209846668 -0.29204783 -0.3555015\nage     -0.24026493  1.00000000 -0.74788054  0.456022452  0.50645559  0.2615150\ndis      0.20524621 -0.74788054  1.00000000 -0.494587930 -0.53443158 -0.2324705\nrad     -0.20984667  0.45602245 -0.49458793  1.000000000  0.91022819  0.4647412\ntax     -0.29204783  0.50645559 -0.53443158  0.910228189  1.00000000  0.4608530\nptratio -0.35550149  0.26151501 -0.23247054  0.464741179  0.46085304  1.0000000\nblack    0.12806864 -0.27353398  0.29151167 -0.444412816 -0.44180801 -0.1773833\nlstat   -0.61380827  0.60233853 -0.49699583  0.488676335  0.54399341  0.3740443\nmedv     0.69535995 -0.37695457  0.24992873 -0.381626231 -0.46853593 -0.5077867\n              black      lstat       medv\ncrim    -0.38506394  0.4556215 -0.3883046\nzn       0.17552032 -0.4129946  0.3604453\nindus   -0.35697654  0.6037997 -0.4837252\nchas     0.04878848 -0.0539293  0.1752602\nnox     -0.38005064  0.5908789 -0.4273208\nrm       0.12806864 -0.6138083  0.6953599\nage     -0.27353398  0.6023385 -0.3769546\ndis      0.29151167 -0.4969958  0.2499287\nrad     -0.44441282  0.4886763 -0.3816262\ntax     -0.44180801  0.5439934 -0.4685359\nptratio -0.17738330  0.3740443 -0.5077867\nblack    1.00000000 -0.3660869  0.3334608\nlstat   -0.36608690  1.0000000 -0.7376627\nmedv     0.33346082 -0.7376627  1.0000000\n\ncor(Boston) %&gt;% \n  corrplot::corrplot()\n\n\n\n\n\n\n\n\nHave correlation between crim with other variables\n\n# Assuming Boston dataset is loaded and available\n# Calculate the correlation matrix\ncor_matrix &lt;- cor(Boston)\n\n# Extract the correlation of 'crim' with all other variables\ncrim_correlations &lt;- cor_matrix[\"crim\", ]\n\n# Sort the correlations in descending order\nsorted_crim_correlations &lt;- sort(crim_correlations, decreasing = TRUE)\n\n# Display the sorted correlations\nprint(sorted_crim_correlations)\n\n       crim         rad         tax       lstat         nox       indus \n 1.00000000  0.62550515  0.58276431  0.45562148  0.42097171  0.40658341 \n        age     ptratio        chas          zn          rm         dis \n 0.35273425  0.28994558 -0.05589158 -0.20046922 -0.21924670 -0.37967009 \n      black        medv \n-0.38506394 -0.38830461 \n\n\n\n\np_0c\nAre any of the predictors associated with per capita crime rate? If so, explain the relationship.\n\n\np_0d\nDo any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.\n\nBoston %&gt;% \n  dplyr::select(crim, tax, ptratio) %&gt;% \n  summary()\n\n      crim               tax           ptratio     \n Min.   : 0.00632   Min.   :187.0   Min.   :12.60  \n 1st Qu.: 0.08205   1st Qu.:279.0   1st Qu.:17.40  \n Median : 0.25651   Median :330.0   Median :19.05  \n Mean   : 3.61352   Mean   :408.2   Mean   :18.46  \n 3rd Qu.: 3.67708   3rd Qu.:666.0   3rd Qu.:20.20  \n Max.   :88.97620   Max.   :711.0   Max.   :22.00  \n\n\n\nlibrary(tidyverse)\n\n# Assuming the Boston dataset is available\n# Summarize the range and identify high values for 'crim', 'tax', and 'ptratio'\nsummary_stats &lt;- Boston %&gt;%\n  summarise(\n    crim_min = min(crim),\n    crim_max = max(crim),\n    tax_min = min(tax),\n    tax_max = max(tax),\n    ptratio_min = min(ptratio),\n    ptratio_max = max(ptratio)\n  )\n\n# Identify high values based on chosen thresholds (e.g., top 5% as high)\nhigh_values &lt;- Boston %&gt;%\n  summarise(\n    high_crim = quantile(crim, 0.95),\n    high_tax = quantile(tax, 0.95),\n    high_ptratio = quantile(ptratio, 0.95)\n  ) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"threshold\")\n\n# Find the census tracts with particularly high values\nhigh_tracts &lt;- Boston %&gt;%\n  filter(\n    crim &gt; high_values$threshold[high_values$variable == \"high_crim\"] |\n    tax &gt; high_values$threshold[high_values$variable == \"high_tax\"] |\n    ptratio &gt; high_values$threshold[high_values$variable == \"high_ptratio\"]\n  ) %&gt;%\n  dplyr::select(crim, tax, ptratio)\n\n# Print the range of each predictor\nprint(\"Summary of Ranges:\")\n\n[1] \"Summary of Ranges:\"\n\nprint(summary_stats)\n\n  crim_min crim_max tax_min tax_max ptratio_min ptratio_max\n1  0.00632  88.9762     187     711        12.6          22\n\n# Print the high value thresholds for each variable\nprint(\"Thresholds for High Values (Top 5%):\")\n\n[1] \"Thresholds for High Values (Top 5%):\"\n\nprint(high_values)\n\n# A tibble: 3 × 2\n  variable     threshold\n  &lt;chr&gt;            &lt;dbl&gt;\n1 high_crim         15.8\n2 high_tax         666  \n3 high_ptratio      21  \n\n# Display census tracts with high values\nprint(\"Census Tracts with High Values for 'crim', 'tax', or 'ptratio':\")\n\n[1] \"Census Tracts with High Values for 'crim', 'tax', or 'ptratio':\"\n\nprint(head(high_tracts))\n\n     crim tax ptratio\n1 0.01360 469    21.1\n2 0.25915 437    21.2\n3 0.32543 437    21.2\n4 0.88125 437    21.2\n5 0.34006 437    21.2\n6 1.19294 437    21.2\n\n\n\n\np_0e\nHow many of the census tracts in this data set bound the Charles river?\n\nBoston %&gt;%\n  filter(chas == 1) %&gt;%\n  nrow()\n\n[1] 35\n\n\n\n\np_0f\nWhat is the median pupil-teacher ratio among the towns in this data set?\n\nBoston %&gt;%\n  summarise(median_ptratio = median(ptratio))\n\n  median_ptratio\n1          19.05\n\n\n\n\np_0g\nWhich census tract of Boston has lowest median value of owner occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings.\n\n# Find the census tract with the lowest median value of owner-occupied homes\nlowest_medv_tract &lt;- Boston %&gt;%\n  filter(medv == min(medv)) %&gt;%\n  dplyr::select(everything())\n\n# Summarize the range of each predictor in the dataset\noverall_ranges &lt;- Boston %&gt;%\n  summarise(across(everything(), list(min = min, max = max)))\n\n# Display the census tract with the lowest 'medv'\nprint(\"Census Tract with the Lowest Median Value of Owner-Occupied Homes:\")\n\n[1] \"Census Tract with the Lowest Median Value of Owner-Occupied Homes:\"\n\nprint(lowest_medv_tract)\n\n     crim zn indus chas   nox    rm age    dis rad tax ptratio  black lstat\n1 38.3518  0  18.1    0 0.693 5.453 100 1.4896  24 666    20.2 396.90 30.59\n2 67.9208  0  18.1    0 0.693 5.683 100 1.4254  24 666    20.2 384.97 22.98\n  medv\n1    5\n2    5\n\n# Display the overall ranges for comparison\nprint(\"Overall Ranges for Each Predictor:\")\n\n[1] \"Overall Ranges for Each Predictor:\"\n\nprint(overall_ranges)\n\n  crim_min crim_max zn_min zn_max indus_min indus_max chas_min chas_max nox_min\n1  0.00632  88.9762      0    100      0.46     27.74        0        1   0.385\n  nox_max rm_min rm_max age_min age_max dis_min dis_max rad_min rad_max tax_min\n1   0.871  3.561   8.78     2.9     100  1.1296 12.1265       1      24     187\n  tax_max ptratio_min ptratio_max black_min black_max lstat_min lstat_max\n1     711        12.6          22      0.32     396.9      1.73     37.97\n  medv_min medv_max\n1        5       50\n\n\n\n\np_0h\nIn this data set, how many of the census tracts average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling.\n\n# Count the number of census tracts with more than 7 rooms per dwelling\ncount_more_than_7 &lt;- Boston %&gt;%\n  filter(rm &gt; 7) %&gt;%\n  summarise(count = n())\n\n# Count the number of census tracts with more than 8 rooms per dwelling\ncount_more_than_8 &lt;- Boston %&gt;%\n  filter(rm &gt; 8) %&gt;%\n  summarise(count = n())\n\n# Display the results\nprint(\"Number of census tracts averaging more than 7 rooms per dwelling:\")\n\n[1] \"Number of census tracts averaging more than 7 rooms per dwelling:\"\n\nprint(count_more_than_7)\n\n  count\n1    64\n\nprint(\"Number of census tracts averaging more than 8 rooms per dwelling:\")\n\n[1] \"Number of census tracts averaging more than 8 rooms per dwelling:\"\n\nprint(count_more_than_8)\n\n  count\n1    13\n\n# Display details of census tracts with more than 8 rooms per dwelling\ntracts_more_than_8 &lt;- Boston %&gt;%\n  filter(rm &gt; 8) %&gt;%\n  dplyr::select(everything())\n\nprint(\"Details of census tracts averaging more than 8 rooms per dwelling:\")\n\n[1] \"Details of census tracts averaging more than 8 rooms per dwelling:\"\n\nprint(tracts_more_than_8)\n\n      crim zn indus chas    nox    rm  age    dis rad tax ptratio  black lstat\n1  0.12083  0  2.89    0 0.4450 8.069 76.0 3.4952   2 276    18.0 396.90  4.21\n2  1.51902  0 19.58    1 0.6050 8.375 93.9 2.1620   5 403    14.7 388.45  3.32\n3  0.02009 95  2.68    0 0.4161 8.034 31.9 5.1180   4 224    14.7 390.55  2.88\n4  0.31533  0  6.20    0 0.5040 8.266 78.3 2.8944   8 307    17.4 385.05  4.14\n5  0.52693  0  6.20    0 0.5040 8.725 83.0 2.8944   8 307    17.4 382.00  4.63\n6  0.38214  0  6.20    0 0.5040 8.040 86.5 3.2157   8 307    17.4 387.38  3.13\n7  0.57529  0  6.20    0 0.5070 8.337 73.3 3.8384   8 307    17.4 385.91  2.47\n8  0.33147  0  6.20    0 0.5070 8.247 70.4 3.6519   8 307    17.4 378.95  3.95\n9  0.36894 22  5.86    0 0.4310 8.259  8.4 8.9067   7 330    19.1 396.90  3.54\n10 0.61154 20  3.97    0 0.6470 8.704 86.9 1.8010   5 264    13.0 389.70  5.12\n11 0.52014 20  3.97    0 0.6470 8.398 91.5 2.2885   5 264    13.0 386.86  5.91\n12 0.57834 20  3.97    0 0.5750 8.297 67.0 2.4216   5 264    13.0 384.54  7.44\n13 3.47428  0 18.10    1 0.7180 8.780 82.9 1.9047  24 666    20.2 354.55  5.29\n   medv\n1  38.7\n2  50.0\n3  50.0\n4  44.8\n5  50.0\n6  37.6\n7  41.7\n8  48.3\n9  42.8\n10 50.0\n11 48.8\n12 50.0\n13 21.9"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Author",
    "section": "",
    "text": "Template for about page\n\n\n\nIt is possible to use different templates for the About page. Check this link to find out more."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Author",
    "section": "Education",
    "text": "Education\nPakistan Institute of Development Economics | Islamabad, Pakistan\nPhD in Economics | July 2024\nBZU | Multan, Pakistan\nMSc in Statistics"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Author",
    "section": "Experience",
    "text": "Experience\nI have published widely both in Applied Economics and Statistics both in national and international journals.\nI have also been serving as a trainer for different organizations in the field of data analysis and econometrics. My expertise is in R, Stata, Python, EVIEWS, and Excel."
  },
  {
    "objectID": "ch03.html",
    "href": "ch03.html",
    "title": "ISLR chapter 3",
    "section": "",
    "text": "This is a little modified version of the code from the book Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. The code is written in R and I have used the tidyverse package to make it more readable and easy to understand instead of using the base R functions.\nThe first thing we need to do is install and then load the tidyverse set of R packages to provide us with lots of extra functionality. You only need to install this once: once it’s installed we can simply load it into the workspace using the library(packagename) function each time we open a new R session.\nUnderstanding data sets requires many hours/days or in some cases weeks.There are many commercially available software but open source community based software have now dominated and R is one of these. Here I also load the MASS package, which is a very large collection of data sets and functions. We also load the ISLR2 package, which includes the data sets associated with the book Introduction to Statistical Learning.\n\nlibrary(MASS)\nlibrary(ISLR2)\nlibrary(tidyverse)\n\n\n\n\nHere’s a tabular format representing the meta data for the Boston data set from the ISLR2 library. This table provides the column name, a brief description, and the type of data in each column.\n\n\n\n\n\n\n\n\nColumn Name\nDescription\nData Type\n\n\n\n\ncrim\nPer capita crime rate by town\nNumeric\n\n\nzn\nProportion of residential land zoned for lots over 25,000 sq. ft.\nNumeric\n\n\nindus\nProportion of non-retail business acres per town\nNumeric\n\n\nchas\nCharles River dummy variable (1 if tract bounds river; 0 otherwise)\nCategorical\n\n\nnox\nNitrogen oxides concentration (parts per 10 million)\nNumeric\n\n\nrm\nAverage number of rooms per dwelling\nNumeric\n\n\nage\nProportion of owner-occupied units built prior to 1940\nNumeric\n\n\ndis\nWeighted distances to five Boston employment centers\nNumeric\n\n\nrad\nIndex of accessibility to radial highways\nCategorical\n\n\ntax\nFull-value property-tax rate per $10,000\nNumeric\n\n\nptratio\nPupil-teacher ratio by town\nNumeric\n\n\nblack\n1000(Bk - 0.63)^2 where Bk is the proportion of Black people by town\nNumeric\n\n\nlstat\nPercentage of lower status of the population\nNumeric\n\n\nmedv\nMedian value of owner-occupied homes in $1000’s\nNumeric\n\n\n\nThis meta data outlines the features available in the Boston dataset that can be used to predict the medv variable.\n\nhead(Boston)\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat medv\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98 24.0\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14 21.6\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03 34.7\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7  2.94 33.4\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7  5.33 36.2\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7  5.21 28.7\n\n\nTo find out more about the data set, we can type ?Boston.\nWe will start by using the lm() function to fit a simple linear regression model, with medv as the response and lstat as the predictor. The basic syntax is lm(y ~ x, data), where y is the response, x is the predictor, and data is the data set in which these two variables are kept.\n\nlm_fit &lt;- lm(medv ~ lstat)\n\nError in eval(predvars, data, env): object 'medv' not found\n\n\nThe command causes an error because R does not know where to find the variables medv and lstat. The next line tells R that the variables are in Boston. If we attach Boston, the first line works fine because R now recognizes the variables.\n\nlm_fit &lt;- lm(medv ~ lstat, data = Boston)\nattach(Boston)  # attach the Boston data set\nlm_fit &lt;- lm(medv ~ lstat) # fit the model with data already attached\n\nIf we type lm_fit, some basic information about the model is output. For more detailed information, one may use summary(lm_fit) and I am using here modelsummary package to present summary in a more readable format. The summary() function outputs the coefficients of the model as well as their standard errors, \\(t\\)-statistics, and \\(p\\)-values. The summary() function also outputs the \\(R^2\\) statistic and an analysis of variance (ANOVA) table, which breaks down the variance associated with the regression model and the residuals. This gives us \\(p\\)-values and standard errors for the coefficients, as well as the \\(R^2\\) statistic and \\(F\\)-statistic for the model.\n\nlibrary(modelsummary)\n\n`modelsummary` 2.0.0 now uses `tinytable` as its default table-drawing\n  backend. Learn more at: https://vincentarelbundock.github.io/tinytable/\n\nRevert to `kableExtra` for one session:\n\n  options(modelsummary_factory_default = 'kableExtra')\n  options(modelsummary_factory_latex = 'kableExtra')\n  options(modelsummary_factory_html = 'kableExtra')\n\nSilence this message forever:\n\n  config_modelsummary(startup_message = FALSE)\n\nmsummary(lm_fit)\n\n \n\n  \n    \n    \n    tinytable_aqnu32b4kv5m5p83ubbl\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  34.554   \n                \n                \n                             \n                  (0.563)  \n                \n                \n                  lstat      \n                  -0.950   \n                \n                \n                             \n                  (0.039)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.544    \n                \n                \n                  R2 Adj.    \n                  0.543    \n                \n                \n                  AIC        \n                  3289.0   \n                \n                \n                  BIC        \n                  3301.7   \n                \n                \n                  Log.Lik.   \n                  -1641.487\n                \n                \n                  F          \n                  601.618  \n                \n                \n                  RMSE       \n                  6.20     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nOne may extract the coefficients of the model using the coef() function. The names() function can be used to extract the names of the coefficients.\n\nnames(lm_fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm_fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n\nFor confidence intervals, we can use the confint() function. By default, confint() provides 95 % confidence intervals; however, this can be changed using the level argument.\n\nconfint(lm_fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\nTo predict the median house value for a given percentage of lower status of the population, we can use the predict() function. The predict() function can be used to produce confidence intervals and prediction intervals for the prediction.\n\npredict(lm_fit, data.frame(lstat = (c(5, 10, 15))),\n        interval = \"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm_fit, data.frame(lstat = (c(5, 10, 15))),\n        interval = \"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n\n\nlibrary(modelsummary)\nlibrary(broom)\n\n# Fit the linear model\nlm_fit &lt;- lm(medv ~ lstat, data = Boston)\n\n# Create predictions with confidence intervals\npredictions &lt;- predict(lm_fit, newdata = data.frame(lstat = c(5, 10, 15)), interval = \"confidence\")\n\n# Convert predictions to a data frame\npred_df &lt;- as.data.frame(predictions)\npred_df$lstat &lt;- c(5, 10, 15)\n\nlibrary(gt)\npred_df |&gt; gt() %&gt;%\n  tab_header(title = \"Predicted Values with Confidence Intervals\")\n\n\n\n\n\n\n\n\nPredicted Values with Confidence Intervals\n\n\nfit\nlwr\nupr\nlstat\n\n\n\n\n29.80359\n29.00741\n30.59978\n5\n\n\n25.05335\n24.47413\n25.63256\n10\n\n\n20.30310\n19.73159\n20.87461\n15\n\n\n\n\n\n\n\n\nFor instance, the 95 % confidence interval associated with a lstat value of 10 is \\((24.47, 25.63)\\), and the 95 % prediction interval is \\((12.828, 37.28)\\). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of \\(25.05\\) for medv when lstat equals 10), but the latter are substantially wider.\nWe will now plot medv and lstat along with the least squares regression line using ggplot2.\n\nggplot(Boston, aes(x = lstat, y = medv)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere is some evidence for non-linearity in the relationship between lstat and medv. We will explore this issue later in this lab.\n\nggplot(Boston, aes(x = lstat, y = medv)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_abline(intercept = 34, slope = -1, color = \"red\", size = 3) +\n  geom_point(color = \"red\") +\n  geom_point(shape = 20) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNext we examine some diagnostic plots, several of which were discussed in Section 3.3.3. Four diagnostic plots are automatically produced by applying the plot() function directly to the output from lm(). In general, this command will produce one plot at a time, and hitting Enter will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the par() and mfrow() functions, which tell R to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, par(mfrow = c(2, 2)) divides the plotting region into a \\(2 \\times 2\\) grid of panels.\n\n# Load necessary libraries\n\nlibrary(broom)\n\n# Fit the linear model\nlm_fit &lt;- lm(medv ~ lstat, data = Boston)\n\n# Extract augmented data for diagnostics\naugmented_data &lt;- augment(lm_fit)\n\n\n# Residuals vs Fitted Plot\nggplot(augmented_data, aes(.fitted, .resid)) +\n  geom_point() +\n  geom_smooth(method = 'loess', col = 'blue', se = FALSE) +\n  labs(title = \"Residuals vs Fitted\", x = \"Fitted values\", y = \"Residuals\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Normal Q-Q Plot\nggplot(augmented_data, aes(sample = .std.resid)) +\n  geom_qq() +\n  geom_qq_line() +\n  labs(title = \"Normal Q-Q\", x = \"Theoretical Quantiles\", y = \"Standardized Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Scale-Location Plot (Spread of residuals)\nggplot(augmented_data, aes(.fitted, sqrt(abs(.std.resid)))) +\n  geom_point() +\n  geom_smooth(method = 'loess', col = 'blue', se = FALSE) +\n  labs(title = \"Scale-Location\", x = \"Fitted values\", y = \"Sqrt(|Standardized Residuals|)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Residuals vs Leverage Plot\nggplot(augmented_data, aes(.hat, .std.resid)) +\n  geom_point() +\n  geom_smooth(method = 'loess', col = 'blue', se = FALSE) +\n  geom_hline(yintercept = c(-3, 3), linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residuals vs Leverage\", x = \"Leverage\", y = \"Standardized Residuals\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Highlight influential points\n# Replace .rownames with row_number if needed\n# Add a row number column to identify rows\naugmented_data &lt;- augmented_data %&gt;%\n  mutate(row_id = row_number())\n\n# Identify influential points using the row_id\ninfluential_points &lt;- augmented_data %&gt;%\n  filter(.hat &gt; 2 * mean(.hat)) %&gt;%\n  pull(row_id)\n\n# Print the identified influential points\nprint(influential_points)\n\n [1]   9  33  49 124 127 142 143 144 145 146 148 149 215 374 375 385 386 387 388\n[20] 389 393 399 400 401 405 409 413 415 416 417 418 438 439 491\n\nprint(influential_points)\n\n [1]   9  33  49 124 127 142 143 144 145 146 148 149 215 374 375 385 386 387 388\n[20] 389 393 399 400 401 405 409 413 415 416 417 418 438 439 491\n\n\nOn the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the hatvalues() function.\n\nleverage &lt;- hatvalues(lm_fit)\n\nThe which.max() function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.\n\nwhich.max(leverage)\n\n375 \n375 \n\n\n\n\n\nMutiple linear regression is an extension of simple linear regression to the case of two or more predictors. Each predictor has a regression coefficient (i.e. a slope), and there is one intercept. The model is given by\n\\[ Y_i = \\beta_0 + \\beta_1 X_1i + \\beta_2 X_2i + \\ldots + \\beta_p X_pi + \\epsilon_i. \\]\n\\[ Y_i = \\beta_0 + \\beta_1 X_1i + \\beta_2 X_2i + \\ldots + \\beta_p X_pi + \\epsilon_i. \\] The summary() function produces a detailed summary of the regression fit. I will use modelsummary package to produce a more readable output.\n\nlm_fit &lt;- lm(medv ~ lstat + age, data = Boston)\nlibrary(modelsummary)\n\nmodelsummary(lm_fit, estimate = \"std.error\", stars = TRUE)\n\n \n\n  \n    \n    \n    tinytable_y0wwvmf6m7jgyuk0gd6t\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  0.731*** \n                \n                \n                             \n                  (0.731)  \n                \n                \n                  lstat      \n                  0.048*** \n                \n                \n                             \n                  (0.048)  \n                \n                \n                  age        \n                  0.012**  \n                \n                \n                             \n                  (0.012)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.551    \n                \n                \n                  R2 Adj.    \n                  0.549    \n                \n                \n                  AIC        \n                  3283.0   \n                \n                \n                  BIC        \n                  3299.9   \n                \n                \n                  Log.Lik.   \n                  -1637.503\n                \n                \n                  F          \n                  308.969  \n                \n                \n                  RMSE       \n                  6.15     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nThe Boston data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:\n\nlm_fit &lt;- lm(medv ~ ., data = Boston)\nmsummary(lm_fit, estimate = \"std.error\", stars = TRUE)\n\n \n\n  \n    \n    \n    tinytable_rtim97tkxk9igoa2jdiv\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  4.936*** \n                \n                \n                             \n                  (4.936)  \n                \n                \n                  crim       \n                  0.033*** \n                \n                \n                             \n                  (0.033)  \n                \n                \n                  zn         \n                  0.014*** \n                \n                \n                             \n                  (0.014)  \n                \n                \n                  indus      \n                  0.062    \n                \n                \n                             \n                  (0.062)  \n                \n                \n                  chas       \n                  0.870**  \n                \n                \n                             \n                  (0.870)  \n                \n                \n                  nox        \n                  3.851*** \n                \n                \n                             \n                  (3.851)  \n                \n                \n                  rm         \n                  0.420*** \n                \n                \n                             \n                  (0.420)  \n                \n                \n                  age        \n                  0.013    \n                \n                \n                             \n                  (0.013)  \n                \n                \n                  dis        \n                  0.202*** \n                \n                \n                             \n                  (0.202)  \n                \n                \n                  rad        \n                  0.067*** \n                \n                \n                             \n                  (0.067)  \n                \n                \n                  tax        \n                  0.004*** \n                \n                \n                             \n                  (0.004)  \n                \n                \n                  ptratio    \n                  0.132*** \n                \n                \n                             \n                  (0.132)  \n                \n                \n                  lstat      \n                  0.051*** \n                \n                \n                             \n                  (0.051)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.734    \n                \n                \n                  R2 Adj.    \n                  0.728    \n                \n                \n                  AIC        \n                  3037.8   \n                \n                \n                  BIC        \n                  3097.0   \n                \n                \n                  Log.Lik.   \n                  -1504.910\n                \n                \n                  F          \n                  113.544  \n                \n                \n                  RMSE       \n                  4.74     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nWe can access the individual components of a summary object by name. The names() function can be used to obtain the names of the components. The summary() function returns a list with components such as call, terms, residuals, coefficients, aliased, sigma, df, r.squared, adj.r.squared, fstatistic, cov.unscaled, na.action. The vif() function, part of the car package, can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data. The car package is not part of the base R installation so it must be downloaded the first time you use it via the install.packages() function in R.\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nWhat if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high \\(p\\)-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except age.\n\nlm_fit1 &lt;- lm(medv ~ . - age, data = Boston)\nmsummary(lm_fit1)\n\n \n\n  \n    \n    \n    tinytable_fh533nvskvlgyxfrijo2\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  41.525   \n                \n                \n                             \n                  (4.920)  \n                \n                \n                  crim       \n                  -0.121   \n                \n                \n                             \n                  (0.033)  \n                \n                \n                  zn         \n                  0.047    \n                \n                \n                             \n                  (0.014)  \n                \n                \n                  indus      \n                  0.013    \n                \n                \n                             \n                  (0.062)  \n                \n                \n                  chas       \n                  2.853    \n                \n                \n                             \n                  (0.868)  \n                \n                \n                  nox        \n                  -18.485  \n                \n                \n                             \n                  (3.714)  \n                \n                \n                  rm         \n                  3.681    \n                \n                \n                             \n                  (0.411)  \n                \n                \n                  dis        \n                  -1.507   \n                \n                \n                             \n                  (0.193)  \n                \n                \n                  rad        \n                  0.288    \n                \n                \n                             \n                  (0.067)  \n                \n                \n                  tax        \n                  -0.013   \n                \n                \n                             \n                  (0.004)  \n                \n                \n                  ptratio    \n                  -0.935   \n                \n                \n                             \n                  (0.132)  \n                \n                \n                  lstat      \n                  -0.547   \n                \n                \n                             \n                  (0.048)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.734    \n                \n                \n                  R2 Adj.    \n                  0.728    \n                \n                \n                  AIC        \n                  3035.9   \n                \n                \n                  BIC        \n                  3090.8   \n                \n                \n                  Log.Lik.   \n                  -1504.948\n                \n                \n                  F          \n                  124.092  \n                \n                \n                  RMSE       \n                  4.74     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nAlternatively, the update() function can be used.\n\nlm_fit1 &lt;- update(lm_fit, ~ . - age)\n\n\n\n\nIt is easy to include interaction terms in a linear model using the lm() function. The syntax lstat:age tells R to include an interaction term between lstat and age. The syntax lstat * age simultaneously includes lstat, age, and the interaction term lstat\\(\\times\\)age as predictors; it is a shorthand for lstat + age + lstat:age. %We can also pass in transformed versions of the predictors.\n\nmsummary(lm(medv ~ lstat * age, data = Boston), estimate = \"std.error\", stars = TRUE)\n\n \n\n  \n    \n    \n    tinytable_9mhvxbwi0wbwi0kjtzmk\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  1.470*** \n                \n                \n                             \n                  (1.470)  \n                \n                \n                  lstat      \n                  0.167*** \n                \n                \n                             \n                  (0.167)  \n                \n                \n                  age        \n                  0.020    \n                \n                \n                             \n                  (0.020)  \n                \n                \n                  lstat × age\n                  0.002*   \n                \n                \n                             \n                  (0.002)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.556    \n                \n                \n                  R2 Adj.    \n                  0.553    \n                \n                \n                  AIC        \n                  3280.0   \n                \n                \n                  BIC        \n                  3301.1   \n                \n                \n                  Log.Lik.   \n                  -1634.977\n                \n                \n                  F          \n                  209.312  \n                \n                \n                  RMSE       \n                  6.12     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\n\n\nThe lm() function can also accommodate non-linear transformations of the predictors. For instance, given a predictor \\(X\\), we can create a predictor \\(X^2\\) using I(X^2). The function I() is needed since the ^ has a special meaning in a formula object; wrapping as we do allows the standard usage in R, which is to raise X to the power 2. We now perform a regression of medv onto lstat and lstat^2.\n\nlm_fit2 &lt;- lm(medv ~ lstat + I(lstat^2))\nmsummary(lm_fit2, estimate = \"std.error\", stars = TRUE)\n\n \n\n  \n    \n    \n    tinytable_td1tu8wa9ttv779t6uut\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  0.872*** \n                \n                \n                             \n                  (0.872)  \n                \n                \n                  lstat      \n                  0.124*** \n                \n                \n                             \n                  (0.124)  \n                \n                \n                  I(lstat^2) \n                  0.004*** \n                \n                \n                             \n                  (0.004)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.641    \n                \n                \n                  R2 Adj.    \n                  0.639    \n                \n                \n                  AIC        \n                  3170.5   \n                \n                \n                  BIC        \n                  3187.4   \n                \n                \n                  Log.Lik.   \n                  -1581.258\n                \n                \n                  F          \n                  448.505  \n                \n                \n                  RMSE       \n                  5.51     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nThe near-zero \\(p\\)-value associated with the quadratic term suggests that it leads to an improved model. We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\nsummary(lm(medv ~ ., data = Boston))\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1304  -2.7673  -0.5814   1.9414  26.2526 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.617270   4.936039   8.431 3.79e-16 ***\ncrim         -0.121389   0.033000  -3.678 0.000261 ***\nzn            0.046963   0.013879   3.384 0.000772 ***\nindus         0.013468   0.062145   0.217 0.828520    \nchas          2.839993   0.870007   3.264 0.001173 ** \nnox         -18.758022   3.851355  -4.870 1.50e-06 ***\nrm            3.658119   0.420246   8.705  &lt; 2e-16 ***\nage           0.003611   0.013329   0.271 0.786595    \ndis          -1.490754   0.201623  -7.394 6.17e-13 ***\nrad           0.289405   0.066908   4.325 1.84e-05 ***\ntax          -0.012682   0.003801  -3.337 0.000912 ***\nptratio      -0.937533   0.132206  -7.091 4.63e-12 ***\nlstat        -0.552019   0.050659 -10.897  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.798 on 493 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7278 \nF-statistic: 113.5 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nanova(lm_fit, lm_fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + \n    tax + ptratio + lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS  Df Sum of Sq      F    Pr(&gt;F)    \n1    493 11349                                   \n2    503 15347 -10   -3997.8 17.366 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere Model 1 represents the linear submodel containing only one predictor, lstat \\[ medv_i = \\beta_0 + \\beta_1 \\times lstat_i + \\epsilon_i, \\]\nwhile Model 2 corresponds to the larger quadratic model that has two predictors, lstat and lstat^2\n\\[ medv_i = \\beta_0 + \\beta_1 \\times lstat_i + \\beta_2 \\times lstat_i^2 + \\epsilon_i, \\]\nThe anova() function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the \\(F\\)-statistic is \\(135\\) and the associated \\(p\\)-value is virtually zero. This provides very clear evidence that the model containing the predictors lstat and lstat^2 is far superior to the model that only contains the predictor lstat. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat. Diagnostic plots can be used to further investigate the quality of the model fit using residuals.\n\npar(mfrow = c(2, 2))\nplot(lm_fit2)\n\n\n\n\n\n\n\n\n\npar(mfrow = c(2, 2))\n# Base R diagnostic plots\nplot(lm_fit2)\n\n\n\n\n\n\n\n# Enhanced diagnostic plots using the `car` package\nlibrary(car)\nresidualPlots(lm_fit2)\n\n\n\n\n\n\n\n\n           Test stat Pr(&gt;|Test stat|)    \nlstat        -0.7542           0.4511    \nI(lstat^2)   -4.3276        1.820e-05 ***\nTukey test    6.1962        5.783e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nqqPlot(lm_fit2)\n\n[1] 372 373\n\nspreadLevelPlot(lm_fit2)\n\n\nSuggested power transformation:  0.01056432 \n\ninfluencePlot(lm_fit2)\n\n       StudRes         Hat      CookD\n215  2.1564869 0.020322879 0.03192524\n372  4.7026322 0.002557843 0.01814214\n373  4.5109537 0.002598989 0.01701993\n375 -0.6268209 0.110612307 0.01630807\n415 -1.7417833 0.092762866 0.10298355\n\n\n\n\n\n\n\n\n\n\nlibrary(olsrr)\n\n\nAttaching package: 'olsrr'\n\n\nThe following object is masked from 'package:MASS':\n\n    cement\n\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\nols_plot_resid_fit(lm_fit2)\n\n\n\n\n\n\n\n\n\nlibrary(olsrr)\nols_plot_resid_hist(lm_fit2)\n\n\n\n\n\n\n\n\n\nlibrary(olsrr)\nols_plot_resid_qq(lm_fit2)\n\n\n\n\n\n\n\n\n\nols_test_correlation(lm_fit2)\n\n[1] 0.9667391\n\n\nthen we see that when the lstat^2 term is included in the model, there is little discernible pattern in the residuals.\nIn order to create a cubic fit, we can include a predictor of the form I(X^3). However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the poly() function to create the polynomial within lm(). For example, the following command produces a fifth-order polynomial fit:\n\nlm_fit5 &lt;- lm(medv ~ poly(lstat, 5))\nmsummary(lm_fit5)\n\n \n\n  \n    \n    \n    tinytable_mf6mlew1ej1u2njfi0bx\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)    \n                  22.533   \n                \n                \n                                 \n                  (0.232)  \n                \n                \n                  poly(lstat, 5)1\n                  -152.460 \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  poly(lstat, 5)2\n                  64.227   \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  poly(lstat, 5)3\n                  -27.051  \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  poly(lstat, 5)4\n                  25.452   \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  poly(lstat, 5)5\n                  -19.252  \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  Num.Obs.       \n                  506      \n                \n                \n                  R2             \n                  0.682    \n                \n                \n                  R2 Adj.        \n                  0.679    \n                \n                \n                  AIC            \n                  3115.2   \n                \n                \n                  BIC            \n                  3144.8   \n                \n                \n                  Log.Lik.       \n                  -1550.624\n                \n                \n                  F              \n                  214.159  \n                \n                \n                  RMSE           \n                  5.18     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nThis suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant \\(p\\)-values in a regression fit.\nBy default, the poly() function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the poly() function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the poly() function, the argument raw = TRUE must be used.\nOf course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.\n\nmsummary(lm(medv ~ log(rm), data = Boston))\n\n \n\n  \n    \n    \n    tinytable_v28p2dzkxm9i4i5xijlr\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  -76.488  \n                \n                \n                             \n                  (5.028)  \n                \n                \n                  log(rm)    \n                  54.055   \n                \n                \n                             \n                  (2.739)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.436    \n                \n                \n                  R2 Adj.    \n                  0.435    \n                \n                \n                  AIC        \n                  3396.8   \n                \n                \n                  BIC        \n                  3409.5   \n                \n                \n                  Log.Lik.   \n                  -1695.424\n                \n                \n                  F          \n                  389.345  \n                \n                \n                  RMSE       \n                  6.90     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\n\n\nWe will now examine the Carseats data, which is part of the ISLR2 library. We will attempt to predict Sales (child car seat sales) in \\(400\\) locations based on a number of predictors. Carseats have 400 rows and 11 columns.\n\nhead(Carseats)\n\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1  9.50       138     73          11        276   120       Bad  42        17\n2 11.22       111     48          16        260    83      Good  65        10\n3 10.06       113     35          10        269    80    Medium  59        12\n4  7.40       117    100           4        466    97    Medium  55        14\n5  4.15       141     64           3        340   128       Bad  38        13\n6 10.81       124    113          13        501    72       Bad  78        16\n  Urban  US\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\nCarseats |&gt; janitor::clean_names() |&gt; head()\n\n  sales comp_price income advertising population price shelve_loc age education\n1  9.50        138     73          11        276   120        Bad  42        17\n2 11.22        111     48          16        260    83       Good  65        10\n3 10.06        113     35          10        269    80     Medium  59        12\n4  7.40        117    100           4        466    97     Medium  55        14\n5  4.15        141     64           3        340   128        Bad  38        13\n6 10.81        124    113          13        501    72        Bad  78        16\n  urban  us\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\n\nThe Carseats data includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.\n\nlm_fit &lt;- lm(Sales ~ . + Income:Advertising + Price:Age,\n    data = Carseats)\nmsummary(lm_fit)\n\n \n\n  \n    \n    \n    tinytable_qa0h6vgcsouzmzzdvmxa\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)         \n                  6.576   \n                \n                \n                                      \n                  (1.009) \n                \n                \n                  CompPrice           \n                  0.093   \n                \n                \n                                      \n                  (0.004) \n                \n                \n                  Income              \n                  0.011   \n                \n                \n                                      \n                  (0.003) \n                \n                \n                  Advertising         \n                  0.070   \n                \n                \n                                      \n                  (0.023) \n                \n                \n                  Population          \n                  0.000   \n                \n                \n                                      \n                  (0.000) \n                \n                \n                  Price               \n                  -0.101  \n                \n                \n                                      \n                  (0.007) \n                \n                \n                  ShelveLocGood       \n                  4.849   \n                \n                \n                                      \n                  (0.153) \n                \n                \n                  ShelveLocMedium     \n                  1.953   \n                \n                \n                                      \n                  (0.126) \n                \n                \n                  Age                 \n                  -0.058  \n                \n                \n                                      \n                  (0.016) \n                \n                \n                  Education           \n                  -0.021  \n                \n                \n                                      \n                  (0.020) \n                \n                \n                  UrbanYes            \n                  0.140   \n                \n                \n                                      \n                  (0.112) \n                \n                \n                  USYes               \n                  -0.158  \n                \n                \n                                      \n                  (0.149) \n                \n                \n                  Income × Advertising\n                  0.001   \n                \n                \n                                      \n                  (0.000) \n                \n                \n                  Price × Age         \n                  0.000   \n                \n                \n                                      \n                  (0.000) \n                \n                \n                  Num.Obs.            \n                  400     \n                \n                \n                  R2                  \n                  0.876   \n                \n                \n                  R2 Adj.             \n                  0.872   \n                \n                \n                  AIC                 \n                  1159.3  \n                \n                \n                  BIC                 \n                  1219.2  \n                \n                \n                  Log.Lik.            \n                  -564.669\n                \n                \n                  F                   \n                  209.988 \n                \n                \n                  RMSE                \n                  0.99    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nThe contrasts() function returns the coding that R uses for the dummy variables.\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\nUse ?contrasts to learn about other contrasts, and how to set them.\nR has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.\n\n\n\nR comes with numerous built-in functions, and many more are accessible through R libraries. However, there are times when a specific operation is needed for which no existing function is available. In such cases, we can create our own custom functions. Below, we illustrate this by creating a function called LoadLibraries() that loads the tidyverse, broom, and car packages. Attempting to call the function before its definition results in an error, as shown:\nNext, we define the function. The + symbols are printed by R automatically and should not be typed by the user. The { symbol indicates that a block of commands is about to follow. Pressing Enter after { causes R to display the + prompt, allowing for the input of multiple commands. The block is closed with } to signal the end of the function.\n\nLoadLibraries &lt;- function() {\n  library(tidyverse)\n  library(broom)\n  library(car)\n  library(MASS)\n  library(ISLR2)\n  print(\"The libraries have been loaded.\")\n}\n\nTyping LoadLibraries in the console after defining the function will display its content:\n\nLoadLibraries\n\nfunction() {\n  library(tidyverse)\n  library(broom)\n  library(car)\n  library(MASS)\n  library(ISLR2)\n  print(\"The libraries have been loaded.\")\n}\n\n\nCalling the function will load the specified libraries and display the print message:\n\nLoadLibraries()\n\n[1] \"The libraries have been loaded.\""
  },
  {
    "objectID": "ch03.html#libraries",
    "href": "ch03.html#libraries",
    "title": "ISLR chapter 3",
    "section": "",
    "text": "This is a little modified version of the code from the book Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. The code is written in R and I have used the tidyverse package to make it more readable and easy to understand instead of using the base R functions.\nThe first thing we need to do is install and then load the tidyverse set of R packages to provide us with lots of extra functionality. You only need to install this once: once it’s installed we can simply load it into the workspace using the library(packagename) function each time we open a new R session.\nUnderstanding data sets requires many hours/days or in some cases weeks.There are many commercially available software but open source community based software have now dominated and R is one of these. Here I also load the MASS package, which is a very large collection of data sets and functions. We also load the ISLR2 package, which includes the data sets associated with the book Introduction to Statistical Learning.\n\nlibrary(MASS)\nlibrary(ISLR2)\nlibrary(tidyverse)"
  },
  {
    "objectID": "ch03.html#simple-linear-regression",
    "href": "ch03.html#simple-linear-regression",
    "title": "ISLR chapter 3",
    "section": "",
    "text": "Here’s a tabular format representing the meta data for the Boston data set from the ISLR2 library. This table provides the column name, a brief description, and the type of data in each column.\n\n\n\n\n\n\n\n\nColumn Name\nDescription\nData Type\n\n\n\n\ncrim\nPer capita crime rate by town\nNumeric\n\n\nzn\nProportion of residential land zoned for lots over 25,000 sq. ft.\nNumeric\n\n\nindus\nProportion of non-retail business acres per town\nNumeric\n\n\nchas\nCharles River dummy variable (1 if tract bounds river; 0 otherwise)\nCategorical\n\n\nnox\nNitrogen oxides concentration (parts per 10 million)\nNumeric\n\n\nrm\nAverage number of rooms per dwelling\nNumeric\n\n\nage\nProportion of owner-occupied units built prior to 1940\nNumeric\n\n\ndis\nWeighted distances to five Boston employment centers\nNumeric\n\n\nrad\nIndex of accessibility to radial highways\nCategorical\n\n\ntax\nFull-value property-tax rate per $10,000\nNumeric\n\n\nptratio\nPupil-teacher ratio by town\nNumeric\n\n\nblack\n1000(Bk - 0.63)^2 where Bk is the proportion of Black people by town\nNumeric\n\n\nlstat\nPercentage of lower status of the population\nNumeric\n\n\nmedv\nMedian value of owner-occupied homes in $1000’s\nNumeric\n\n\n\nThis meta data outlines the features available in the Boston dataset that can be used to predict the medv variable.\n\nhead(Boston)\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat medv\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98 24.0\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14 21.6\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03 34.7\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7  2.94 33.4\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7  5.33 36.2\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7  5.21 28.7\n\n\nTo find out more about the data set, we can type ?Boston.\nWe will start by using the lm() function to fit a simple linear regression model, with medv as the response and lstat as the predictor. The basic syntax is lm(y ~ x, data), where y is the response, x is the predictor, and data is the data set in which these two variables are kept.\n\nlm_fit &lt;- lm(medv ~ lstat)\n\nError in eval(predvars, data, env): object 'medv' not found\n\n\nThe command causes an error because R does not know where to find the variables medv and lstat. The next line tells R that the variables are in Boston. If we attach Boston, the first line works fine because R now recognizes the variables.\n\nlm_fit &lt;- lm(medv ~ lstat, data = Boston)\nattach(Boston)  # attach the Boston data set\nlm_fit &lt;- lm(medv ~ lstat) # fit the model with data already attached\n\nIf we type lm_fit, some basic information about the model is output. For more detailed information, one may use summary(lm_fit) and I am using here modelsummary package to present summary in a more readable format. The summary() function outputs the coefficients of the model as well as their standard errors, \\(t\\)-statistics, and \\(p\\)-values. The summary() function also outputs the \\(R^2\\) statistic and an analysis of variance (ANOVA) table, which breaks down the variance associated with the regression model and the residuals. This gives us \\(p\\)-values and standard errors for the coefficients, as well as the \\(R^2\\) statistic and \\(F\\)-statistic for the model.\n\nlibrary(modelsummary)\n\n`modelsummary` 2.0.0 now uses `tinytable` as its default table-drawing\n  backend. Learn more at: https://vincentarelbundock.github.io/tinytable/\n\nRevert to `kableExtra` for one session:\n\n  options(modelsummary_factory_default = 'kableExtra')\n  options(modelsummary_factory_latex = 'kableExtra')\n  options(modelsummary_factory_html = 'kableExtra')\n\nSilence this message forever:\n\n  config_modelsummary(startup_message = FALSE)\n\nmsummary(lm_fit)\n\n \n\n  \n    \n    \n    tinytable_aqnu32b4kv5m5p83ubbl\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  34.554   \n                \n                \n                             \n                  (0.563)  \n                \n                \n                  lstat      \n                  -0.950   \n                \n                \n                             \n                  (0.039)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.544    \n                \n                \n                  R2 Adj.    \n                  0.543    \n                \n                \n                  AIC        \n                  3289.0   \n                \n                \n                  BIC        \n                  3301.7   \n                \n                \n                  Log.Lik.   \n                  -1641.487\n                \n                \n                  F          \n                  601.618  \n                \n                \n                  RMSE       \n                  6.20     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nOne may extract the coefficients of the model using the coef() function. The names() function can be used to extract the names of the coefficients.\n\nnames(lm_fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm_fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n\nFor confidence intervals, we can use the confint() function. By default, confint() provides 95 % confidence intervals; however, this can be changed using the level argument.\n\nconfint(lm_fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\nTo predict the median house value for a given percentage of lower status of the population, we can use the predict() function. The predict() function can be used to produce confidence intervals and prediction intervals for the prediction.\n\npredict(lm_fit, data.frame(lstat = (c(5, 10, 15))),\n        interval = \"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm_fit, data.frame(lstat = (c(5, 10, 15))),\n        interval = \"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n\n\nlibrary(modelsummary)\nlibrary(broom)\n\n# Fit the linear model\nlm_fit &lt;- lm(medv ~ lstat, data = Boston)\n\n# Create predictions with confidence intervals\npredictions &lt;- predict(lm_fit, newdata = data.frame(lstat = c(5, 10, 15)), interval = \"confidence\")\n\n# Convert predictions to a data frame\npred_df &lt;- as.data.frame(predictions)\npred_df$lstat &lt;- c(5, 10, 15)\n\nlibrary(gt)\npred_df |&gt; gt() %&gt;%\n  tab_header(title = \"Predicted Values with Confidence Intervals\")\n\n\n\n\n\n\n\n\nPredicted Values with Confidence Intervals\n\n\nfit\nlwr\nupr\nlstat\n\n\n\n\n29.80359\n29.00741\n30.59978\n5\n\n\n25.05335\n24.47413\n25.63256\n10\n\n\n20.30310\n19.73159\n20.87461\n15\n\n\n\n\n\n\n\n\nFor instance, the 95 % confidence interval associated with a lstat value of 10 is \\((24.47, 25.63)\\), and the 95 % prediction interval is \\((12.828, 37.28)\\). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of \\(25.05\\) for medv when lstat equals 10), but the latter are substantially wider.\nWe will now plot medv and lstat along with the least squares regression line using ggplot2.\n\nggplot(Boston, aes(x = lstat, y = medv)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere is some evidence for non-linearity in the relationship between lstat and medv. We will explore this issue later in this lab.\n\nggplot(Boston, aes(x = lstat, y = medv)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_abline(intercept = 34, slope = -1, color = \"red\", size = 3) +\n  geom_point(color = \"red\") +\n  geom_point(shape = 20) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNext we examine some diagnostic plots, several of which were discussed in Section 3.3.3. Four diagnostic plots are automatically produced by applying the plot() function directly to the output from lm(). In general, this command will produce one plot at a time, and hitting Enter will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the par() and mfrow() functions, which tell R to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, par(mfrow = c(2, 2)) divides the plotting region into a \\(2 \\times 2\\) grid of panels.\n\n# Load necessary libraries\n\nlibrary(broom)\n\n# Fit the linear model\nlm_fit &lt;- lm(medv ~ lstat, data = Boston)\n\n# Extract augmented data for diagnostics\naugmented_data &lt;- augment(lm_fit)\n\n\n# Residuals vs Fitted Plot\nggplot(augmented_data, aes(.fitted, .resid)) +\n  geom_point() +\n  geom_smooth(method = 'loess', col = 'blue', se = FALSE) +\n  labs(title = \"Residuals vs Fitted\", x = \"Fitted values\", y = \"Residuals\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Normal Q-Q Plot\nggplot(augmented_data, aes(sample = .std.resid)) +\n  geom_qq() +\n  geom_qq_line() +\n  labs(title = \"Normal Q-Q\", x = \"Theoretical Quantiles\", y = \"Standardized Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Scale-Location Plot (Spread of residuals)\nggplot(augmented_data, aes(.fitted, sqrt(abs(.std.resid)))) +\n  geom_point() +\n  geom_smooth(method = 'loess', col = 'blue', se = FALSE) +\n  labs(title = \"Scale-Location\", x = \"Fitted values\", y = \"Sqrt(|Standardized Residuals|)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Residuals vs Leverage Plot\nggplot(augmented_data, aes(.hat, .std.resid)) +\n  geom_point() +\n  geom_smooth(method = 'loess', col = 'blue', se = FALSE) +\n  geom_hline(yintercept = c(-3, 3), linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residuals vs Leverage\", x = \"Leverage\", y = \"Standardized Residuals\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Highlight influential points\n# Replace .rownames with row_number if needed\n# Add a row number column to identify rows\naugmented_data &lt;- augmented_data %&gt;%\n  mutate(row_id = row_number())\n\n# Identify influential points using the row_id\ninfluential_points &lt;- augmented_data %&gt;%\n  filter(.hat &gt; 2 * mean(.hat)) %&gt;%\n  pull(row_id)\n\n# Print the identified influential points\nprint(influential_points)\n\n [1]   9  33  49 124 127 142 143 144 145 146 148 149 215 374 375 385 386 387 388\n[20] 389 393 399 400 401 405 409 413 415 416 417 418 438 439 491\n\nprint(influential_points)\n\n [1]   9  33  49 124 127 142 143 144 145 146 148 149 215 374 375 385 386 387 388\n[20] 389 393 399 400 401 405 409 413 415 416 417 418 438 439 491\n\n\nOn the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the hatvalues() function.\n\nleverage &lt;- hatvalues(lm_fit)\n\nThe which.max() function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.\n\nwhich.max(leverage)\n\n375 \n375"
  },
  {
    "objectID": "ch03.html#multiple-linear-regression",
    "href": "ch03.html#multiple-linear-regression",
    "title": "ISLR chapter 3",
    "section": "",
    "text": "Mutiple linear regression is an extension of simple linear regression to the case of two or more predictors. Each predictor has a regression coefficient (i.e. a slope), and there is one intercept. The model is given by\n\\[ Y_i = \\beta_0 + \\beta_1 X_1i + \\beta_2 X_2i + \\ldots + \\beta_p X_pi + \\epsilon_i. \\]\n\\[ Y_i = \\beta_0 + \\beta_1 X_1i + \\beta_2 X_2i + \\ldots + \\beta_p X_pi + \\epsilon_i. \\] The summary() function produces a detailed summary of the regression fit. I will use modelsummary package to produce a more readable output.\n\nlm_fit &lt;- lm(medv ~ lstat + age, data = Boston)\nlibrary(modelsummary)\n\nmodelsummary(lm_fit, estimate = \"std.error\", stars = TRUE)\n\n \n\n  \n    \n    \n    tinytable_y0wwvmf6m7jgyuk0gd6t\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  0.731*** \n                \n                \n                             \n                  (0.731)  \n                \n                \n                  lstat      \n                  0.048*** \n                \n                \n                             \n                  (0.048)  \n                \n                \n                  age        \n                  0.012**  \n                \n                \n                             \n                  (0.012)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.551    \n                \n                \n                  R2 Adj.    \n                  0.549    \n                \n                \n                  AIC        \n                  3283.0   \n                \n                \n                  BIC        \n                  3299.9   \n                \n                \n                  Log.Lik.   \n                  -1637.503\n                \n                \n                  F          \n                  308.969  \n                \n                \n                  RMSE       \n                  6.15     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nThe Boston data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:\n\nlm_fit &lt;- lm(medv ~ ., data = Boston)\nmsummary(lm_fit, estimate = \"std.error\", stars = TRUE)\n\n \n\n  \n    \n    \n    tinytable_rtim97tkxk9igoa2jdiv\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  4.936*** \n                \n                \n                             \n                  (4.936)  \n                \n                \n                  crim       \n                  0.033*** \n                \n                \n                             \n                  (0.033)  \n                \n                \n                  zn         \n                  0.014*** \n                \n                \n                             \n                  (0.014)  \n                \n                \n                  indus      \n                  0.062    \n                \n                \n                             \n                  (0.062)  \n                \n                \n                  chas       \n                  0.870**  \n                \n                \n                             \n                  (0.870)  \n                \n                \n                  nox        \n                  3.851*** \n                \n                \n                             \n                  (3.851)  \n                \n                \n                  rm         \n                  0.420*** \n                \n                \n                             \n                  (0.420)  \n                \n                \n                  age        \n                  0.013    \n                \n                \n                             \n                  (0.013)  \n                \n                \n                  dis        \n                  0.202*** \n                \n                \n                             \n                  (0.202)  \n                \n                \n                  rad        \n                  0.067*** \n                \n                \n                             \n                  (0.067)  \n                \n                \n                  tax        \n                  0.004*** \n                \n                \n                             \n                  (0.004)  \n                \n                \n                  ptratio    \n                  0.132*** \n                \n                \n                             \n                  (0.132)  \n                \n                \n                  lstat      \n                  0.051*** \n                \n                \n                             \n                  (0.051)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.734    \n                \n                \n                  R2 Adj.    \n                  0.728    \n                \n                \n                  AIC        \n                  3037.8   \n                \n                \n                  BIC        \n                  3097.0   \n                \n                \n                  Log.Lik.   \n                  -1504.910\n                \n                \n                  F          \n                  113.544  \n                \n                \n                  RMSE       \n                  4.74     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nWe can access the individual components of a summary object by name. The names() function can be used to obtain the names of the components. The summary() function returns a list with components such as call, terms, residuals, coefficients, aliased, sigma, df, r.squared, adj.r.squared, fstatistic, cov.unscaled, na.action. The vif() function, part of the car package, can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data. The car package is not part of the base R installation so it must be downloaded the first time you use it via the install.packages() function in R.\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nWhat if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high \\(p\\)-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except age.\n\nlm_fit1 &lt;- lm(medv ~ . - age, data = Boston)\nmsummary(lm_fit1)\n\n \n\n  \n    \n    \n    tinytable_fh533nvskvlgyxfrijo2\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  41.525   \n                \n                \n                             \n                  (4.920)  \n                \n                \n                  crim       \n                  -0.121   \n                \n                \n                             \n                  (0.033)  \n                \n                \n                  zn         \n                  0.047    \n                \n                \n                             \n                  (0.014)  \n                \n                \n                  indus      \n                  0.013    \n                \n                \n                             \n                  (0.062)  \n                \n                \n                  chas       \n                  2.853    \n                \n                \n                             \n                  (0.868)  \n                \n                \n                  nox        \n                  -18.485  \n                \n                \n                             \n                  (3.714)  \n                \n                \n                  rm         \n                  3.681    \n                \n                \n                             \n                  (0.411)  \n                \n                \n                  dis        \n                  -1.507   \n                \n                \n                             \n                  (0.193)  \n                \n                \n                  rad        \n                  0.288    \n                \n                \n                             \n                  (0.067)  \n                \n                \n                  tax        \n                  -0.013   \n                \n                \n                             \n                  (0.004)  \n                \n                \n                  ptratio    \n                  -0.935   \n                \n                \n                             \n                  (0.132)  \n                \n                \n                  lstat      \n                  -0.547   \n                \n                \n                             \n                  (0.048)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.734    \n                \n                \n                  R2 Adj.    \n                  0.728    \n                \n                \n                  AIC        \n                  3035.9   \n                \n                \n                  BIC        \n                  3090.8   \n                \n                \n                  Log.Lik.   \n                  -1504.948\n                \n                \n                  F          \n                  124.092  \n                \n                \n                  RMSE       \n                  4.74     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nAlternatively, the update() function can be used.\n\nlm_fit1 &lt;- update(lm_fit, ~ . - age)"
  },
  {
    "objectID": "ch03.html#interaction-terms",
    "href": "ch03.html#interaction-terms",
    "title": "ISLR chapter 3",
    "section": "",
    "text": "It is easy to include interaction terms in a linear model using the lm() function. The syntax lstat:age tells R to include an interaction term between lstat and age. The syntax lstat * age simultaneously includes lstat, age, and the interaction term lstat\\(\\times\\)age as predictors; it is a shorthand for lstat + age + lstat:age. %We can also pass in transformed versions of the predictors.\n\nmsummary(lm(medv ~ lstat * age, data = Boston), estimate = \"std.error\", stars = TRUE)\n\n \n\n  \n    \n    \n    tinytable_9mhvxbwi0wbwi0kjtzmk\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  1.470*** \n                \n                \n                             \n                  (1.470)  \n                \n                \n                  lstat      \n                  0.167*** \n                \n                \n                             \n                  (0.167)  \n                \n                \n                  age        \n                  0.020    \n                \n                \n                             \n                  (0.020)  \n                \n                \n                  lstat × age\n                  0.002*   \n                \n                \n                             \n                  (0.002)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.556    \n                \n                \n                  R2 Adj.    \n                  0.553    \n                \n                \n                  AIC        \n                  3280.0   \n                \n                \n                  BIC        \n                  3301.1   \n                \n                \n                  Log.Lik.   \n                  -1634.977\n                \n                \n                  F          \n                  209.312  \n                \n                \n                  RMSE       \n                  6.12"
  },
  {
    "objectID": "ch03.html#non-linear-transformations-of-the-predictors",
    "href": "ch03.html#non-linear-transformations-of-the-predictors",
    "title": "ISLR chapter 3",
    "section": "",
    "text": "The lm() function can also accommodate non-linear transformations of the predictors. For instance, given a predictor \\(X\\), we can create a predictor \\(X^2\\) using I(X^2). The function I() is needed since the ^ has a special meaning in a formula object; wrapping as we do allows the standard usage in R, which is to raise X to the power 2. We now perform a regression of medv onto lstat and lstat^2.\n\nlm_fit2 &lt;- lm(medv ~ lstat + I(lstat^2))\nmsummary(lm_fit2, estimate = \"std.error\", stars = TRUE)\n\n \n\n  \n    \n    \n    tinytable_td1tu8wa9ttv779t6uut\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  0.872*** \n                \n                \n                             \n                  (0.872)  \n                \n                \n                  lstat      \n                  0.124*** \n                \n                \n                             \n                  (0.124)  \n                \n                \n                  I(lstat^2) \n                  0.004*** \n                \n                \n                             \n                  (0.004)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.641    \n                \n                \n                  R2 Adj.    \n                  0.639    \n                \n                \n                  AIC        \n                  3170.5   \n                \n                \n                  BIC        \n                  3187.4   \n                \n                \n                  Log.Lik.   \n                  -1581.258\n                \n                \n                  F          \n                  448.505  \n                \n                \n                  RMSE       \n                  5.51     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nThe near-zero \\(p\\)-value associated with the quadratic term suggests that it leads to an improved model. We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\nsummary(lm(medv ~ ., data = Boston))\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1304  -2.7673  -0.5814   1.9414  26.2526 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.617270   4.936039   8.431 3.79e-16 ***\ncrim         -0.121389   0.033000  -3.678 0.000261 ***\nzn            0.046963   0.013879   3.384 0.000772 ***\nindus         0.013468   0.062145   0.217 0.828520    \nchas          2.839993   0.870007   3.264 0.001173 ** \nnox         -18.758022   3.851355  -4.870 1.50e-06 ***\nrm            3.658119   0.420246   8.705  &lt; 2e-16 ***\nage           0.003611   0.013329   0.271 0.786595    \ndis          -1.490754   0.201623  -7.394 6.17e-13 ***\nrad           0.289405   0.066908   4.325 1.84e-05 ***\ntax          -0.012682   0.003801  -3.337 0.000912 ***\nptratio      -0.937533   0.132206  -7.091 4.63e-12 ***\nlstat        -0.552019   0.050659 -10.897  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.798 on 493 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7278 \nF-statistic: 113.5 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nanova(lm_fit, lm_fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + \n    tax + ptratio + lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS  Df Sum of Sq      F    Pr(&gt;F)    \n1    493 11349                                   \n2    503 15347 -10   -3997.8 17.366 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere Model 1 represents the linear submodel containing only one predictor, lstat \\[ medv_i = \\beta_0 + \\beta_1 \\times lstat_i + \\epsilon_i, \\]\nwhile Model 2 corresponds to the larger quadratic model that has two predictors, lstat and lstat^2\n\\[ medv_i = \\beta_0 + \\beta_1 \\times lstat_i + \\beta_2 \\times lstat_i^2 + \\epsilon_i, \\]\nThe anova() function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the \\(F\\)-statistic is \\(135\\) and the associated \\(p\\)-value is virtually zero. This provides very clear evidence that the model containing the predictors lstat and lstat^2 is far superior to the model that only contains the predictor lstat. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat. Diagnostic plots can be used to further investigate the quality of the model fit using residuals.\n\npar(mfrow = c(2, 2))\nplot(lm_fit2)\n\n\n\n\n\n\n\n\n\npar(mfrow = c(2, 2))\n# Base R diagnostic plots\nplot(lm_fit2)\n\n\n\n\n\n\n\n# Enhanced diagnostic plots using the `car` package\nlibrary(car)\nresidualPlots(lm_fit2)\n\n\n\n\n\n\n\n\n           Test stat Pr(&gt;|Test stat|)    \nlstat        -0.7542           0.4511    \nI(lstat^2)   -4.3276        1.820e-05 ***\nTukey test    6.1962        5.783e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nqqPlot(lm_fit2)\n\n[1] 372 373\n\nspreadLevelPlot(lm_fit2)\n\n\nSuggested power transformation:  0.01056432 \n\ninfluencePlot(lm_fit2)\n\n       StudRes         Hat      CookD\n215  2.1564869 0.020322879 0.03192524\n372  4.7026322 0.002557843 0.01814214\n373  4.5109537 0.002598989 0.01701993\n375 -0.6268209 0.110612307 0.01630807\n415 -1.7417833 0.092762866 0.10298355\n\n\n\n\n\n\n\n\n\n\nlibrary(olsrr)\n\n\nAttaching package: 'olsrr'\n\n\nThe following object is masked from 'package:MASS':\n\n    cement\n\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\nols_plot_resid_fit(lm_fit2)\n\n\n\n\n\n\n\n\n\nlibrary(olsrr)\nols_plot_resid_hist(lm_fit2)\n\n\n\n\n\n\n\n\n\nlibrary(olsrr)\nols_plot_resid_qq(lm_fit2)\n\n\n\n\n\n\n\n\n\nols_test_correlation(lm_fit2)\n\n[1] 0.9667391\n\n\nthen we see that when the lstat^2 term is included in the model, there is little discernible pattern in the residuals.\nIn order to create a cubic fit, we can include a predictor of the form I(X^3). However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the poly() function to create the polynomial within lm(). For example, the following command produces a fifth-order polynomial fit:\n\nlm_fit5 &lt;- lm(medv ~ poly(lstat, 5))\nmsummary(lm_fit5)\n\n \n\n  \n    \n    \n    tinytable_mf6mlew1ej1u2njfi0bx\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)    \n                  22.533   \n                \n                \n                                 \n                  (0.232)  \n                \n                \n                  poly(lstat, 5)1\n                  -152.460 \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  poly(lstat, 5)2\n                  64.227   \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  poly(lstat, 5)3\n                  -27.051  \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  poly(lstat, 5)4\n                  25.452   \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  poly(lstat, 5)5\n                  -19.252  \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  Num.Obs.       \n                  506      \n                \n                \n                  R2             \n                  0.682    \n                \n                \n                  R2 Adj.        \n                  0.679    \n                \n                \n                  AIC            \n                  3115.2   \n                \n                \n                  BIC            \n                  3144.8   \n                \n                \n                  Log.Lik.       \n                  -1550.624\n                \n                \n                  F              \n                  214.159  \n                \n                \n                  RMSE           \n                  5.18     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nThis suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant \\(p\\)-values in a regression fit.\nBy default, the poly() function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the poly() function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the poly() function, the argument raw = TRUE must be used.\nOf course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.\n\nmsummary(lm(medv ~ log(rm), data = Boston))\n\n \n\n  \n    \n    \n    tinytable_v28p2dzkxm9i4i5xijlr\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  -76.488  \n                \n                \n                             \n                  (5.028)  \n                \n                \n                  log(rm)    \n                  54.055   \n                \n                \n                             \n                  (2.739)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.436    \n                \n                \n                  R2 Adj.    \n                  0.435    \n                \n                \n                  AIC        \n                  3396.8   \n                \n                \n                  BIC        \n                  3409.5   \n                \n                \n                  Log.Lik.   \n                  -1695.424\n                \n                \n                  F          \n                  389.345  \n                \n                \n                  RMSE       \n                  6.90"
  },
  {
    "objectID": "ch03.html#qualitative-predictors",
    "href": "ch03.html#qualitative-predictors",
    "title": "ISLR chapter 3",
    "section": "",
    "text": "We will now examine the Carseats data, which is part of the ISLR2 library. We will attempt to predict Sales (child car seat sales) in \\(400\\) locations based on a number of predictors. Carseats have 400 rows and 11 columns.\n\nhead(Carseats)\n\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1  9.50       138     73          11        276   120       Bad  42        17\n2 11.22       111     48          16        260    83      Good  65        10\n3 10.06       113     35          10        269    80    Medium  59        12\n4  7.40       117    100           4        466    97    Medium  55        14\n5  4.15       141     64           3        340   128       Bad  38        13\n6 10.81       124    113          13        501    72       Bad  78        16\n  Urban  US\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\nCarseats |&gt; janitor::clean_names() |&gt; head()\n\n  sales comp_price income advertising population price shelve_loc age education\n1  9.50        138     73          11        276   120        Bad  42        17\n2 11.22        111     48          16        260    83       Good  65        10\n3 10.06        113     35          10        269    80     Medium  59        12\n4  7.40        117    100           4        466    97     Medium  55        14\n5  4.15        141     64           3        340   128        Bad  38        13\n6 10.81        124    113          13        501    72        Bad  78        16\n  urban  us\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\n\nThe Carseats data includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.\n\nlm_fit &lt;- lm(Sales ~ . + Income:Advertising + Price:Age,\n    data = Carseats)\nmsummary(lm_fit)\n\n \n\n  \n    \n    \n    tinytable_qa0h6vgcsouzmzzdvmxa\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)         \n                  6.576   \n                \n                \n                                      \n                  (1.009) \n                \n                \n                  CompPrice           \n                  0.093   \n                \n                \n                                      \n                  (0.004) \n                \n                \n                  Income              \n                  0.011   \n                \n                \n                                      \n                  (0.003) \n                \n                \n                  Advertising         \n                  0.070   \n                \n                \n                                      \n                  (0.023) \n                \n                \n                  Population          \n                  0.000   \n                \n                \n                                      \n                  (0.000) \n                \n                \n                  Price               \n                  -0.101  \n                \n                \n                                      \n                  (0.007) \n                \n                \n                  ShelveLocGood       \n                  4.849   \n                \n                \n                                      \n                  (0.153) \n                \n                \n                  ShelveLocMedium     \n                  1.953   \n                \n                \n                                      \n                  (0.126) \n                \n                \n                  Age                 \n                  -0.058  \n                \n                \n                                      \n                  (0.016) \n                \n                \n                  Education           \n                  -0.021  \n                \n                \n                                      \n                  (0.020) \n                \n                \n                  UrbanYes            \n                  0.140   \n                \n                \n                                      \n                  (0.112) \n                \n                \n                  USYes               \n                  -0.158  \n                \n                \n                                      \n                  (0.149) \n                \n                \n                  Income × Advertising\n                  0.001   \n                \n                \n                                      \n                  (0.000) \n                \n                \n                  Price × Age         \n                  0.000   \n                \n                \n                                      \n                  (0.000) \n                \n                \n                  Num.Obs.            \n                  400     \n                \n                \n                  R2                  \n                  0.876   \n                \n                \n                  R2 Adj.             \n                  0.872   \n                \n                \n                  AIC                 \n                  1159.3  \n                \n                \n                  BIC                 \n                  1219.2  \n                \n                \n                  Log.Lik.            \n                  -564.669\n                \n                \n                  F                   \n                  209.988 \n                \n                \n                  RMSE                \n                  0.99    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nThe contrasts() function returns the coding that R uses for the dummy variables.\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\nUse ?contrasts to learn about other contrasts, and how to set them.\nR has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location."
  },
  {
    "objectID": "ch03.html#writing-functions",
    "href": "ch03.html#writing-functions",
    "title": "ISLR chapter 3",
    "section": "",
    "text": "R comes with numerous built-in functions, and many more are accessible through R libraries. However, there are times when a specific operation is needed for which no existing function is available. In such cases, we can create our own custom functions. Below, we illustrate this by creating a function called LoadLibraries() that loads the tidyverse, broom, and car packages. Attempting to call the function before its definition results in an error, as shown:\nNext, we define the function. The + symbols are printed by R automatically and should not be typed by the user. The { symbol indicates that a block of commands is about to follow. Pressing Enter after { causes R to display the + prompt, allowing for the input of multiple commands. The block is closed with } to signal the end of the function.\n\nLoadLibraries &lt;- function() {\n  library(tidyverse)\n  library(broom)\n  library(car)\n  library(MASS)\n  library(ISLR2)\n  print(\"The libraries have been loaded.\")\n}\n\nTyping LoadLibraries in the console after defining the function will display its content:\n\nLoadLibraries\n\nfunction() {\n  library(tidyverse)\n  library(broom)\n  library(car)\n  library(MASS)\n  library(ISLR2)\n  print(\"The libraries have been loaded.\")\n}\n\n\nCalling the function will load the specified libraries and display the print message:\n\nLoadLibraries()\n\n[1] \"The libraries have been loaded.\""
  },
  {
    "objectID": "CH2 stat learning.html",
    "href": "CH2 stat learning.html",
    "title": "Ch2.ISLR",
    "section": "",
    "text": "#install.packages(\"ISLR\")\nlibrary(ISLR)\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nAdv&lt;-read.csv(\"Advertising.csv\",header = TRUE) \n\nsummarise(Adv)\n\ndata frame with 0 columns and 1 row\n\npar(mfrow=c(3,1))\n\nggplot(Adv)+aes(TV,Sales)+geom_point(size=2, aes(color=\"Red\"))+stat_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(Adv)+aes(Radio,Sales)+geom_point(size=2, aes(color=\"Red\"))+stat_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(Adv)+aes(Newspaper,Sales)+geom_point(size=2, aes(color=\"Red\"))+stat_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNow we would see which medium of advertisement has stronger relationship with Sales"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Author",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github\n  \n\n  \n  \nI am a Professor at School of Economics, Quaid-i-Azam University, Islamabad.\nMy work is about\nI am interested in developing packages. For example, PakPC, PakNAcc …"
  }
]