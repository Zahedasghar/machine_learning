[
  {
    "objectID": "psx.html",
    "href": "psx.html",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "",
    "text": "In this tutorial, we will explore several machine learning classification models to predict the direction of stock movement for the Pakistan Stock Exchange (PSX) using historical data. We will use models such as Logistic Regression, Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), K-Nearest Neighbors (KNN), and Decision Trees (CART).\nThe goal is to understand some machine learning techniques and how they can be applied to real-world financial data. We will use historical stock data from the PSX. We can understand that predicting movements in stock prices is a challenging task due to the complex nature of financial markets. This is a simplified example to demonstrate how machine learning models can be used in real financial applications. We have created confusion matrix and calculated accuracy, precision, recall, and F1 score to evaluate the performance of each model. We predict whether the stock price will go “Up” or “Down” based on historical changes in stock price and trading volume."
  },
  {
    "objectID": "psx.html#introduction",
    "href": "psx.html#introduction",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "",
    "text": "In this tutorial, we will explore several machine learning classification models to predict the direction of stock movement for the Pakistan Stock Exchange (PSX) using historical data. We will use models such as Logistic Regression, Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), K-Nearest Neighbors (KNN), and Decision Trees (CART).\nThe goal is to understand some machine learning techniques and how they can be applied to real-world financial data. We will use historical stock data from the PSX. We can understand that predicting movements in stock prices is a challenging task due to the complex nature of financial markets. This is a simplified example to demonstrate how machine learning models can be used in real financial applications. We have created confusion matrix and calculated accuracy, precision, recall, and F1 score to evaluate the performance of each model. We predict whether the stock price will go “Up” or “Down” based on historical changes in stock price and trading volume."
  },
  {
    "objectID": "psx.html#loading-and-preparing-the-data",
    "href": "psx.html#loading-and-preparing-the-data",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "1. Loading and Preparing the Data",
    "text": "1. Loading and Preparing the Data\nThe first step is to load the PSX data, clean it, and perform necessary transformations for feature engineering.\n\nlibrary(caret)      # For model evaluation and cross-validation\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(rpart)      # For Decision Trees\nlibrary(tidyverse)  # For data manipulation and visualization\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ purrr::lift()   masks caret::lift()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(Metrics)    # For additional evaluation metrics\n\nWarning: package 'Metrics' was built under R version 4.4.2\n\n\n\nAttaching package: 'Metrics'\n\nThe following objects are masked from 'package:caret':\n\n    precision, recall\n\nlibrary(janitor)    # For cleaning column names\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(class)    # For KNN"
  },
  {
    "objectID": "psx.html#load-data",
    "href": "psx.html#load-data",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "Load data",
    "text": "Load data\nWe have psx data in csv format. We will load the data and inspect its structure.\n\npsx &lt;- read_csv(\"data/psx.csv\")\n\nRows: 1827 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Date, Vol., Change %\ndbl (4): Price, Open, High, Low\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Inspect the data structure (rows, columns, and types)\ndim(psx)\n\n[1] 1827    7\n\nglimpse(psx)\n\nRows: 1,827\nColumns: 7\n$ Date       &lt;chr&gt; \"11/15/2024\", \"11/14/2024\", \"11/13/2024\", \"11/12/2024\", \"11…\n$ Price      &lt;dbl&gt; 17.42, 17.19, 16.78, 16.86, 17.02, 17.22, 17.14, 17.31, 16.…\n$ Open       &lt;dbl&gt; 17.40, 16.70, 16.95, 17.00, 17.46, 17.14, 17.24, 17.24, 17.…\n$ High       &lt;dbl&gt; 17.60, 17.60, 16.95, 17.20, 17.46, 17.49, 17.60, 17.50, 17.…\n$ Low        &lt;dbl&gt; 16.90, 16.55, 16.50, 16.70, 16.97, 17.01, 17.00, 16.60, 16.…\n$ Vol.       &lt;chr&gt; \"896.83K\", \"1.01M\", \"239.44K\", \"179.95K\", \"364.92K\", \"208.0…\n$ `Change %` &lt;chr&gt; \"1.34%\", \"2.44%\", \"-0.47%\", \"-0.94%\", \"-1.16%\", \"0.47%\", \"-…"
  },
  {
    "objectID": "psx.html#data-cleaning-and-feature-engineering",
    "href": "psx.html#data-cleaning-and-feature-engineering",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "Data cleaning and feature engineering",
    "text": "Data cleaning and feature engineering\nWe will clean the data, convert columns to appropriate types, create new features, and handle missing values.\n\n# Convert the `Date` column to a proper Date format\npsx &lt;- psx %&gt;% \n  mutate(date = as.Date(Date, format = \"%m/%d/%Y\")) |&gt; dplyr::select(-Date)\n# Clean column names to make them consistent and readable\npsx &lt;- psx %&gt;% clean_names()\n\n# Convert `change_percent` to numeric and create a new column `direction` for the stock movement (Up/Down)\npsx &lt;- psx %&gt;% \n  mutate(\n    change_percent = as.numeric(str_remove(change_percent, \"%\")),\n    direction = if_else(change_percent &gt; 0, \"Up\", \"Down\")\n  )\n\n# Arrange data by ascending date to maintain temporal order\npsx &lt;- psx %&gt;% arrange(date)\n\n# Create lag variables for `change_percent` (lags of 1 to 5 days)\npsx &lt;- psx %&gt;% \n  mutate(\n    lag1 = lag(change_percent, 1),\n    lag2 = lag(change_percent, 2),\n    lag3 = lag(change_percent, 3),\n    lag4 = lag(change_percent, 4),\n    lag5 = lag(change_percent, 5)\n  )\n\n# Convert `vol` (volume) to numeric, handling the M and K suffixes\npsx &lt;- psx %&gt;% mutate(vol = as.numeric(str_replace_all(vol, c(\"M\" = \"e6\", \"K\" = \"e3\"))))\n\n# Drop rows with missing values (NA)\npsx_clean &lt;- psx %&gt;% drop_na()\n\n# Convert `direction` to numeric for binary classification (1 for \"Up\", 0 for \"Down\")\npsx_clean &lt;- psx_clean %&gt;%\n  mutate(direction = if_else(direction == \"Up\", 1, 0))\n\nThere are 1822 rows in the cleaned dataset after data cleaning and feature engineering.\nExplanation:\n\nWe loaded the dataset and cleaned column names.\nWe transformed the change_percent column from character to numeric.\nWe created a direction column to classify stock movements as “Up” or “Down”.\nLag features are created to provide past data (1-day lag to 5-day lag) as predictors for stock movement.\nWe converted the volume (vol) column to numeric format for use in the model.\nRows with missing values were dropped, and the target variable direction was converted to binary numeric values for classification."
  },
  {
    "objectID": "psx.html#logistic-regression-glm",
    "href": "psx.html#logistic-regression-glm",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "2. Logistic Regression (GLM)",
    "text": "2. Logistic Regression (GLM)\nLogistic regression will model the probability that the stock will go “Up” based on the lag features and volume.\n\n# Fit logistic regression model\n# glm_fit &lt;- glm(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol, \n#                data = psx_clean, family = binomial)\n# \n# # Model summary\n# summary(glm_fit)\n# \n# # Add predicted probabilities to the dataset\n# psx_clean &lt;- psx_clean %&gt;%\n#   mutate(\n#     glm_probs = predict(glm_fit, type = \"response\"),\n#     glm_pred = if_else(glm_probs &gt; 0.5, 1, 0)\n#   )\n# \n# # Evaluate logistic regression performance using confusion matrix and accuracy\n# confusion_matrix &lt;- table(glm_pred = psx_clean$glm_pred, actual = psx_clean$direction)\n# accuracy &lt;- mean(psx_clean$glm_pred == psx_clean$direction)\n\n# Display results\n#print(confusion_matrix)\n#print(paste(\"Overall Accuracy:\", round(accuracy, 3)))\n\nExplanation: 1. The logistic regression model is fit with the lag features and volume as predictors. 2. Predictions are made for each observation using predict(), and the predicted probabilities are added to the dataset. 3. We then classify the probability as “Up” or “Down” based on a threshold of 0.5. 4. The performance is evaluated by comparing predicted values against actual values using a confusion matrix and accuracy."
  },
  {
    "objectID": "psx.html#train-test-split",
    "href": "psx.html#train-test-split",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "3. Train-Test Split",
    "text": "3. Train-Test Split\nWe split the data into training and testing datasets to evaluate the performance of our models.\n\n# Train-test split based on the date\ntrain_data &lt;- psx_clean %&gt;%\n  filter(date &lt; as.Date(\"2024-01-01\"))\n\ntest_data &lt;- psx_clean %&gt;%\n  filter(date &gt;= as.Date(\"2024-01-01\"))\n# \n# # Ensure direction is numeric (0 for Down, 1 for Up)\n# train_data &lt;- train_data %&gt;%\n#   mutate(direction = as.factor(direction))\n# \n# test_data &lt;- test_data %&gt;%\n#   mutate(direction = as.factor(direction))\n# \n# # Ensure consistent factor levels for direction in both training and test data\n# train_data$direction &lt;- factor(train_data$direction, levels = c(0, 1))  # 0 = Down, 1 = Up\n# test_data$direction &lt;- factor(test_data$direction, levels = c(0, 1))"
  },
  {
    "objectID": "psx.html#linear-discriminant-analysis-lda",
    "href": "psx.html#linear-discriminant-analysis-lda",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "Linear Discriminant Analysis (LDA)",
    "text": "Linear Discriminant Analysis (LDA)\n\nlda_fit &lt;- lda(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol,\n               data = train_data)\n\n# Predict on test data\n\nlda_pred &lt;- predict(lda_fit, newdata = test_data)\n\n# Evaluate LDA model\n\nlda_accuracy &lt;- mean(lda_pred$class == test_data$direction)\n\nExplanation: 1. We split the data based on the date (all data before January 1, 2024 for training, and the rest for testing). 2. The target variable direction is converted into a factor to comply with classification model requirements."
  },
  {
    "objectID": "psx.html#linear-discriminant-analysis-lda-1",
    "href": "psx.html#linear-discriminant-analysis-lda-1",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "4. Linear Discriminant Analysis (LDA)",
    "text": "4. Linear Discriminant Analysis (LDA)\nLDA is another classification technique that assumes normality in the predictor variables and tries to separate the classes (Up/Down) by maximizing the ratio of between-class variance to within-class variance.\nError in Ops.factor(lda_pred, test_data$direction) : level sets of factors are different &gt;\nExplanation: 1. We fit the LDA model using the training data. 2. Predictions are made on the test set using the fitted model. 3. The model accuracy is evaluated by comparing predicted and actual values."
  },
  {
    "objectID": "psx.html#quadratic-discriminant-analysis-qda",
    "href": "psx.html#quadratic-discriminant-analysis-qda",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "5. Quadratic Discriminant Analysis (QDA)",
    "text": "5. Quadratic Discriminant Analysis (QDA)\nQDA is similar to LDA but allows for each class to have its own covariance matrix, making it more flexible when the data distribution is not the same across classes.\n\n# Fit QDA model\nqda_fit &lt;- qda(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol, data = train_data)\n\n# Predict on test data\nqda_pred &lt;- predict(qda_fit, newdata = test_data)$class\n\n# Evaluate QDA model performance\nqda_accuracy &lt;- mean(qda_pred == test_data$direction)\n\n# Display QDA results\nprint(paste(\"QDA Accuracy:\", round(qda_accuracy, 3)))\n\n[1] \"QDA Accuracy: 0.605\"\n\n\nExplanation: 1. We fit the QDA model using the training data. 2. Predictions are made on the test set, and the model’s accuracy is evaluated."
  },
  {
    "objectID": "psx.html#k-nearest-neighbors-knn",
    "href": "psx.html#k-nearest-neighbors-knn",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "6. K-Nearest Neighbors (KNN)",
    "text": "6. K-Nearest Neighbors (KNN)\nKNN is a non-parametric method that assigns the class of an observation based on the majority class of its k nearest neighbors in the feature space.\n\n# Fit KNN model\n\nknn_fit &lt;- knn(train = train_data[, c(\"lag1\", \"lag2\", \"lag3\", \"lag4\", \"lag5\", \"vol\")],\n               test = test_data[, c(\"lag1\", \"lag2\", \"lag3\", \"lag4\", \"lag5\", \"vol\")],\n               cl = train_data$direction,\n               k = 5)\n\n# Evaluate KNN model\n\nknn_accuracy &lt;- mean(knn_fit == test_data$direction)\n\nExplanation: 1. Data is normalized to ensure all features have the same scale for KNN. 2. We fit the KNN model with k = 5 and evaluate its accuracy."
  },
  {
    "objectID": "psx.html#decision-trees-cart",
    "href": "psx.html#decision-trees-cart",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "7. Decision Trees (CART)",
    "text": "7. Decision Trees (CART)\nThe Decision Tree algorithm creates a tree-like model of decisions and their possible consequences.\n\n# Fit Decision Tree model (CART)\ndt_fit &lt;- rpart(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol,\n                 data = train_data,\n                 method = \"class\")\n\n# Predict on test data\n\ndt_pred &lt;- predict(dt_fit, newdata = test_data, type = \"class\")\n\n\n# Evaluate Decision Tree model\n\ndt_accuracy &lt;- mean(dt_pred == test_data$direction)\n\nExplanation: 1. We fit the Decision Tree (CART) model using the training data. 2. Predictions are made on the test set, and the accuracy is calculated."
  },
  {
    "objectID": "psx.html#model-evaluation",
    "href": "psx.html#model-evaluation",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "8. Model Evaluation",
    "text": "8. Model Evaluation\nWe evaluate the performance of each model using accuracy, which is the proportion of correct predictions over the total number of predictions."
  },
  {
    "objectID": "psx.html#output-results",
    "href": "psx.html#output-results",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "Output results",
    "text": "Output results\n\nlist( glm_accuracy = glm_test_accuracy,\n  lda_accuracy = lda_accuracy,\n  qda_accuracy = qda_accuracy,\n  knn_accuracy = knn_accuracy,\n  dt_accuracy = dt_accuracy\n)\n\n$glm_accuracy\n[1] 0.6139535\n\n$lda_accuracy\n[1] 0.6325581\n\n$qda_accuracy\n[1] 0.6046512\n\n$knn_accuracy\n[1] 0.5348837\n\n$dt_accuracy\n[1] 0.5813953"
  },
  {
    "objectID": "psx.html#conclusion",
    "href": "psx.html#conclusion",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "Conclusion",
    "text": "Conclusion\nWe have successfully implemented various machine learning classification models to predict stock movement direction. The models were evaluated using accuracy, and further improvements can be made by tuning hyperparameters, adding more features, or using ensemble methods.\nEach model has its strengths and weaknesses, and the results from this tutorial can be used to guide further research or model enhancement."
  },
  {
    "objectID": "gapminder_map_fn.html",
    "href": "gapminder_map_fn.html",
    "title": "gapminder_map_function",
    "section": "",
    "text": "The provided code chunks are in R and make use of tidyverse packages to work with the gapminder dataset and apply a linear model (lm) to analyze life expectancy as a function of year for each continent.\nHere’s a breakdown of what the code does:\n\nLoad necessary libraries:\n\n\n   library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nLoad and clean the gapminder data:\n\n\n   dat_gapminder &lt;- gapminder::gapminder |&gt;\n     janitor::clean_names()\n\n\nThis creates a data frame dat_gapminder from the gapminder dataset and standardizes column names using janitor::clean_names().\n\n\nApply a linear model using map:\n\n\n   dat_gapminder |&gt;\n     nest(.by = continent) |&gt;\n     mutate(\n       lin_mod = map(\n         data,\n         ~ lm(life_exp ~ year, data = .x)\n       )\n     )\n\n# A tibble: 5 × 3\n  continent data               lin_mod\n  &lt;fct&gt;     &lt;list&gt;             &lt;list&gt; \n1 Asia      &lt;tibble [396 × 5]&gt; &lt;lm&gt;   \n2 Europe    &lt;tibble [360 × 5]&gt; &lt;lm&gt;   \n3 Africa    &lt;tibble [624 × 5]&gt; &lt;lm&gt;   \n4 Americas  &lt;tibble [300 × 5]&gt; &lt;lm&gt;   \n5 Oceania   &lt;tibble [24 × 5]&gt;  &lt;lm&gt;   \n\n\n\nnest(.by = continent): Groups and nests the data by the continent column, creating a list-column with data frames for each continent.\nmutate() creates a new column, lin_mod, where the map() function is applied.\nmap() iterates over each nested data frame (data) and applies the lm(life_exp ~ year, data = .x) function, fitting a linear model for life expectancy as a function of year.\n\nThis process results in a data frame with a column that stores the fitted linear model for each continent. To extract coefficients (intercept and slope) from these models, an additional step using map() combined with broom::tidy() or coef() can be added:\n\ndat_gapminder |&gt;\n  nest(.by = country) |&gt;\n  mutate(\n    lin_mod = map(\n      data,\n      \\(x) lm(life_exp ~ year, data = x)\n    ),\n    coeffs = map(\n      lin_mod,\n      coefficients\n    ),\n    slope = map_dbl(coeffs, \\(x) x[2]),\n    intercept = map_dbl(coeffs, \\(x) x[1])\n  )\n\n# A tibble: 142 × 6\n   country     data              lin_mod coeffs    slope intercept\n   &lt;fct&gt;       &lt;list&gt;            &lt;list&gt;  &lt;list&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan &lt;tibble [12 × 5]&gt; &lt;lm&gt;    &lt;dbl [2]&gt; 0.275     -508.\n 2 Albania     &lt;tibble [12 × 5]&gt; &lt;lm&gt;    &lt;dbl [2]&gt; 0.335     -594.\n 3 Algeria     &lt;tibble [12 × 5]&gt; &lt;lm&gt;    &lt;dbl [2]&gt; 0.569    -1068.\n 4 Angola      &lt;tibble [12 × 5]&gt; &lt;lm&gt;    &lt;dbl [2]&gt; 0.209     -377.\n 5 Argentina   &lt;tibble [12 × 5]&gt; &lt;lm&gt;    &lt;dbl [2]&gt; 0.232     -390.\n 6 Australia   &lt;tibble [12 × 5]&gt; &lt;lm&gt;    &lt;dbl [2]&gt; 0.228     -376.\n 7 Austria     &lt;tibble [12 × 5]&gt; &lt;lm&gt;    &lt;dbl [2]&gt; 0.242     -406.\n 8 Bahrain     &lt;tibble [12 × 5]&gt; &lt;lm&gt;    &lt;dbl [2]&gt; 0.468     -860.\n 9 Bangladesh  &lt;tibble [12 × 5]&gt; &lt;lm&gt;    &lt;dbl [2]&gt; 0.498     -936.\n10 Belgium     &lt;tibble [12 × 5]&gt; &lt;lm&gt;    &lt;dbl [2]&gt; 0.209     -340.\n# ℹ 132 more rows"
  },
  {
    "objectID": "Ch1.html",
    "href": "Ch1.html",
    "title": "ISLR Ch.1",
    "section": "",
    "text": "library(dplyr) library(ggplot2) install.packages(“ISLR”) library(ISLR)"
  },
  {
    "objectID": "ch02_10.html",
    "href": "ch02_10.html",
    "title": "Chapter 2 - Exercise 10",
    "section": "",
    "text": "This exercise involves the Boston housing data set. (a) To begin, load in the Boston data set. The Boston data set is part of the ISLR2 library. Now the data set is contained in the object Boston.\nRead about the data set:\n\np_01\n\nlibrary(ISLR2)\nlibrary(tidyverse)\nlibrary(MASS)\n\ndata(\"Boston\")\n#?Boston\n\n\n\np_0b\nHow many rows are in this data set? How many columns? What do the rows and columns represent?\n\ndim(Boston)\n\n[1] 506  14\n\nglimpse(Boston)\n\nRows: 506\nColumns: 14\n$ crim    &lt;dbl&gt; 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829,…\n$ zn      &lt;dbl&gt; 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 12.5, 1…\n$ indus   &lt;dbl&gt; 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87, 7.…\n$ chas    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ nox     &lt;dbl&gt; 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524,…\n$ rm      &lt;dbl&gt; 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, 6.012, 6.172, 5.631,…\n$ age     &lt;dbl&gt; 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9, 9…\n$ dis     &lt;dbl&gt; 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5605, 5.9505…\n$ rad     &lt;int&gt; 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ tax     &lt;dbl&gt; 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 311, 31…\n$ ptratio &lt;dbl&gt; 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2, 15…\n$ black   &lt;dbl&gt; 396.90, 396.90, 392.83, 394.63, 396.90, 394.12, 395.60, 396.90…\n$ lstat   &lt;dbl&gt; 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.10…\n$ medv    &lt;dbl&gt; 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15…\n\n\n\n\np_1b\nMake some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.\n\npairs(Boston)\n\n\n\n\n\n\n\n\nAlso have correlation matrix in heatmap with labels\n\ncor(Boston)\n\n               crim          zn       indus         chas         nox\ncrim     1.00000000 -0.20046922  0.40658341 -0.055891582  0.42097171\nzn      -0.20046922  1.00000000 -0.53382819 -0.042696719 -0.51660371\nindus    0.40658341 -0.53382819  1.00000000  0.062938027  0.76365145\nchas    -0.05589158 -0.04269672  0.06293803  1.000000000  0.09120281\nnox      0.42097171 -0.51660371  0.76365145  0.091202807  1.00000000\nrm      -0.21924670  0.31199059 -0.39167585  0.091251225 -0.30218819\nage      0.35273425 -0.56953734  0.64477851  0.086517774  0.73147010\ndis     -0.37967009  0.66440822 -0.70802699 -0.099175780 -0.76923011\nrad      0.62550515 -0.31194783  0.59512927 -0.007368241  0.61144056\ntax      0.58276431 -0.31456332  0.72076018 -0.035586518  0.66802320\nptratio  0.28994558 -0.39167855  0.38324756 -0.121515174  0.18893268\nblack   -0.38506394  0.17552032 -0.35697654  0.048788485 -0.38005064\nlstat    0.45562148 -0.41299457  0.60379972 -0.053929298  0.59087892\nmedv    -0.38830461  0.36044534 -0.48372516  0.175260177 -0.42732077\n                 rm         age         dis          rad         tax    ptratio\ncrim    -0.21924670  0.35273425 -0.37967009  0.625505145  0.58276431  0.2899456\nzn       0.31199059 -0.56953734  0.66440822 -0.311947826 -0.31456332 -0.3916785\nindus   -0.39167585  0.64477851 -0.70802699  0.595129275  0.72076018  0.3832476\nchas     0.09125123  0.08651777 -0.09917578 -0.007368241 -0.03558652 -0.1215152\nnox     -0.30218819  0.73147010 -0.76923011  0.611440563  0.66802320  0.1889327\nrm       1.00000000 -0.24026493  0.20524621 -0.209846668 -0.29204783 -0.3555015\nage     -0.24026493  1.00000000 -0.74788054  0.456022452  0.50645559  0.2615150\ndis      0.20524621 -0.74788054  1.00000000 -0.494587930 -0.53443158 -0.2324705\nrad     -0.20984667  0.45602245 -0.49458793  1.000000000  0.91022819  0.4647412\ntax     -0.29204783  0.50645559 -0.53443158  0.910228189  1.00000000  0.4608530\nptratio -0.35550149  0.26151501 -0.23247054  0.464741179  0.46085304  1.0000000\nblack    0.12806864 -0.27353398  0.29151167 -0.444412816 -0.44180801 -0.1773833\nlstat   -0.61380827  0.60233853 -0.49699583  0.488676335  0.54399341  0.3740443\nmedv     0.69535995 -0.37695457  0.24992873 -0.381626231 -0.46853593 -0.5077867\n              black      lstat       medv\ncrim    -0.38506394  0.4556215 -0.3883046\nzn       0.17552032 -0.4129946  0.3604453\nindus   -0.35697654  0.6037997 -0.4837252\nchas     0.04878848 -0.0539293  0.1752602\nnox     -0.38005064  0.5908789 -0.4273208\nrm       0.12806864 -0.6138083  0.6953599\nage     -0.27353398  0.6023385 -0.3769546\ndis      0.29151167 -0.4969958  0.2499287\nrad     -0.44441282  0.4886763 -0.3816262\ntax     -0.44180801  0.5439934 -0.4685359\nptratio -0.17738330  0.3740443 -0.5077867\nblack    1.00000000 -0.3660869  0.3334608\nlstat   -0.36608690  1.0000000 -0.7376627\nmedv     0.33346082 -0.7376627  1.0000000\n\ncor(Boston) %&gt;% \n  corrplot::corrplot()\n\n\n\n\n\n\n\n\nHave correlation between crim with other variables\n\n# Assuming Boston dataset is loaded and available\n# Calculate the correlation matrix\ncor_matrix &lt;- cor(Boston)\n\n# Extract the correlation of 'crim' with all other variables\ncrim_correlations &lt;- cor_matrix[\"crim\", ]\n\n# Sort the correlations in descending order\nsorted_crim_correlations &lt;- sort(crim_correlations, decreasing = TRUE)\n\n# Display the sorted correlations\nprint(sorted_crim_correlations)\n\n       crim         rad         tax       lstat         nox       indus \n 1.00000000  0.62550515  0.58276431  0.45562148  0.42097171  0.40658341 \n        age     ptratio        chas          zn          rm         dis \n 0.35273425  0.28994558 -0.05589158 -0.20046922 -0.21924670 -0.37967009 \n      black        medv \n-0.38506394 -0.38830461 \n\n\n\n\np_0c\nAre any of the predictors associated with per capita crime rate? If so, explain the relationship.\n\n\np_0d\nDo any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.\n\nBoston %&gt;% \n  dplyr::select(crim, tax, ptratio) %&gt;% \n  summary()\n\n      crim               tax           ptratio     \n Min.   : 0.00632   Min.   :187.0   Min.   :12.60  \n 1st Qu.: 0.08205   1st Qu.:279.0   1st Qu.:17.40  \n Median : 0.25651   Median :330.0   Median :19.05  \n Mean   : 3.61352   Mean   :408.2   Mean   :18.46  \n 3rd Qu.: 3.67708   3rd Qu.:666.0   3rd Qu.:20.20  \n Max.   :88.97620   Max.   :711.0   Max.   :22.00  \n\n\n\nlibrary(tidyverse)\n\n# Assuming the Boston dataset is available\n# Summarize the range and identify high values for 'crim', 'tax', and 'ptratio'\nsummary_stats &lt;- Boston %&gt;%\n  summarise(\n    crim_min = min(crim),\n    crim_max = max(crim),\n    tax_min = min(tax),\n    tax_max = max(tax),\n    ptratio_min = min(ptratio),\n    ptratio_max = max(ptratio)\n  )\n\n# Identify high values based on chosen thresholds (e.g., top 5% as high)\nhigh_values &lt;- Boston %&gt;%\n  summarise(\n    high_crim = quantile(crim, 0.95),\n    high_tax = quantile(tax, 0.95),\n    high_ptratio = quantile(ptratio, 0.95)\n  ) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"threshold\")\n\n# Find the census tracts with particularly high values\nhigh_tracts &lt;- Boston %&gt;%\n  filter(\n    crim &gt; high_values$threshold[high_values$variable == \"high_crim\"] |\n    tax &gt; high_values$threshold[high_values$variable == \"high_tax\"] |\n    ptratio &gt; high_values$threshold[high_values$variable == \"high_ptratio\"]\n  ) %&gt;%\n  dplyr::select(crim, tax, ptratio)\n\n# Print the range of each predictor\nprint(\"Summary of Ranges:\")\n\n[1] \"Summary of Ranges:\"\n\nprint(summary_stats)\n\n  crim_min crim_max tax_min tax_max ptratio_min ptratio_max\n1  0.00632  88.9762     187     711        12.6          22\n\n# Print the high value thresholds for each variable\nprint(\"Thresholds for High Values (Top 5%):\")\n\n[1] \"Thresholds for High Values (Top 5%):\"\n\nprint(high_values)\n\n# A tibble: 3 × 2\n  variable     threshold\n  &lt;chr&gt;            &lt;dbl&gt;\n1 high_crim         15.8\n2 high_tax         666  \n3 high_ptratio      21  \n\n# Display census tracts with high values\nprint(\"Census Tracts with High Values for 'crim', 'tax', or 'ptratio':\")\n\n[1] \"Census Tracts with High Values for 'crim', 'tax', or 'ptratio':\"\n\nprint(head(high_tracts))\n\n     crim tax ptratio\n1 0.01360 469    21.1\n2 0.25915 437    21.2\n3 0.32543 437    21.2\n4 0.88125 437    21.2\n5 0.34006 437    21.2\n6 1.19294 437    21.2\n\n\n\n\np_0e\nHow many of the census tracts in this data set bound the Charles river?\n\nBoston %&gt;%\n  filter(chas == 1) %&gt;%\n  nrow()\n\n[1] 35\n\n\n\n\np_0f\nWhat is the median pupil-teacher ratio among the towns in this data set?\n\nBoston %&gt;%\n  summarise(median_ptratio = median(ptratio))\n\n  median_ptratio\n1          19.05\n\n\n\n\np_0g\nWhich census tract of Boston has lowest median value of owner occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings.\n\n# Find the census tract with the lowest median value of owner-occupied homes\nlowest_medv_tract &lt;- Boston %&gt;%\n  filter(medv == min(medv)) %&gt;%\n  dplyr::select(everything())\n\n# Summarize the range of each predictor in the dataset\noverall_ranges &lt;- Boston %&gt;%\n  summarise(across(everything(), list(min = min, max = max)))\n\n# Display the census tract with the lowest 'medv'\nprint(\"Census Tract with the Lowest Median Value of Owner-Occupied Homes:\")\n\n[1] \"Census Tract with the Lowest Median Value of Owner-Occupied Homes:\"\n\nprint(lowest_medv_tract)\n\n     crim zn indus chas   nox    rm age    dis rad tax ptratio  black lstat\n1 38.3518  0  18.1    0 0.693 5.453 100 1.4896  24 666    20.2 396.90 30.59\n2 67.9208  0  18.1    0 0.693 5.683 100 1.4254  24 666    20.2 384.97 22.98\n  medv\n1    5\n2    5\n\n# Display the overall ranges for comparison\nprint(\"Overall Ranges for Each Predictor:\")\n\n[1] \"Overall Ranges for Each Predictor:\"\n\nprint(overall_ranges)\n\n  crim_min crim_max zn_min zn_max indus_min indus_max chas_min chas_max nox_min\n1  0.00632  88.9762      0    100      0.46     27.74        0        1   0.385\n  nox_max rm_min rm_max age_min age_max dis_min dis_max rad_min rad_max tax_min\n1   0.871  3.561   8.78     2.9     100  1.1296 12.1265       1      24     187\n  tax_max ptratio_min ptratio_max black_min black_max lstat_min lstat_max\n1     711        12.6          22      0.32     396.9      1.73     37.97\n  medv_min medv_max\n1        5       50\n\n\n\n\np_0h\nIn this data set, how many of the census tracts average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling.\n\n# Count the number of census tracts with more than 7 rooms per dwelling\ncount_more_than_7 &lt;- Boston %&gt;%\n  filter(rm &gt; 7) %&gt;%\n  summarise(count = n())\n\n# Count the number of census tracts with more than 8 rooms per dwelling\ncount_more_than_8 &lt;- Boston %&gt;%\n  filter(rm &gt; 8) %&gt;%\n  summarise(count = n())\n\n# Display the results\nprint(\"Number of census tracts averaging more than 7 rooms per dwelling:\")\n\n[1] \"Number of census tracts averaging more than 7 rooms per dwelling:\"\n\nprint(count_more_than_7)\n\n  count\n1    64\n\nprint(\"Number of census tracts averaging more than 8 rooms per dwelling:\")\n\n[1] \"Number of census tracts averaging more than 8 rooms per dwelling:\"\n\nprint(count_more_than_8)\n\n  count\n1    13\n\n# Display details of census tracts with more than 8 rooms per dwelling\ntracts_more_than_8 &lt;- Boston %&gt;%\n  filter(rm &gt; 8) %&gt;%\n  dplyr::select(everything())\n\nprint(\"Details of census tracts averaging more than 8 rooms per dwelling:\")\n\n[1] \"Details of census tracts averaging more than 8 rooms per dwelling:\"\n\nprint(tracts_more_than_8)\n\n      crim zn indus chas    nox    rm  age    dis rad tax ptratio  black lstat\n1  0.12083  0  2.89    0 0.4450 8.069 76.0 3.4952   2 276    18.0 396.90  4.21\n2  1.51902  0 19.58    1 0.6050 8.375 93.9 2.1620   5 403    14.7 388.45  3.32\n3  0.02009 95  2.68    0 0.4161 8.034 31.9 5.1180   4 224    14.7 390.55  2.88\n4  0.31533  0  6.20    0 0.5040 8.266 78.3 2.8944   8 307    17.4 385.05  4.14\n5  0.52693  0  6.20    0 0.5040 8.725 83.0 2.8944   8 307    17.4 382.00  4.63\n6  0.38214  0  6.20    0 0.5040 8.040 86.5 3.2157   8 307    17.4 387.38  3.13\n7  0.57529  0  6.20    0 0.5070 8.337 73.3 3.8384   8 307    17.4 385.91  2.47\n8  0.33147  0  6.20    0 0.5070 8.247 70.4 3.6519   8 307    17.4 378.95  3.95\n9  0.36894 22  5.86    0 0.4310 8.259  8.4 8.9067   7 330    19.1 396.90  3.54\n10 0.61154 20  3.97    0 0.6470 8.704 86.9 1.8010   5 264    13.0 389.70  5.12\n11 0.52014 20  3.97    0 0.6470 8.398 91.5 2.2885   5 264    13.0 386.86  5.91\n12 0.57834 20  3.97    0 0.5750 8.297 67.0 2.4216   5 264    13.0 384.54  7.44\n13 3.47428  0 18.10    1 0.7180 8.780 82.9 1.9047  24 666    20.2 354.55  5.29\n   medv\n1  38.7\n2  50.0\n3  50.0\n4  44.8\n5  50.0\n6  37.6\n7  41.7\n8  48.3\n9  42.8\n10 50.0\n11 48.8\n12 50.0\n13 21.9"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Author",
    "section": "",
    "text": "Template for about page\n\n\n\nIt is possible to use different templates for the About page. Check this link to find out more."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Author",
    "section": "Education",
    "text": "Education\nPakistan Institute of Development Economics | Islamabad, Pakistan\nPhD in Economics | July 2024\nBZU | Multan, Pakistan\nMSc in Statistics"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Author",
    "section": "Experience",
    "text": "Experience\nI have published widely both in Applied Economics and Statistics both in national and international journals.\nI have also been serving as a trainer for different organizations in the field of data analysis and econometrics. My expertise is in R, Stata, Python, EVIEWS, and Excel."
  },
  {
    "objectID": "ch03.html",
    "href": "ch03.html",
    "title": "ISLR chapter 3",
    "section": "",
    "text": "This is a little modified version of the code from the book Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. The code is written in R and I have used the tidyverse package to make it more readable and easy to understand instead of using the base R functions.\nThe first thing we need to do is install and then load the tidyverse set of R packages to provide us with lots of extra functionality. You only need to install this once: once it’s installed we can simply load it into the workspace using the library(packagename) function each time we open a new R session.\nUnderstanding data sets requires many hours/days or in some cases weeks.There are many commercially available software but open source community based software have now dominated and R is one of these. Here I also load the MASS package, which is a very large collection of data sets and functions. We also load the ISLR2 package, which includes the data sets associated with the book Introduction to Statistical Learning.\n\nlibrary(MASS)\nlibrary(ISLR2)\nlibrary(tidyverse)\n\n\n\n\nHere’s a tabular format representing the meta data for the Boston data set from the ISLR2 library. This table provides the column name, a brief description, and the type of data in each column.\n\n\n\n\n\n\n\n\nColumn Name\nDescription\nData Type\n\n\n\n\ncrim\nPer capita crime rate by town\nNumeric\n\n\nzn\nProportion of residential land zoned for lots over 25,000 sq. ft.\nNumeric\n\n\nindus\nProportion of non-retail business acres per town\nNumeric\n\n\nchas\nCharles River dummy variable (1 if tract bounds river; 0 otherwise)\nCategorical\n\n\nnox\nNitrogen oxides concentration (parts per 10 million)\nNumeric\n\n\nrm\nAverage number of rooms per dwelling\nNumeric\n\n\nage\nProportion of owner-occupied units built prior to 1940\nNumeric\n\n\ndis\nWeighted distances to five Boston employment centers\nNumeric\n\n\nrad\nIndex of accessibility to radial highways\nCategorical\n\n\ntax\nFull-value property-tax rate per $10,000\nNumeric\n\n\nptratio\nPupil-teacher ratio by town\nNumeric\n\n\nblack\n1000(Bk - 0.63)^2 where Bk is the proportion of Black people by town\nNumeric\n\n\nlstat\nPercentage of lower status of the population\nNumeric\n\n\nmedv\nMedian value of owner-occupied homes in $1000’s\nNumeric\n\n\n\nThis meta data outlines the features available in the Boston dataset that can be used to predict the medv variable.\n\nhead(Boston)\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat medv\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98 24.0\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14 21.6\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03 34.7\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7  2.94 33.4\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7  5.33 36.2\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7  5.21 28.7\n\n\nTo find out more about the data set, we can type ?Boston.\nWe will start by using the lm() function to fit a simple linear regression model, with medv as the response and lstat as the predictor. The basic syntax is lm(y ~ x, data), where y is the response, x is the predictor, and data is the data set in which these two variables are kept.\n\nlm_fit &lt;- lm(medv ~ lstat)\n\nError in eval(predvars, data, env): object 'medv' not found\n\n\nThe command causes an error because R does not know where to find the variables medv and lstat. The next line tells R that the variables are in Boston. If we attach Boston, the first line works fine because R now recognizes the variables.\n\nlm_fit &lt;- lm(medv ~ lstat, data = Boston)\nattach(Boston)  # attach the Boston data set\nlm_fit &lt;- lm(medv ~ lstat) # fit the model with data already attached\n\nIf we type lm_fit, some basic information about the model is output. For more detailed information, one may use summary(lm_fit) and I am using here modelsummary package to present summary in a more readable format. The summary() function outputs the coefficients of the model as well as their standard errors, \\(t\\)-statistics, and \\(p\\)-values. The summary() function also outputs the \\(R^2\\) statistic and an analysis of variance (ANOVA) table, which breaks down the variance associated with the regression model and the residuals. This gives us \\(p\\)-values and standard errors for the coefficients, as well as the \\(R^2\\) statistic and \\(F\\)-statistic for the model.\n\nlibrary(modelsummary)\n\n`modelsummary` 2.0.0 now uses `tinytable` as its default table-drawing\n  backend. Learn more at: https://vincentarelbundock.github.io/tinytable/\n\nRevert to `kableExtra` for one session:\n\n  options(modelsummary_factory_default = 'kableExtra')\n  options(modelsummary_factory_latex = 'kableExtra')\n  options(modelsummary_factory_html = 'kableExtra')\n\nSilence this message forever:\n\n  config_modelsummary(startup_message = FALSE)\n\nmsummary(lm_fit)\n\n \n\n  \n    \n    \n    tinytable_aqnu32b4kv5m5p83ubbl\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  34.554   \n                \n                \n                             \n                  (0.563)  \n                \n                \n                  lstat      \n                  -0.950   \n                \n                \n                             \n                  (0.039)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.544    \n                \n                \n                  R2 Adj.    \n                  0.543    \n                \n                \n                  AIC        \n                  3289.0   \n                \n                \n                  BIC        \n                  3301.7   \n                \n                \n                  Log.Lik.   \n                  -1641.487\n                \n                \n                  F          \n                  601.618  \n                \n                \n                  RMSE       \n                  6.20     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nOne may extract the coefficients of the model using the coef() function. The names() function can be used to extract the names of the coefficients.\n\nnames(lm_fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm_fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n\nFor confidence intervals, we can use the confint() function. By default, confint() provides 95 % confidence intervals; however, this can be changed using the level argument.\n\nconfint(lm_fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\nTo predict the median house value for a given percentage of lower status of the population, we can use the predict() function. The predict() function can be used to produce confidence intervals and prediction intervals for the prediction.\n\npredict(lm_fit, data.frame(lstat = (c(5, 10, 15))),\n        interval = \"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm_fit, data.frame(lstat = (c(5, 10, 15))),\n        interval = \"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n\n\nlibrary(modelsummary)\nlibrary(broom)\n\n# Fit the linear model\nlm_fit &lt;- lm(medv ~ lstat, data = Boston)\n\n# Create predictions with confidence intervals\npredictions &lt;- predict(lm_fit, newdata = data.frame(lstat = c(5, 10, 15)), interval = \"confidence\")\n\n# Convert predictions to a data frame\npred_df &lt;- as.data.frame(predictions)\npred_df$lstat &lt;- c(5, 10, 15)\n\nlibrary(gt)\npred_df |&gt; gt() %&gt;%\n  tab_header(title = \"Predicted Values with Confidence Intervals\")\n\n\n\n\n\n\n\n\nPredicted Values with Confidence Intervals\n\n\nfit\nlwr\nupr\nlstat\n\n\n\n\n29.80359\n29.00741\n30.59978\n5\n\n\n25.05335\n24.47413\n25.63256\n10\n\n\n20.30310\n19.73159\n20.87461\n15\n\n\n\n\n\n\n\n\nFor instance, the 95 % confidence interval associated with a lstat value of 10 is \\((24.47, 25.63)\\), and the 95 % prediction interval is \\((12.828, 37.28)\\). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of \\(25.05\\) for medv when lstat equals 10), but the latter are substantially wider.\nWe will now plot medv and lstat along with the least squares regression line using ggplot2.\n\nggplot(Boston, aes(x = lstat, y = medv)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere is some evidence for non-linearity in the relationship between lstat and medv. We will explore this issue later in this lab.\n\nggplot(Boston, aes(x = lstat, y = medv)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_abline(intercept = 34, slope = -1, color = \"red\", size = 3) +\n  geom_point(color = \"red\") +\n  geom_point(shape = 20) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNext we examine some diagnostic plots, several of which were discussed in Section 3.3.3. Four diagnostic plots are automatically produced by applying the plot() function directly to the output from lm(). In general, this command will produce one plot at a time, and hitting Enter will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the par() and mfrow() functions, which tell R to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, par(mfrow = c(2, 2)) divides the plotting region into a \\(2 \\times 2\\) grid of panels.\n\n# Load necessary libraries\n\nlibrary(broom)\n\n# Fit the linear model\nlm_fit &lt;- lm(medv ~ lstat, data = Boston)\n\n# Extract augmented data for diagnostics\naugmented_data &lt;- augment(lm_fit)\n\n\n# Residuals vs Fitted Plot\nggplot(augmented_data, aes(.fitted, .resid)) +\n  geom_point() +\n  geom_smooth(method = 'loess', col = 'blue', se = FALSE) +\n  labs(title = \"Residuals vs Fitted\", x = \"Fitted values\", y = \"Residuals\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Normal Q-Q Plot\nggplot(augmented_data, aes(sample = .std.resid)) +\n  geom_qq() +\n  geom_qq_line() +\n  labs(title = \"Normal Q-Q\", x = \"Theoretical Quantiles\", y = \"Standardized Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Scale-Location Plot (Spread of residuals)\nggplot(augmented_data, aes(.fitted, sqrt(abs(.std.resid)))) +\n  geom_point() +\n  geom_smooth(method = 'loess', col = 'blue', se = FALSE) +\n  labs(title = \"Scale-Location\", x = \"Fitted values\", y = \"Sqrt(|Standardized Residuals|)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Residuals vs Leverage Plot\nggplot(augmented_data, aes(.hat, .std.resid)) +\n  geom_point() +\n  geom_smooth(method = 'loess', col = 'blue', se = FALSE) +\n  geom_hline(yintercept = c(-3, 3), linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residuals vs Leverage\", x = \"Leverage\", y = \"Standardized Residuals\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Highlight influential points\n# Replace .rownames with row_number if needed\n# Add a row number column to identify rows\naugmented_data &lt;- augmented_data %&gt;%\n  mutate(row_id = row_number())\n\n# Identify influential points using the row_id\ninfluential_points &lt;- augmented_data %&gt;%\n  filter(.hat &gt; 2 * mean(.hat)) %&gt;%\n  pull(row_id)\n\n# Print the identified influential points\nprint(influential_points)\n\n [1]   9  33  49 124 127 142 143 144 145 146 148 149 215 374 375 385 386 387 388\n[20] 389 393 399 400 401 405 409 413 415 416 417 418 438 439 491\n\nprint(influential_points)\n\n [1]   9  33  49 124 127 142 143 144 145 146 148 149 215 374 375 385 386 387 388\n[20] 389 393 399 400 401 405 409 413 415 416 417 418 438 439 491\n\n\nOn the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the hatvalues() function.\n\nleverage &lt;- hatvalues(lm_fit)\n\nThe which.max() function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.\n\nwhich.max(leverage)\n\n375 \n375 \n\n\n\n\n\nMutiple linear regression is an extension of simple linear regression to the case of two or more predictors. Each predictor has a regression coefficient (i.e. a slope), and there is one intercept. The model is given by\n\\[ Y_i = \\beta_0 + \\beta_1 X_1i + \\beta_2 X_2i + \\ldots + \\beta_p X_pi + \\epsilon_i. \\]\n\\[ Y_i = \\beta_0 + \\beta_1 X_1i + \\beta_2 X_2i + \\ldots + \\beta_p X_pi + \\epsilon_i. \\] The summary() function produces a detailed summary of the regression fit. I will use modelsummary package to produce a more readable output.\n\nlm_fit &lt;- lm(medv ~ lstat + age, data = Boston)\nlibrary(modelsummary)\n\nmodelsummary(lm_fit, estimate = \"std.error\", stars = TRUE)\n\n \n\n  \n    \n    \n    tinytable_y0wwvmf6m7jgyuk0gd6t\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  0.731*** \n                \n                \n                             \n                  (0.731)  \n                \n                \n                  lstat      \n                  0.048*** \n                \n                \n                             \n                  (0.048)  \n                \n                \n                  age        \n                  0.012**  \n                \n                \n                             \n                  (0.012)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.551    \n                \n                \n                  R2 Adj.    \n                  0.549    \n                \n                \n                  AIC        \n                  3283.0   \n                \n                \n                  BIC        \n                  3299.9   \n                \n                \n                  Log.Lik.   \n                  -1637.503\n                \n                \n                  F          \n                  308.969  \n                \n                \n                  RMSE       \n                  6.15     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nThe Boston data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:\n\nlm_fit &lt;- lm(medv ~ ., data = Boston)\nmsummary(lm_fit, estimate = \"std.error\", stars = TRUE)\n\n \n\n  \n    \n    \n    tinytable_rtim97tkxk9igoa2jdiv\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  4.936*** \n                \n                \n                             \n                  (4.936)  \n                \n                \n                  crim       \n                  0.033*** \n                \n                \n                             \n                  (0.033)  \n                \n                \n                  zn         \n                  0.014*** \n                \n                \n                             \n                  (0.014)  \n                \n                \n                  indus      \n                  0.062    \n                \n                \n                             \n                  (0.062)  \n                \n                \n                  chas       \n                  0.870**  \n                \n                \n                             \n                  (0.870)  \n                \n                \n                  nox        \n                  3.851*** \n                \n                \n                             \n                  (3.851)  \n                \n                \n                  rm         \n                  0.420*** \n                \n                \n                             \n                  (0.420)  \n                \n                \n                  age        \n                  0.013    \n                \n                \n                             \n                  (0.013)  \n                \n                \n                  dis        \n                  0.202*** \n                \n                \n                             \n                  (0.202)  \n                \n                \n                  rad        \n                  0.067*** \n                \n                \n                             \n                  (0.067)  \n                \n                \n                  tax        \n                  0.004*** \n                \n                \n                             \n                  (0.004)  \n                \n                \n                  ptratio    \n                  0.132*** \n                \n                \n                             \n                  (0.132)  \n                \n                \n                  lstat      \n                  0.051*** \n                \n                \n                             \n                  (0.051)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.734    \n                \n                \n                  R2 Adj.    \n                  0.728    \n                \n                \n                  AIC        \n                  3037.8   \n                \n                \n                  BIC        \n                  3097.0   \n                \n                \n                  Log.Lik.   \n                  -1504.910\n                \n                \n                  F          \n                  113.544  \n                \n                \n                  RMSE       \n                  4.74     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nWe can access the individual components of a summary object by name. The names() function can be used to obtain the names of the components. The summary() function returns a list with components such as call, terms, residuals, coefficients, aliased, sigma, df, r.squared, adj.r.squared, fstatistic, cov.unscaled, na.action. The vif() function, part of the car package, can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data. The car package is not part of the base R installation so it must be downloaded the first time you use it via the install.packages() function in R.\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nWhat if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high \\(p\\)-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except age.\n\nlm_fit1 &lt;- lm(medv ~ . - age, data = Boston)\nmsummary(lm_fit1)\n\n \n\n  \n    \n    \n    tinytable_fh533nvskvlgyxfrijo2\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  41.525   \n                \n                \n                             \n                  (4.920)  \n                \n                \n                  crim       \n                  -0.121   \n                \n                \n                             \n                  (0.033)  \n                \n                \n                  zn         \n                  0.047    \n                \n                \n                             \n                  (0.014)  \n                \n                \n                  indus      \n                  0.013    \n                \n                \n                             \n                  (0.062)  \n                \n                \n                  chas       \n                  2.853    \n                \n                \n                             \n                  (0.868)  \n                \n                \n                  nox        \n                  -18.485  \n                \n                \n                             \n                  (3.714)  \n                \n                \n                  rm         \n                  3.681    \n                \n                \n                             \n                  (0.411)  \n                \n                \n                  dis        \n                  -1.507   \n                \n                \n                             \n                  (0.193)  \n                \n                \n                  rad        \n                  0.288    \n                \n                \n                             \n                  (0.067)  \n                \n                \n                  tax        \n                  -0.013   \n                \n                \n                             \n                  (0.004)  \n                \n                \n                  ptratio    \n                  -0.935   \n                \n                \n                             \n                  (0.132)  \n                \n                \n                  lstat      \n                  -0.547   \n                \n                \n                             \n                  (0.048)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.734    \n                \n                \n                  R2 Adj.    \n                  0.728    \n                \n                \n                  AIC        \n                  3035.9   \n                \n                \n                  BIC        \n                  3090.8   \n                \n                \n                  Log.Lik.   \n                  -1504.948\n                \n                \n                  F          \n                  124.092  \n                \n                \n                  RMSE       \n                  4.74     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nAlternatively, the update() function can be used.\n\nlm_fit1 &lt;- update(lm_fit, ~ . - age)\n\n\n\n\nIt is easy to include interaction terms in a linear model using the lm() function. The syntax lstat:age tells R to include an interaction term between lstat and age. The syntax lstat * age simultaneously includes lstat, age, and the interaction term lstat\\(\\times\\)age as predictors; it is a shorthand for lstat + age + lstat:age. %We can also pass in transformed versions of the predictors.\n\nmsummary(lm(medv ~ lstat * age, data = Boston), estimate = \"std.error\", stars = TRUE)\n\n \n\n  \n    \n    \n    tinytable_9mhvxbwi0wbwi0kjtzmk\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  1.470*** \n                \n                \n                             \n                  (1.470)  \n                \n                \n                  lstat      \n                  0.167*** \n                \n                \n                             \n                  (0.167)  \n                \n                \n                  age        \n                  0.020    \n                \n                \n                             \n                  (0.020)  \n                \n                \n                  lstat × age\n                  0.002*   \n                \n                \n                             \n                  (0.002)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.556    \n                \n                \n                  R2 Adj.    \n                  0.553    \n                \n                \n                  AIC        \n                  3280.0   \n                \n                \n                  BIC        \n                  3301.1   \n                \n                \n                  Log.Lik.   \n                  -1634.977\n                \n                \n                  F          \n                  209.312  \n                \n                \n                  RMSE       \n                  6.12     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\n\n\nThe lm() function can also accommodate non-linear transformations of the predictors. For instance, given a predictor \\(X\\), we can create a predictor \\(X^2\\) using I(X^2). The function I() is needed since the ^ has a special meaning in a formula object; wrapping as we do allows the standard usage in R, which is to raise X to the power 2. We now perform a regression of medv onto lstat and lstat^2.\n\nlm_fit2 &lt;- lm(medv ~ lstat + I(lstat^2))\nmsummary(lm_fit2, estimate = \"std.error\", stars = TRUE)\n\n \n\n  \n    \n    \n    tinytable_td1tu8wa9ttv779t6uut\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  0.872*** \n                \n                \n                             \n                  (0.872)  \n                \n                \n                  lstat      \n                  0.124*** \n                \n                \n                             \n                  (0.124)  \n                \n                \n                  I(lstat^2) \n                  0.004*** \n                \n                \n                             \n                  (0.004)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.641    \n                \n                \n                  R2 Adj.    \n                  0.639    \n                \n                \n                  AIC        \n                  3170.5   \n                \n                \n                  BIC        \n                  3187.4   \n                \n                \n                  Log.Lik.   \n                  -1581.258\n                \n                \n                  F          \n                  448.505  \n                \n                \n                  RMSE       \n                  5.51     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nThe near-zero \\(p\\)-value associated with the quadratic term suggests that it leads to an improved model. We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\nsummary(lm(medv ~ ., data = Boston))\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1304  -2.7673  -0.5814   1.9414  26.2526 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.617270   4.936039   8.431 3.79e-16 ***\ncrim         -0.121389   0.033000  -3.678 0.000261 ***\nzn            0.046963   0.013879   3.384 0.000772 ***\nindus         0.013468   0.062145   0.217 0.828520    \nchas          2.839993   0.870007   3.264 0.001173 ** \nnox         -18.758022   3.851355  -4.870 1.50e-06 ***\nrm            3.658119   0.420246   8.705  &lt; 2e-16 ***\nage           0.003611   0.013329   0.271 0.786595    \ndis          -1.490754   0.201623  -7.394 6.17e-13 ***\nrad           0.289405   0.066908   4.325 1.84e-05 ***\ntax          -0.012682   0.003801  -3.337 0.000912 ***\nptratio      -0.937533   0.132206  -7.091 4.63e-12 ***\nlstat        -0.552019   0.050659 -10.897  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.798 on 493 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7278 \nF-statistic: 113.5 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nanova(lm_fit, lm_fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + \n    tax + ptratio + lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS  Df Sum of Sq      F    Pr(&gt;F)    \n1    493 11349                                   \n2    503 15347 -10   -3997.8 17.366 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere Model 1 represents the linear submodel containing only one predictor, lstat \\[ medv_i = \\beta_0 + \\beta_1 \\times lstat_i + \\epsilon_i, \\]\nwhile Model 2 corresponds to the larger quadratic model that has two predictors, lstat and lstat^2\n\\[ medv_i = \\beta_0 + \\beta_1 \\times lstat_i + \\beta_2 \\times lstat_i^2 + \\epsilon_i, \\]\nThe anova() function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the \\(F\\)-statistic is \\(135\\) and the associated \\(p\\)-value is virtually zero. This provides very clear evidence that the model containing the predictors lstat and lstat^2 is far superior to the model that only contains the predictor lstat. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat. Diagnostic plots can be used to further investigate the quality of the model fit using residuals.\n\npar(mfrow = c(2, 2))\nplot(lm_fit2)\n\n\n\n\n\n\n\n\n\npar(mfrow = c(2, 2))\n# Base R diagnostic plots\nplot(lm_fit2)\n\n\n\n\n\n\n\n# Enhanced diagnostic plots using the `car` package\nlibrary(car)\nresidualPlots(lm_fit2)\n\n\n\n\n\n\n\n\n           Test stat Pr(&gt;|Test stat|)    \nlstat        -0.7542           0.4511    \nI(lstat^2)   -4.3276        1.820e-05 ***\nTukey test    6.1962        5.783e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nqqPlot(lm_fit2)\n\n[1] 372 373\n\nspreadLevelPlot(lm_fit2)\n\n\nSuggested power transformation:  0.01056432 \n\ninfluencePlot(lm_fit2)\n\n       StudRes         Hat      CookD\n215  2.1564869 0.020322879 0.03192524\n372  4.7026322 0.002557843 0.01814214\n373  4.5109537 0.002598989 0.01701993\n375 -0.6268209 0.110612307 0.01630807\n415 -1.7417833 0.092762866 0.10298355\n\n\n\n\n\n\n\n\n\n\nlibrary(olsrr)\n\n\nAttaching package: 'olsrr'\n\n\nThe following object is masked from 'package:MASS':\n\n    cement\n\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\nols_plot_resid_fit(lm_fit2)\n\n\n\n\n\n\n\n\n\nlibrary(olsrr)\nols_plot_resid_hist(lm_fit2)\n\n\n\n\n\n\n\n\n\nlibrary(olsrr)\nols_plot_resid_qq(lm_fit2)\n\n\n\n\n\n\n\n\n\nols_test_correlation(lm_fit2)\n\n[1] 0.9667391\n\n\nthen we see that when the lstat^2 term is included in the model, there is little discernible pattern in the residuals.\nIn order to create a cubic fit, we can include a predictor of the form I(X^3). However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the poly() function to create the polynomial within lm(). For example, the following command produces a fifth-order polynomial fit:\n\nlm_fit5 &lt;- lm(medv ~ poly(lstat, 5))\nmsummary(lm_fit5)\n\n \n\n  \n    \n    \n    tinytable_mf6mlew1ej1u2njfi0bx\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)    \n                  22.533   \n                \n                \n                                 \n                  (0.232)  \n                \n                \n                  poly(lstat, 5)1\n                  -152.460 \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  poly(lstat, 5)2\n                  64.227   \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  poly(lstat, 5)3\n                  -27.051  \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  poly(lstat, 5)4\n                  25.452   \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  poly(lstat, 5)5\n                  -19.252  \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  Num.Obs.       \n                  506      \n                \n                \n                  R2             \n                  0.682    \n                \n                \n                  R2 Adj.        \n                  0.679    \n                \n                \n                  AIC            \n                  3115.2   \n                \n                \n                  BIC            \n                  3144.8   \n                \n                \n                  Log.Lik.       \n                  -1550.624\n                \n                \n                  F              \n                  214.159  \n                \n                \n                  RMSE           \n                  5.18     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nThis suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant \\(p\\)-values in a regression fit.\nBy default, the poly() function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the poly() function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the poly() function, the argument raw = TRUE must be used.\nOf course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.\n\nmsummary(lm(medv ~ log(rm), data = Boston))\n\n \n\n  \n    \n    \n    tinytable_v28p2dzkxm9i4i5xijlr\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  -76.488  \n                \n                \n                             \n                  (5.028)  \n                \n                \n                  log(rm)    \n                  54.055   \n                \n                \n                             \n                  (2.739)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.436    \n                \n                \n                  R2 Adj.    \n                  0.435    \n                \n                \n                  AIC        \n                  3396.8   \n                \n                \n                  BIC        \n                  3409.5   \n                \n                \n                  Log.Lik.   \n                  -1695.424\n                \n                \n                  F          \n                  389.345  \n                \n                \n                  RMSE       \n                  6.90     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\n\n\nWe will now examine the Carseats data, which is part of the ISLR2 library. We will attempt to predict Sales (child car seat sales) in \\(400\\) locations based on a number of predictors. Carseats have 400 rows and 11 columns.\n\nhead(Carseats)\n\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1  9.50       138     73          11        276   120       Bad  42        17\n2 11.22       111     48          16        260    83      Good  65        10\n3 10.06       113     35          10        269    80    Medium  59        12\n4  7.40       117    100           4        466    97    Medium  55        14\n5  4.15       141     64           3        340   128       Bad  38        13\n6 10.81       124    113          13        501    72       Bad  78        16\n  Urban  US\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\nCarseats |&gt; janitor::clean_names() |&gt; head()\n\n  sales comp_price income advertising population price shelve_loc age education\n1  9.50        138     73          11        276   120        Bad  42        17\n2 11.22        111     48          16        260    83       Good  65        10\n3 10.06        113     35          10        269    80     Medium  59        12\n4  7.40        117    100           4        466    97     Medium  55        14\n5  4.15        141     64           3        340   128        Bad  38        13\n6 10.81        124    113          13        501    72        Bad  78        16\n  urban  us\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\n\nThe Carseats data includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.\n\nlm_fit &lt;- lm(Sales ~ . + Income:Advertising + Price:Age,\n    data = Carseats)\nmsummary(lm_fit)\n\n \n\n  \n    \n    \n    tinytable_qa0h6vgcsouzmzzdvmxa\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)         \n                  6.576   \n                \n                \n                                      \n                  (1.009) \n                \n                \n                  CompPrice           \n                  0.093   \n                \n                \n                                      \n                  (0.004) \n                \n                \n                  Income              \n                  0.011   \n                \n                \n                                      \n                  (0.003) \n                \n                \n                  Advertising         \n                  0.070   \n                \n                \n                                      \n                  (0.023) \n                \n                \n                  Population          \n                  0.000   \n                \n                \n                                      \n                  (0.000) \n                \n                \n                  Price               \n                  -0.101  \n                \n                \n                                      \n                  (0.007) \n                \n                \n                  ShelveLocGood       \n                  4.849   \n                \n                \n                                      \n                  (0.153) \n                \n                \n                  ShelveLocMedium     \n                  1.953   \n                \n                \n                                      \n                  (0.126) \n                \n                \n                  Age                 \n                  -0.058  \n                \n                \n                                      \n                  (0.016) \n                \n                \n                  Education           \n                  -0.021  \n                \n                \n                                      \n                  (0.020) \n                \n                \n                  UrbanYes            \n                  0.140   \n                \n                \n                                      \n                  (0.112) \n                \n                \n                  USYes               \n                  -0.158  \n                \n                \n                                      \n                  (0.149) \n                \n                \n                  Income × Advertising\n                  0.001   \n                \n                \n                                      \n                  (0.000) \n                \n                \n                  Price × Age         \n                  0.000   \n                \n                \n                                      \n                  (0.000) \n                \n                \n                  Num.Obs.            \n                  400     \n                \n                \n                  R2                  \n                  0.876   \n                \n                \n                  R2 Adj.             \n                  0.872   \n                \n                \n                  AIC                 \n                  1159.3  \n                \n                \n                  BIC                 \n                  1219.2  \n                \n                \n                  Log.Lik.            \n                  -564.669\n                \n                \n                  F                   \n                  209.988 \n                \n                \n                  RMSE                \n                  0.99    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nThe contrasts() function returns the coding that R uses for the dummy variables.\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\nUse ?contrasts to learn about other contrasts, and how to set them.\nR has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.\n\n\n\nR comes with numerous built-in functions, and many more are accessible through R libraries. However, there are times when a specific operation is needed for which no existing function is available. In such cases, we can create our own custom functions. Below, we illustrate this by creating a function called LoadLibraries() that loads the tidyverse, broom, and car packages. Attempting to call the function before its definition results in an error, as shown:\nNext, we define the function. The + symbols are printed by R automatically and should not be typed by the user. The { symbol indicates that a block of commands is about to follow. Pressing Enter after { causes R to display the + prompt, allowing for the input of multiple commands. The block is closed with } to signal the end of the function.\n\nLoadLibraries &lt;- function() {\n  library(tidyverse)\n  library(broom)\n  library(car)\n  library(MASS)\n  library(ISLR2)\n  print(\"The libraries have been loaded.\")\n}\n\nTyping LoadLibraries in the console after defining the function will display its content:\n\nLoadLibraries\n\nfunction() {\n  library(tidyverse)\n  library(broom)\n  library(car)\n  library(MASS)\n  library(ISLR2)\n  print(\"The libraries have been loaded.\")\n}\n\n\nCalling the function will load the specified libraries and display the print message:\n\nLoadLibraries()\n\n[1] \"The libraries have been loaded.\""
  },
  {
    "objectID": "ch03.html#libraries",
    "href": "ch03.html#libraries",
    "title": "ISLR chapter 3",
    "section": "",
    "text": "This is a little modified version of the code from the book Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. The code is written in R and I have used the tidyverse package to make it more readable and easy to understand instead of using the base R functions.\nThe first thing we need to do is install and then load the tidyverse set of R packages to provide us with lots of extra functionality. You only need to install this once: once it’s installed we can simply load it into the workspace using the library(packagename) function each time we open a new R session.\nUnderstanding data sets requires many hours/days or in some cases weeks.There are many commercially available software but open source community based software have now dominated and R is one of these. Here I also load the MASS package, which is a very large collection of data sets and functions. We also load the ISLR2 package, which includes the data sets associated with the book Introduction to Statistical Learning.\n\nlibrary(MASS)\nlibrary(ISLR2)\nlibrary(tidyverse)"
  },
  {
    "objectID": "ch03.html#simple-linear-regression",
    "href": "ch03.html#simple-linear-regression",
    "title": "ISLR chapter 3",
    "section": "",
    "text": "Here’s a tabular format representing the meta data for the Boston data set from the ISLR2 library. This table provides the column name, a brief description, and the type of data in each column.\n\n\n\n\n\n\n\n\nColumn Name\nDescription\nData Type\n\n\n\n\ncrim\nPer capita crime rate by town\nNumeric\n\n\nzn\nProportion of residential land zoned for lots over 25,000 sq. ft.\nNumeric\n\n\nindus\nProportion of non-retail business acres per town\nNumeric\n\n\nchas\nCharles River dummy variable (1 if tract bounds river; 0 otherwise)\nCategorical\n\n\nnox\nNitrogen oxides concentration (parts per 10 million)\nNumeric\n\n\nrm\nAverage number of rooms per dwelling\nNumeric\n\n\nage\nProportion of owner-occupied units built prior to 1940\nNumeric\n\n\ndis\nWeighted distances to five Boston employment centers\nNumeric\n\n\nrad\nIndex of accessibility to radial highways\nCategorical\n\n\ntax\nFull-value property-tax rate per $10,000\nNumeric\n\n\nptratio\nPupil-teacher ratio by town\nNumeric\n\n\nblack\n1000(Bk - 0.63)^2 where Bk is the proportion of Black people by town\nNumeric\n\n\nlstat\nPercentage of lower status of the population\nNumeric\n\n\nmedv\nMedian value of owner-occupied homes in $1000’s\nNumeric\n\n\n\nThis meta data outlines the features available in the Boston dataset that can be used to predict the medv variable.\n\nhead(Boston)\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat medv\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98 24.0\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14 21.6\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03 34.7\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7  2.94 33.4\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7  5.33 36.2\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7  5.21 28.7\n\n\nTo find out more about the data set, we can type ?Boston.\nWe will start by using the lm() function to fit a simple linear regression model, with medv as the response and lstat as the predictor. The basic syntax is lm(y ~ x, data), where y is the response, x is the predictor, and data is the data set in which these two variables are kept.\n\nlm_fit &lt;- lm(medv ~ lstat)\n\nError in eval(predvars, data, env): object 'medv' not found\n\n\nThe command causes an error because R does not know where to find the variables medv and lstat. The next line tells R that the variables are in Boston. If we attach Boston, the first line works fine because R now recognizes the variables.\n\nlm_fit &lt;- lm(medv ~ lstat, data = Boston)\nattach(Boston)  # attach the Boston data set\nlm_fit &lt;- lm(medv ~ lstat) # fit the model with data already attached\n\nIf we type lm_fit, some basic information about the model is output. For more detailed information, one may use summary(lm_fit) and I am using here modelsummary package to present summary in a more readable format. The summary() function outputs the coefficients of the model as well as their standard errors, \\(t\\)-statistics, and \\(p\\)-values. The summary() function also outputs the \\(R^2\\) statistic and an analysis of variance (ANOVA) table, which breaks down the variance associated with the regression model and the residuals. This gives us \\(p\\)-values and standard errors for the coefficients, as well as the \\(R^2\\) statistic and \\(F\\)-statistic for the model.\n\nlibrary(modelsummary)\n\n`modelsummary` 2.0.0 now uses `tinytable` as its default table-drawing\n  backend. Learn more at: https://vincentarelbundock.github.io/tinytable/\n\nRevert to `kableExtra` for one session:\n\n  options(modelsummary_factory_default = 'kableExtra')\n  options(modelsummary_factory_latex = 'kableExtra')\n  options(modelsummary_factory_html = 'kableExtra')\n\nSilence this message forever:\n\n  config_modelsummary(startup_message = FALSE)\n\nmsummary(lm_fit)\n\n \n\n  \n    \n    \n    tinytable_aqnu32b4kv5m5p83ubbl\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  34.554   \n                \n                \n                             \n                  (0.563)  \n                \n                \n                  lstat      \n                  -0.950   \n                \n                \n                             \n                  (0.039)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.544    \n                \n                \n                  R2 Adj.    \n                  0.543    \n                \n                \n                  AIC        \n                  3289.0   \n                \n                \n                  BIC        \n                  3301.7   \n                \n                \n                  Log.Lik.   \n                  -1641.487\n                \n                \n                  F          \n                  601.618  \n                \n                \n                  RMSE       \n                  6.20     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nOne may extract the coefficients of the model using the coef() function. The names() function can be used to extract the names of the coefficients.\n\nnames(lm_fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm_fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n\nFor confidence intervals, we can use the confint() function. By default, confint() provides 95 % confidence intervals; however, this can be changed using the level argument.\n\nconfint(lm_fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\nTo predict the median house value for a given percentage of lower status of the population, we can use the predict() function. The predict() function can be used to produce confidence intervals and prediction intervals for the prediction.\n\npredict(lm_fit, data.frame(lstat = (c(5, 10, 15))),\n        interval = \"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm_fit, data.frame(lstat = (c(5, 10, 15))),\n        interval = \"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n\n\nlibrary(modelsummary)\nlibrary(broom)\n\n# Fit the linear model\nlm_fit &lt;- lm(medv ~ lstat, data = Boston)\n\n# Create predictions with confidence intervals\npredictions &lt;- predict(lm_fit, newdata = data.frame(lstat = c(5, 10, 15)), interval = \"confidence\")\n\n# Convert predictions to a data frame\npred_df &lt;- as.data.frame(predictions)\npred_df$lstat &lt;- c(5, 10, 15)\n\nlibrary(gt)\npred_df |&gt; gt() %&gt;%\n  tab_header(title = \"Predicted Values with Confidence Intervals\")\n\n\n\n\n\n\n\n\nPredicted Values with Confidence Intervals\n\n\nfit\nlwr\nupr\nlstat\n\n\n\n\n29.80359\n29.00741\n30.59978\n5\n\n\n25.05335\n24.47413\n25.63256\n10\n\n\n20.30310\n19.73159\n20.87461\n15\n\n\n\n\n\n\n\n\nFor instance, the 95 % confidence interval associated with a lstat value of 10 is \\((24.47, 25.63)\\), and the 95 % prediction interval is \\((12.828, 37.28)\\). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of \\(25.05\\) for medv when lstat equals 10), but the latter are substantially wider.\nWe will now plot medv and lstat along with the least squares regression line using ggplot2.\n\nggplot(Boston, aes(x = lstat, y = medv)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere is some evidence for non-linearity in the relationship between lstat and medv. We will explore this issue later in this lab.\n\nggplot(Boston, aes(x = lstat, y = medv)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_abline(intercept = 34, slope = -1, color = \"red\", size = 3) +\n  geom_point(color = \"red\") +\n  geom_point(shape = 20) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNext we examine some diagnostic plots, several of which were discussed in Section 3.3.3. Four diagnostic plots are automatically produced by applying the plot() function directly to the output from lm(). In general, this command will produce one plot at a time, and hitting Enter will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the par() and mfrow() functions, which tell R to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, par(mfrow = c(2, 2)) divides the plotting region into a \\(2 \\times 2\\) grid of panels.\n\n# Load necessary libraries\n\nlibrary(broom)\n\n# Fit the linear model\nlm_fit &lt;- lm(medv ~ lstat, data = Boston)\n\n# Extract augmented data for diagnostics\naugmented_data &lt;- augment(lm_fit)\n\n\n# Residuals vs Fitted Plot\nggplot(augmented_data, aes(.fitted, .resid)) +\n  geom_point() +\n  geom_smooth(method = 'loess', col = 'blue', se = FALSE) +\n  labs(title = \"Residuals vs Fitted\", x = \"Fitted values\", y = \"Residuals\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Normal Q-Q Plot\nggplot(augmented_data, aes(sample = .std.resid)) +\n  geom_qq() +\n  geom_qq_line() +\n  labs(title = \"Normal Q-Q\", x = \"Theoretical Quantiles\", y = \"Standardized Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Scale-Location Plot (Spread of residuals)\nggplot(augmented_data, aes(.fitted, sqrt(abs(.std.resid)))) +\n  geom_point() +\n  geom_smooth(method = 'loess', col = 'blue', se = FALSE) +\n  labs(title = \"Scale-Location\", x = \"Fitted values\", y = \"Sqrt(|Standardized Residuals|)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Residuals vs Leverage Plot\nggplot(augmented_data, aes(.hat, .std.resid)) +\n  geom_point() +\n  geom_smooth(method = 'loess', col = 'blue', se = FALSE) +\n  geom_hline(yintercept = c(-3, 3), linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residuals vs Leverage\", x = \"Leverage\", y = \"Standardized Residuals\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Highlight influential points\n# Replace .rownames with row_number if needed\n# Add a row number column to identify rows\naugmented_data &lt;- augmented_data %&gt;%\n  mutate(row_id = row_number())\n\n# Identify influential points using the row_id\ninfluential_points &lt;- augmented_data %&gt;%\n  filter(.hat &gt; 2 * mean(.hat)) %&gt;%\n  pull(row_id)\n\n# Print the identified influential points\nprint(influential_points)\n\n [1]   9  33  49 124 127 142 143 144 145 146 148 149 215 374 375 385 386 387 388\n[20] 389 393 399 400 401 405 409 413 415 416 417 418 438 439 491\n\nprint(influential_points)\n\n [1]   9  33  49 124 127 142 143 144 145 146 148 149 215 374 375 385 386 387 388\n[20] 389 393 399 400 401 405 409 413 415 416 417 418 438 439 491\n\n\nOn the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the hatvalues() function.\n\nleverage &lt;- hatvalues(lm_fit)\n\nThe which.max() function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.\n\nwhich.max(leverage)\n\n375 \n375"
  },
  {
    "objectID": "ch03.html#multiple-linear-regression",
    "href": "ch03.html#multiple-linear-regression",
    "title": "ISLR chapter 3",
    "section": "",
    "text": "Mutiple linear regression is an extension of simple linear regression to the case of two or more predictors. Each predictor has a regression coefficient (i.e. a slope), and there is one intercept. The model is given by\n\\[ Y_i = \\beta_0 + \\beta_1 X_1i + \\beta_2 X_2i + \\ldots + \\beta_p X_pi + \\epsilon_i. \\]\n\\[ Y_i = \\beta_0 + \\beta_1 X_1i + \\beta_2 X_2i + \\ldots + \\beta_p X_pi + \\epsilon_i. \\] The summary() function produces a detailed summary of the regression fit. I will use modelsummary package to produce a more readable output.\n\nlm_fit &lt;- lm(medv ~ lstat + age, data = Boston)\nlibrary(modelsummary)\n\nmodelsummary(lm_fit, estimate = \"std.error\", stars = TRUE)\n\n \n\n  \n    \n    \n    tinytable_y0wwvmf6m7jgyuk0gd6t\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  0.731*** \n                \n                \n                             \n                  (0.731)  \n                \n                \n                  lstat      \n                  0.048*** \n                \n                \n                             \n                  (0.048)  \n                \n                \n                  age        \n                  0.012**  \n                \n                \n                             \n                  (0.012)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.551    \n                \n                \n                  R2 Adj.    \n                  0.549    \n                \n                \n                  AIC        \n                  3283.0   \n                \n                \n                  BIC        \n                  3299.9   \n                \n                \n                  Log.Lik.   \n                  -1637.503\n                \n                \n                  F          \n                  308.969  \n                \n                \n                  RMSE       \n                  6.15     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nThe Boston data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:\n\nlm_fit &lt;- lm(medv ~ ., data = Boston)\nmsummary(lm_fit, estimate = \"std.error\", stars = TRUE)\n\n \n\n  \n    \n    \n    tinytable_rtim97tkxk9igoa2jdiv\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  4.936*** \n                \n                \n                             \n                  (4.936)  \n                \n                \n                  crim       \n                  0.033*** \n                \n                \n                             \n                  (0.033)  \n                \n                \n                  zn         \n                  0.014*** \n                \n                \n                             \n                  (0.014)  \n                \n                \n                  indus      \n                  0.062    \n                \n                \n                             \n                  (0.062)  \n                \n                \n                  chas       \n                  0.870**  \n                \n                \n                             \n                  (0.870)  \n                \n                \n                  nox        \n                  3.851*** \n                \n                \n                             \n                  (3.851)  \n                \n                \n                  rm         \n                  0.420*** \n                \n                \n                             \n                  (0.420)  \n                \n                \n                  age        \n                  0.013    \n                \n                \n                             \n                  (0.013)  \n                \n                \n                  dis        \n                  0.202*** \n                \n                \n                             \n                  (0.202)  \n                \n                \n                  rad        \n                  0.067*** \n                \n                \n                             \n                  (0.067)  \n                \n                \n                  tax        \n                  0.004*** \n                \n                \n                             \n                  (0.004)  \n                \n                \n                  ptratio    \n                  0.132*** \n                \n                \n                             \n                  (0.132)  \n                \n                \n                  lstat      \n                  0.051*** \n                \n                \n                             \n                  (0.051)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.734    \n                \n                \n                  R2 Adj.    \n                  0.728    \n                \n                \n                  AIC        \n                  3037.8   \n                \n                \n                  BIC        \n                  3097.0   \n                \n                \n                  Log.Lik.   \n                  -1504.910\n                \n                \n                  F          \n                  113.544  \n                \n                \n                  RMSE       \n                  4.74     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nWe can access the individual components of a summary object by name. The names() function can be used to obtain the names of the components. The summary() function returns a list with components such as call, terms, residuals, coefficients, aliased, sigma, df, r.squared, adj.r.squared, fstatistic, cov.unscaled, na.action. The vif() function, part of the car package, can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data. The car package is not part of the base R installation so it must be downloaded the first time you use it via the install.packages() function in R.\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nWhat if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high \\(p\\)-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except age.\n\nlm_fit1 &lt;- lm(medv ~ . - age, data = Boston)\nmsummary(lm_fit1)\n\n \n\n  \n    \n    \n    tinytable_fh533nvskvlgyxfrijo2\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  41.525   \n                \n                \n                             \n                  (4.920)  \n                \n                \n                  crim       \n                  -0.121   \n                \n                \n                             \n                  (0.033)  \n                \n                \n                  zn         \n                  0.047    \n                \n                \n                             \n                  (0.014)  \n                \n                \n                  indus      \n                  0.013    \n                \n                \n                             \n                  (0.062)  \n                \n                \n                  chas       \n                  2.853    \n                \n                \n                             \n                  (0.868)  \n                \n                \n                  nox        \n                  -18.485  \n                \n                \n                             \n                  (3.714)  \n                \n                \n                  rm         \n                  3.681    \n                \n                \n                             \n                  (0.411)  \n                \n                \n                  dis        \n                  -1.507   \n                \n                \n                             \n                  (0.193)  \n                \n                \n                  rad        \n                  0.288    \n                \n                \n                             \n                  (0.067)  \n                \n                \n                  tax        \n                  -0.013   \n                \n                \n                             \n                  (0.004)  \n                \n                \n                  ptratio    \n                  -0.935   \n                \n                \n                             \n                  (0.132)  \n                \n                \n                  lstat      \n                  -0.547   \n                \n                \n                             \n                  (0.048)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.734    \n                \n                \n                  R2 Adj.    \n                  0.728    \n                \n                \n                  AIC        \n                  3035.9   \n                \n                \n                  BIC        \n                  3090.8   \n                \n                \n                  Log.Lik.   \n                  -1504.948\n                \n                \n                  F          \n                  124.092  \n                \n                \n                  RMSE       \n                  4.74     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nAlternatively, the update() function can be used.\n\nlm_fit1 &lt;- update(lm_fit, ~ . - age)"
  },
  {
    "objectID": "ch03.html#interaction-terms",
    "href": "ch03.html#interaction-terms",
    "title": "ISLR chapter 3",
    "section": "",
    "text": "It is easy to include interaction terms in a linear model using the lm() function. The syntax lstat:age tells R to include an interaction term between lstat and age. The syntax lstat * age simultaneously includes lstat, age, and the interaction term lstat\\(\\times\\)age as predictors; it is a shorthand for lstat + age + lstat:age. %We can also pass in transformed versions of the predictors.\n\nmsummary(lm(medv ~ lstat * age, data = Boston), estimate = \"std.error\", stars = TRUE)\n\n \n\n  \n    \n    \n    tinytable_9mhvxbwi0wbwi0kjtzmk\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  1.470*** \n                \n                \n                             \n                  (1.470)  \n                \n                \n                  lstat      \n                  0.167*** \n                \n                \n                             \n                  (0.167)  \n                \n                \n                  age        \n                  0.020    \n                \n                \n                             \n                  (0.020)  \n                \n                \n                  lstat × age\n                  0.002*   \n                \n                \n                             \n                  (0.002)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.556    \n                \n                \n                  R2 Adj.    \n                  0.553    \n                \n                \n                  AIC        \n                  3280.0   \n                \n                \n                  BIC        \n                  3301.1   \n                \n                \n                  Log.Lik.   \n                  -1634.977\n                \n                \n                  F          \n                  209.312  \n                \n                \n                  RMSE       \n                  6.12"
  },
  {
    "objectID": "ch03.html#non-linear-transformations-of-the-predictors",
    "href": "ch03.html#non-linear-transformations-of-the-predictors",
    "title": "ISLR chapter 3",
    "section": "",
    "text": "The lm() function can also accommodate non-linear transformations of the predictors. For instance, given a predictor \\(X\\), we can create a predictor \\(X^2\\) using I(X^2). The function I() is needed since the ^ has a special meaning in a formula object; wrapping as we do allows the standard usage in R, which is to raise X to the power 2. We now perform a regression of medv onto lstat and lstat^2.\n\nlm_fit2 &lt;- lm(medv ~ lstat + I(lstat^2))\nmsummary(lm_fit2, estimate = \"std.error\", stars = TRUE)\n\n \n\n  \n    \n    \n    tinytable_td1tu8wa9ttv779t6uut\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  0.872*** \n                \n                \n                             \n                  (0.872)  \n                \n                \n                  lstat      \n                  0.124*** \n                \n                \n                             \n                  (0.124)  \n                \n                \n                  I(lstat^2) \n                  0.004*** \n                \n                \n                             \n                  (0.004)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.641    \n                \n                \n                  R2 Adj.    \n                  0.639    \n                \n                \n                  AIC        \n                  3170.5   \n                \n                \n                  BIC        \n                  3187.4   \n                \n                \n                  Log.Lik.   \n                  -1581.258\n                \n                \n                  F          \n                  448.505  \n                \n                \n                  RMSE       \n                  5.51     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nThe near-zero \\(p\\)-value associated with the quadratic term suggests that it leads to an improved model. We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\nsummary(lm(medv ~ ., data = Boston))\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1304  -2.7673  -0.5814   1.9414  26.2526 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.617270   4.936039   8.431 3.79e-16 ***\ncrim         -0.121389   0.033000  -3.678 0.000261 ***\nzn            0.046963   0.013879   3.384 0.000772 ***\nindus         0.013468   0.062145   0.217 0.828520    \nchas          2.839993   0.870007   3.264 0.001173 ** \nnox         -18.758022   3.851355  -4.870 1.50e-06 ***\nrm            3.658119   0.420246   8.705  &lt; 2e-16 ***\nage           0.003611   0.013329   0.271 0.786595    \ndis          -1.490754   0.201623  -7.394 6.17e-13 ***\nrad           0.289405   0.066908   4.325 1.84e-05 ***\ntax          -0.012682   0.003801  -3.337 0.000912 ***\nptratio      -0.937533   0.132206  -7.091 4.63e-12 ***\nlstat        -0.552019   0.050659 -10.897  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.798 on 493 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7278 \nF-statistic: 113.5 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nanova(lm_fit, lm_fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + \n    tax + ptratio + lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS  Df Sum of Sq      F    Pr(&gt;F)    \n1    493 11349                                   \n2    503 15347 -10   -3997.8 17.366 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere Model 1 represents the linear submodel containing only one predictor, lstat \\[ medv_i = \\beta_0 + \\beta_1 \\times lstat_i + \\epsilon_i, \\]\nwhile Model 2 corresponds to the larger quadratic model that has two predictors, lstat and lstat^2\n\\[ medv_i = \\beta_0 + \\beta_1 \\times lstat_i + \\beta_2 \\times lstat_i^2 + \\epsilon_i, \\]\nThe anova() function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the \\(F\\)-statistic is \\(135\\) and the associated \\(p\\)-value is virtually zero. This provides very clear evidence that the model containing the predictors lstat and lstat^2 is far superior to the model that only contains the predictor lstat. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat. Diagnostic plots can be used to further investigate the quality of the model fit using residuals.\n\npar(mfrow = c(2, 2))\nplot(lm_fit2)\n\n\n\n\n\n\n\n\n\npar(mfrow = c(2, 2))\n# Base R diagnostic plots\nplot(lm_fit2)\n\n\n\n\n\n\n\n# Enhanced diagnostic plots using the `car` package\nlibrary(car)\nresidualPlots(lm_fit2)\n\n\n\n\n\n\n\n\n           Test stat Pr(&gt;|Test stat|)    \nlstat        -0.7542           0.4511    \nI(lstat^2)   -4.3276        1.820e-05 ***\nTukey test    6.1962        5.783e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nqqPlot(lm_fit2)\n\n[1] 372 373\n\nspreadLevelPlot(lm_fit2)\n\n\nSuggested power transformation:  0.01056432 \n\ninfluencePlot(lm_fit2)\n\n       StudRes         Hat      CookD\n215  2.1564869 0.020322879 0.03192524\n372  4.7026322 0.002557843 0.01814214\n373  4.5109537 0.002598989 0.01701993\n375 -0.6268209 0.110612307 0.01630807\n415 -1.7417833 0.092762866 0.10298355\n\n\n\n\n\n\n\n\n\n\nlibrary(olsrr)\n\n\nAttaching package: 'olsrr'\n\n\nThe following object is masked from 'package:MASS':\n\n    cement\n\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\nols_plot_resid_fit(lm_fit2)\n\n\n\n\n\n\n\n\n\nlibrary(olsrr)\nols_plot_resid_hist(lm_fit2)\n\n\n\n\n\n\n\n\n\nlibrary(olsrr)\nols_plot_resid_qq(lm_fit2)\n\n\n\n\n\n\n\n\n\nols_test_correlation(lm_fit2)\n\n[1] 0.9667391\n\n\nthen we see that when the lstat^2 term is included in the model, there is little discernible pattern in the residuals.\nIn order to create a cubic fit, we can include a predictor of the form I(X^3). However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the poly() function to create the polynomial within lm(). For example, the following command produces a fifth-order polynomial fit:\n\nlm_fit5 &lt;- lm(medv ~ poly(lstat, 5))\nmsummary(lm_fit5)\n\n \n\n  \n    \n    \n    tinytable_mf6mlew1ej1u2njfi0bx\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)    \n                  22.533   \n                \n                \n                                 \n                  (0.232)  \n                \n                \n                  poly(lstat, 5)1\n                  -152.460 \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  poly(lstat, 5)2\n                  64.227   \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  poly(lstat, 5)3\n                  -27.051  \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  poly(lstat, 5)4\n                  25.452   \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  poly(lstat, 5)5\n                  -19.252  \n                \n                \n                                 \n                  (5.215)  \n                \n                \n                  Num.Obs.       \n                  506      \n                \n                \n                  R2             \n                  0.682    \n                \n                \n                  R2 Adj.        \n                  0.679    \n                \n                \n                  AIC            \n                  3115.2   \n                \n                \n                  BIC            \n                  3144.8   \n                \n                \n                  Log.Lik.       \n                  -1550.624\n                \n                \n                  F              \n                  214.159  \n                \n                \n                  RMSE           \n                  5.18     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nThis suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant \\(p\\)-values in a regression fit.\nBy default, the poly() function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the poly() function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the poly() function, the argument raw = TRUE must be used.\nOf course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.\n\nmsummary(lm(medv ~ log(rm), data = Boston))\n\n \n\n  \n    \n    \n    tinytable_v28p2dzkxm9i4i5xijlr\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  -76.488  \n                \n                \n                             \n                  (5.028)  \n                \n                \n                  log(rm)    \n                  54.055   \n                \n                \n                             \n                  (2.739)  \n                \n                \n                  Num.Obs.   \n                  506      \n                \n                \n                  R2         \n                  0.436    \n                \n                \n                  R2 Adj.    \n                  0.435    \n                \n                \n                  AIC        \n                  3396.8   \n                \n                \n                  BIC        \n                  3409.5   \n                \n                \n                  Log.Lik.   \n                  -1695.424\n                \n                \n                  F          \n                  389.345  \n                \n                \n                  RMSE       \n                  6.90"
  },
  {
    "objectID": "ch03.html#qualitative-predictors",
    "href": "ch03.html#qualitative-predictors",
    "title": "ISLR chapter 3",
    "section": "",
    "text": "We will now examine the Carseats data, which is part of the ISLR2 library. We will attempt to predict Sales (child car seat sales) in \\(400\\) locations based on a number of predictors. Carseats have 400 rows and 11 columns.\n\nhead(Carseats)\n\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1  9.50       138     73          11        276   120       Bad  42        17\n2 11.22       111     48          16        260    83      Good  65        10\n3 10.06       113     35          10        269    80    Medium  59        12\n4  7.40       117    100           4        466    97    Medium  55        14\n5  4.15       141     64           3        340   128       Bad  38        13\n6 10.81       124    113          13        501    72       Bad  78        16\n  Urban  US\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\nCarseats |&gt; janitor::clean_names() |&gt; head()\n\n  sales comp_price income advertising population price shelve_loc age education\n1  9.50        138     73          11        276   120        Bad  42        17\n2 11.22        111     48          16        260    83       Good  65        10\n3 10.06        113     35          10        269    80     Medium  59        12\n4  7.40        117    100           4        466    97     Medium  55        14\n5  4.15        141     64           3        340   128        Bad  38        13\n6 10.81        124    113          13        501    72        Bad  78        16\n  urban  us\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\n\nThe Carseats data includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.\n\nlm_fit &lt;- lm(Sales ~ . + Income:Advertising + Price:Age,\n    data = Carseats)\nmsummary(lm_fit)\n\n \n\n  \n    \n    \n    tinytable_qa0h6vgcsouzmzzdvmxa\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)         \n                  6.576   \n                \n                \n                                      \n                  (1.009) \n                \n                \n                  CompPrice           \n                  0.093   \n                \n                \n                                      \n                  (0.004) \n                \n                \n                  Income              \n                  0.011   \n                \n                \n                                      \n                  (0.003) \n                \n                \n                  Advertising         \n                  0.070   \n                \n                \n                                      \n                  (0.023) \n                \n                \n                  Population          \n                  0.000   \n                \n                \n                                      \n                  (0.000) \n                \n                \n                  Price               \n                  -0.101  \n                \n                \n                                      \n                  (0.007) \n                \n                \n                  ShelveLocGood       \n                  4.849   \n                \n                \n                                      \n                  (0.153) \n                \n                \n                  ShelveLocMedium     \n                  1.953   \n                \n                \n                                      \n                  (0.126) \n                \n                \n                  Age                 \n                  -0.058  \n                \n                \n                                      \n                  (0.016) \n                \n                \n                  Education           \n                  -0.021  \n                \n                \n                                      \n                  (0.020) \n                \n                \n                  UrbanYes            \n                  0.140   \n                \n                \n                                      \n                  (0.112) \n                \n                \n                  USYes               \n                  -0.158  \n                \n                \n                                      \n                  (0.149) \n                \n                \n                  Income × Advertising\n                  0.001   \n                \n                \n                                      \n                  (0.000) \n                \n                \n                  Price × Age         \n                  0.000   \n                \n                \n                                      \n                  (0.000) \n                \n                \n                  Num.Obs.            \n                  400     \n                \n                \n                  R2                  \n                  0.876   \n                \n                \n                  R2 Adj.             \n                  0.872   \n                \n                \n                  AIC                 \n                  1159.3  \n                \n                \n                  BIC                 \n                  1219.2  \n                \n                \n                  Log.Lik.            \n                  -564.669\n                \n                \n                  F                   \n                  209.988 \n                \n                \n                  RMSE                \n                  0.99    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nThe contrasts() function returns the coding that R uses for the dummy variables.\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\nUse ?contrasts to learn about other contrasts, and how to set them.\nR has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location."
  },
  {
    "objectID": "ch03.html#writing-functions",
    "href": "ch03.html#writing-functions",
    "title": "ISLR chapter 3",
    "section": "",
    "text": "R comes with numerous built-in functions, and many more are accessible through R libraries. However, there are times when a specific operation is needed for which no existing function is available. In such cases, we can create our own custom functions. Below, we illustrate this by creating a function called LoadLibraries() that loads the tidyverse, broom, and car packages. Attempting to call the function before its definition results in an error, as shown:\nNext, we define the function. The + symbols are printed by R automatically and should not be typed by the user. The { symbol indicates that a block of commands is about to follow. Pressing Enter after { causes R to display the + prompt, allowing for the input of multiple commands. The block is closed with } to signal the end of the function.\n\nLoadLibraries &lt;- function() {\n  library(tidyverse)\n  library(broom)\n  library(car)\n  library(MASS)\n  library(ISLR2)\n  print(\"The libraries have been loaded.\")\n}\n\nTyping LoadLibraries in the console after defining the function will display its content:\n\nLoadLibraries\n\nfunction() {\n  library(tidyverse)\n  library(broom)\n  library(car)\n  library(MASS)\n  library(ISLR2)\n  print(\"The libraries have been loaded.\")\n}\n\n\nCalling the function will load the specified libraries and display the print message:\n\nLoadLibraries()\n\n[1] \"The libraries have been loaded.\""
  },
  {
    "objectID": "CH2 stat learning.html",
    "href": "CH2 stat learning.html",
    "title": "Ch2.ISLR",
    "section": "",
    "text": "#install.packages(\"ISLR\")\nlibrary(ISLR)\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nAdv&lt;-read.csv(\"Advertising.csv\",header = TRUE) \n\nsummarise(Adv)\n\ndata frame with 0 columns and 1 row\n\npar(mfrow=c(3,1))\n\nggplot(Adv)+aes(TV,Sales)+geom_point(size=2, aes(color=\"Red\"))+stat_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(Adv)+aes(Radio,Sales)+geom_point(size=2, aes(color=\"Red\"))+stat_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(Adv)+aes(Newspaper,Sales)+geom_point(size=2, aes(color=\"Red\"))+stat_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNow we would see which medium of advertisement has stronger relationship with Sales"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Author",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github\n  \n\n  \n  \nI am a Professor at School of Economics, Quaid-i-Azam University, Islamabad.\nMy work is about\nI am interested in developing packages. For example, PakPC, PakNAcc …"
  },
  {
    "objectID": "psx.html#logistic-regression",
    "href": "psx.html#logistic-regression",
    "title": "Stock Market Prediction using Machine Learning",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n# First ensure direction is properly encoded as numeric binary in psx_clean\npsx_clean &lt;- psx_clean %&gt;%\n  mutate(direction = as.numeric(direction))  # Should be 0 or 1\n\n# Logistic regression model\nglm_fit &lt;- glm(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol,\n               data = psx_clean, family = binomial)\n\n# Model summary\nsummary(glm_fit)\n\n\nCall:\nglm(formula = direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + \n    vol, family = binomial, data = psx_clean)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.237e-01  6.095e-02  -8.592  &lt; 2e-16 ***\nlag1        -6.414e-02  1.678e-02  -3.822 0.000132 ***\nlag2        -1.150e-02  1.549e-02  -0.742 0.458036    \nlag3        -4.223e-03  1.573e-02  -0.268 0.788338    \nlag4        -2.171e-02  1.515e-02  -1.433 0.151943    \nlag5        -4.423e-02  1.526e-02  -2.898 0.003758 ** \nvol          4.184e-07  4.901e-08   8.536  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2512.1  on 1821  degrees of freedom\nResidual deviance: 2396.3  on 1815  degrees of freedom\nAIC: 2410.3\n\nNumber of Fisher Scoring iterations: 4\n\n# Predict probabilities and classify directions\npsx_clean &lt;- psx_clean %&gt;%\n  mutate(\n    glm_probs = predict(glm_fit, type = \"response\"),\n    glm_pred = as.numeric(glm_probs &gt; 0.5)  # Convert to numeric 0/1\n  )\n\n# Evaluate model accuracy\nconfusion_matrix &lt;- table(Predicted = psx_clean$glm_pred, Actual = psx_clean$direction)\naccuracy &lt;- mean(psx_clean$glm_pred == psx_clean$direction)\n\n# Split data into train and test sets (based on date &lt; 2024)\ntrain &lt;- psx_clean %&gt;%\n  filter(date &lt; as.Date(\"2024-01-01\"))\n\ntest &lt;- psx_clean %&gt;%\n  filter(date &gt;= as.Date(\"2024-01-01\"))\n\n# Train logistic regression on train data\nglm_train &lt;- glm(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol,\n                 data = train, family = binomial)\n\n# Predict on test data\ntest &lt;- test %&gt;%\n  mutate(\n    glm_probs = predict(glm_train, newdata = ., type = \"response\"),\n    glm_pred = as.numeric(glm_probs &gt; 0.5)  # Convert to numeric 0/1\n  )\n\n# Evaluate test accuracy\nconfusion_matrix_test &lt;- table(Predicted = test$glm_pred, Actual = test$direction)\ntest_accuracy &lt;- mean(test$glm_pred == test$direction)\n\n# Create detailed evaluation metrics\ntrain_metrics &lt;- list(\n  accuracy = accuracy,\n  sensitivity = confusion_matrix[2,2] / sum(confusion_matrix[,2]),\n  specificity = confusion_matrix[1,1] / sum(confusion_matrix[,1]),\n  confusion_matrix = confusion_matrix\n)\n\ntest_metrics &lt;- list(\n  accuracy = test_accuracy,\n  sensitivity = confusion_matrix_test[2,2] / sum(confusion_matrix_test[,2]),\n  specificity = confusion_matrix_test[1,1] / sum(confusion_matrix_test[,1]),\n  confusion_matrix = confusion_matrix_test\n)\n\n# Output results\nlist(\n  train_metrics = train_metrics,\n  test_metrics = test_metrics\n)\n\n$train_metrics\n$train_metrics$accuracy\n[1] 0.6136114\n\n$train_metrics$sensitivity\n[1] 0.3245192\n\n$train_metrics$specificity\n[1] 0.8565657\n\n$train_metrics$confusion_matrix\n         Actual\nPredicted   0   1\n        0 848 562\n        1 142 270\n\n\n$test_metrics\n$test_metrics$accuracy\n[1] 0.6139535\n\n$test_metrics$sensitivity\n[1] 0.63\n\n$test_metrics$specificity\n[1] 0.6\n\n$test_metrics$confusion_matrix\n         Actual\nPredicted  0  1\n        0 69 37\n        1 46 63\n\nglm_test_accuracy &lt;- test_metrics$accuracy"
  },
  {
    "objectID": "bootstrapping.html",
    "href": "bootstrapping.html",
    "title": "السلام علیکم :)",
    "section": "",
    "text": "Step-by-Step Guide in R:\n\n1. Understanding Bootstrapping\nBootstrapping is a resampling method that involves randomly drawing samples from a dataset with replacement. It is commonly used to estimate the variability of a statistic (e.g., mean, median, regression coefficient).\n\n\n2. Setup\nInstall and load the necessary libraries:\n\n#install.packages(\"ISLR\")  # Contains datasets used in \"Introduction to Statistical Learning\"\n#install.packages(\"boot\")  # Bootstrapping functions\nlibrary(ISLR)\nlibrary(boot)\n\n\n\n3. Select a Dataset\nUse the Auto dataset from the ISLR package as an example:\n\ndata(\"Auto\")\n\n\n\n4. Define a Statistic\nChoose a statistic to compute. For example, estimating the mean of mpg:\n\nmean_mpg &lt;- function(data, indices) {\n  sampled_data &lt;- data[indices]  # Resample using indices\n  return(mean(sampled_data))\n}\n\n\n\n5. Apply the Bootstrapping\nUse the boot function to perform bootstrapping:\n\nset.seed(123)  # For reproducibility\nboot_result &lt;- boot(data = Auto$mpg, statistic = mean_mpg, R = 1000)  # 1000 bootstrap samples\n\n\n\n6. View the Results\nPrint the bootstrap estimate and plot:\n\nprint(boot_result)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto$mpg, statistic = mean_mpg, R = 1000)\n\n\nBootstrap Statistics :\n    original       bias    std. error\nt1* 23.44592 -0.008097704   0.4007151\n\nplot(boot_result)\n\n\n\n\n\n\n\n\n\n\n7. Bootstrap for Regression\nYou can bootstrap regression coefficients as follows:\n\n# Define a statistic to bootstrap coefficients of a linear model\nboot_fn &lt;- function(data, indices) {\n  sampled_data &lt;- data[indices, ]\n  fit &lt;- lm(mpg ~ horsepower, data = sampled_data)\n  return(coef(fit))\n}\n\n# Perform bootstrapping\nset.seed(123)\nboot_reg &lt;- boot(data = Auto, statistic = boot_fn, R = 1000)\nprint(boot_reg)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot_fn, R = 1000)\n\n\nBootstrap Statistics :\n      original        bias    std. error\nt1* 39.9358610  0.0156469811 0.845583773\nt2* -0.1578447 -0.0001803022 0.007393556\n\n\n\n\n8. Interpret the Output\n\nBootstrap Estimates: Mean or coefficients calculated across resampled datasets.\nBias and Standard Error: Provided in the boot result, giving insight into variability.\n\n\n\n9. Visualize the Confidence Intervals\nFor confidence intervals:\n\nboot.ci(boot.out = boot_result, type = c(\"norm\", \"basic\", \"perc\", \"bca\"))\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_result, type = c(\"norm\", \"basic\", \"perc\", \n    \"bca\"))\n\nIntervals : \nLevel      Normal              Basic         \n95%   (22.67, 24.24 )   (22.64, 24.22 )  \n\nLevel     Percentile            BCa          \n95%   (22.67, 24.25 )   (22.69, 24.28 )  \nCalculations and Intervals on Original Scale\n\n\n\n\n10. Adaptation to Machine Learning\nFor machine learning tasks like cross-validation or parameter tuning, you can use bootstrapping in similar ways: - Combine bootstrap resampling with algorithms like glm for logistic regression or randomForest for more complex models.\nWould you like examples using specific algorithms or a different dataset? Let me know!"
  },
  {
    "objectID": "psx_bootstrapping.html",
    "href": "psx_bootstrapping.html",
    "title": "Bootstrapped Logistic Regression and Machine Learning Models for Predicting Stock Market Movements in the Pakistan Stock Exchange (PSX)",
    "section": "",
    "text": "The first step is to load the PSX data, clean it, and perform necessary transformations for feature engineering.\n\nlibrary(caret)      # For model evaluation and cross-validation\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(rpart)      # For Decision Trees\nlibrary(tidyverse)  # For data manipulation and visualization\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ purrr::lift()   masks caret::lift()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(Metrics)    # For additional evaluation metrics\n\nWarning: package 'Metrics' was built under R version 4.4.2\n\n\n\nAttaching package: 'Metrics'\n\nThe following objects are masked from 'package:caret':\n\n    precision, recall\n\nlibrary(janitor)    # For cleaning column names\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(class)    # For KNN\nlibrary(boot)\n\n\nAttaching package: 'boot'\n\nThe following object is masked from 'package:lattice':\n\n    melanoma"
  },
  {
    "objectID": "psx_bootstrapping.html#loading-and-preparing-the-data",
    "href": "psx_bootstrapping.html#loading-and-preparing-the-data",
    "title": "Bootstrapped Logistic Regression and Machine Learning Models for Predicting Stock Market Movements in the Pakistan Stock Exchange (PSX)",
    "section": "",
    "text": "The first step is to load the PSX data, clean it, and perform necessary transformations for feature engineering.\n\nlibrary(caret)      # For model evaluation and cross-validation\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(rpart)      # For Decision Trees\nlibrary(tidyverse)  # For data manipulation and visualization\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ purrr::lift()   masks caret::lift()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(Metrics)    # For additional evaluation metrics\n\nWarning: package 'Metrics' was built under R version 4.4.2\n\n\n\nAttaching package: 'Metrics'\n\nThe following objects are masked from 'package:caret':\n\n    precision, recall\n\nlibrary(janitor)    # For cleaning column names\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(class)    # For KNN\nlibrary(boot)\n\n\nAttaching package: 'boot'\n\nThe following object is masked from 'package:lattice':\n\n    melanoma"
  },
  {
    "objectID": "psx_bootstrapping.html#load-data",
    "href": "psx_bootstrapping.html#load-data",
    "title": "Bootstrapped Logistic Regression and Machine Learning Models for Predicting Stock Market Movements in the Pakistan Stock Exchange (PSX)",
    "section": "Load data",
    "text": "Load data\nWe have psx data in csv format. We will load the data and inspect its structure.\n\npsx &lt;- read_csv(\"data/psx.csv\")\n\nRows: 1827 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Date, Vol., Change %\ndbl (4): Price, Open, High, Low\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Inspect the data structure (rows, columns, and types)\ndim(psx)\n\n[1] 1827    7\n\nglimpse(psx)\n\nRows: 1,827\nColumns: 7\n$ Date       &lt;chr&gt; \"11/15/2024\", \"11/14/2024\", \"11/13/2024\", \"11/12/2024\", \"11…\n$ Price      &lt;dbl&gt; 17.42, 17.19, 16.78, 16.86, 17.02, 17.22, 17.14, 17.31, 16.…\n$ Open       &lt;dbl&gt; 17.40, 16.70, 16.95, 17.00, 17.46, 17.14, 17.24, 17.24, 17.…\n$ High       &lt;dbl&gt; 17.60, 17.60, 16.95, 17.20, 17.46, 17.49, 17.60, 17.50, 17.…\n$ Low        &lt;dbl&gt; 16.90, 16.55, 16.50, 16.70, 16.97, 17.01, 17.00, 16.60, 16.…\n$ Vol.       &lt;chr&gt; \"896.83K\", \"1.01M\", \"239.44K\", \"179.95K\", \"364.92K\", \"208.0…\n$ `Change %` &lt;chr&gt; \"1.34%\", \"2.44%\", \"-0.47%\", \"-0.94%\", \"-1.16%\", \"0.47%\", \"-…\n\n\n\n1. Bootstrap Logistic Regression\nApply bootstrap resampling to estimate logistic regression coefficients and evaluate variability.\n\n## Data cleaning and feature engineering\n\n\n#| label: data-cleaning\n\n# Convert the `Date` column to a proper Date format\npsx &lt;- psx %&gt;% \n  mutate(date = as.Date(Date, format = \"%m/%d/%Y\")) |&gt; dplyr::select(-Date)\n# Clean column names to make them consistent and readable\npsx &lt;- psx %&gt;% clean_names()\n\n# Convert `change_percent` to numeric and create a new column `direction` for the stock movement (Up/Down)\npsx &lt;- psx %&gt;% \n  mutate(\n    change_percent = as.numeric(str_remove(change_percent, \"%\")),\n    direction = if_else(change_percent &gt; 0, \"Up\", \"Down\")\n  )\n\n# Arrange data by ascending date to maintain temporal order\npsx &lt;- psx %&gt;% arrange(date)\n\n# Create lag variables for `change_percent` (lags of 1 to 5 days)\npsx &lt;- psx %&gt;% \n  mutate(\n    lag1 = lag(change_percent, 1),\n    lag2 = lag(change_percent, 2),\n    lag3 = lag(change_percent, 3),\n    lag4 = lag(change_percent, 4),\n    lag5 = lag(change_percent, 5)\n  )\n\n# Convert `vol` (volume) to numeric, handling the M and K suffixes\npsx &lt;- psx %&gt;% mutate(vol = as.numeric(str_replace_all(vol, c(\"M\" = \"e6\", \"K\" = \"e3\"))))\n\n# Drop rows with missing values (NA)\npsx_clean &lt;- psx %&gt;% drop_na()\n\n# Convert `direction` to numeric for binary classification (1 for \"Up\", 0 for \"Down\")\npsx_clean &lt;- psx_clean %&gt;%\n  mutate(direction = if_else(direction == \"Up\", 1, 0))\n\n\nDefine a Bootstrap Function:\n\nboot_logistic &lt;- function(data, indices) {\n  sampled_data &lt;- data[indices, ]\n  model &lt;- glm(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol, \n               data = sampled_data, family = binomial)\n  return(coef(model))\n}\n\n\n\nApply Bootstrapping:\n\nset.seed(123)\nlogistic_boot_result &lt;- boot(data = psx_clean, statistic = boot_logistic, R = 1000)\nprint(logistic_boot_result)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = psx_clean, statistic = boot_logistic, R = 1000)\n\n\nBootstrap Statistics :\n         original        bias     std. error\nt1* -5.236745e-01 -2.097142e-03 6.719302e-02\nt2* -6.414172e-02  7.111280e-05 1.749324e-02\nt3* -1.149754e-02  1.149087e-04 1.588335e-02\nt4* -4.222881e-03  7.199437e-04 1.633129e-02\nt5* -2.171095e-02 -1.378824e-03 1.638450e-02\nt6* -4.423349e-02 -1.337178e-03 1.472036e-02\nt7*  4.183911e-07  4.427846e-09 5.749570e-08\n\n\n\n\nVisualize and Extract Confidence Intervals:\n\nplot(logistic_boot_result)\n\n\n\n\n\n\n\n\n\nboot.ci(boot.out = logistic_boot_result, type = c(\"norm\", \"basic\", \"perc\"))\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = logistic_boot_result, type = c(\"norm\", \"basic\", \n    \"perc\"))\n\nIntervals : \nLevel      Normal              Basic              Percentile     \n95%   (-0.6533, -0.3899 )   (-0.6570, -0.3905 )   (-0.6568, -0.3903 )  \nCalculations and Intervals on Original Scale\n\n\n\n\n\n\n2. Bootstrap for LDA\n\nDefine a Bootstrap Function:\n\nboot_lda &lt;- function(data, indices) {\n  sampled_data &lt;- data[indices, ]\n  lda_model &lt;- lda(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol, \n                    data = sampled_data)\n  predictions &lt;- predict(lda_model, newdata = data)$class\n  accuracy &lt;- mean(predictions == data$direction)\n  return(accuracy)\n}\n\n\n\nApply Bootstrapping:\n\nset.seed(123)\nlda_boot_result &lt;- boot(data = psx_clean, statistic = boot_lda, R = 500)\nprint(lda_boot_result)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = psx_clean, statistic = boot_lda, R = 500)\n\n\nBootstrap Statistics :\n     original     bias    std. error\nt1* 0.6053787 0.00270472 0.004244633\n\n\n\n\n\n\n3. Bootstrap for QDA\n\nDefine a Bootstrap Function:\n\nboot_qda &lt;- function(data, indices) {\n  sampled_data &lt;- data[indices, ]\n  qda_model &lt;- qda(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol, \n                    data = sampled_data)\n  predictions &lt;- predict(qda_model, newdata = data)$class\n  accuracy &lt;- mean(predictions == data$direction)\n  return(accuracy)\n}\n\n\n\nApply Bootstrapping:\n\nset.seed(123)\nqda_boot_result &lt;- boot(data = psx_clean, statistic = boot_qda, R = 500)\nprint(qda_boot_result)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = psx_clean, statistic = boot_qda, R = 500)\n\n\nBootstrap Statistics :\n     original       bias    std. error\nt1* 0.5944018 -0.003982437  0.01522457\n\n\n\n\n\n\n4. Bootstrap for KNN\n\nDefine a Bootstrap Function:\n\nboot_knn &lt;- function(data, indices) {\n  train_data &lt;- data[indices, ]\n  test_data &lt;- data[-indices, ]\n  knn_model &lt;- knn(\n    train = train_data[, c(\"lag1\", \"lag2\", \"lag3\", \"lag4\", \"lag5\", \"vol\")],\n    test = test_data[, c(\"lag1\", \"lag2\", \"lag3\", \"lag4\", \"lag5\", \"vol\")],\n    cl = train_data$direction,\n    k = 5\n  )\n  accuracy &lt;- mean(knn_model == test_data$direction)\n  return(accuracy)\n}\n\n\n\nApply Bootstrapping:\n\nset.seed(123)\nknn_boot_result &lt;- boot(data = psx_clean, statistic = boot_knn, R = 500)\nprint(knn_boot_result)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = psx_clean, statistic = boot_knn, R = 500)\n\n\nBootstrap Statistics :\n    original  bias    std. error\nt1*      NaN     NaN  0.01808283\n\n\n\n\n\n\n5. Bootstrap for Decision Trees\n\nDefine a Bootstrap Function:\n\nboot_dt &lt;- function(data, indices) {\n  sampled_data &lt;- data[indices, ]\n  tree_model &lt;- rpart(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol, \n                      data = sampled_data, method = \"class\")\n  predictions &lt;- predict(tree_model, newdata = data, type = \"class\")\n  accuracy &lt;- mean(predictions == data$direction)\n  return(accuracy)\n}\n\n\n\nApply Bootstrapping:\n\nset.seed(123)\ndt_boot_result &lt;- boot(data = psx_clean, statistic = boot_dt, R = 500)\nprint(dt_boot_result)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = psx_clean, statistic = boot_dt, R = 500)\n\n\nBootstrap Statistics :\n     original        bias    std. error\nt1* 0.6262349 -0.0005060373   0.0107856\n\n\n\n\n\n\nSummarize Results:\nAggregate accuracy or coefficients across models to compare:\n\nlist(\n  logistic = mean(logistic_boot_result$t),\n  lda = mean(lda_boot_result$t),\n  qda = mean(qda_boot_result$t),\n  knn = mean(knn_boot_result$t),\n  decision_tree = mean(dt_boot_result$t)\n)\n\n$logistic\n[1] -0.09619826\n\n$lda\n[1] 0.6080834\n\n$qda\n[1] 0.5904193\n\n$knn\n[1] 0.5456178\n\n$decision_tree\n[1] 0.6257289"
  },
  {
    "objectID": "docs/syllabus.html",
    "href": "docs/syllabus.html",
    "title": "Machine Learning, Big Data & AI in Economics (MS)",
    "section": "",
    "text": "This course introduces statistical learning and high-dimensional methods for prediction and decision support in economics and public policy. We will emphasize practical pipelines, responsible use of models, and the distinction between prediction and causal inference.\nPrerequisites: Econometrics or an equivalent quantitative course.\nTexts (core): - James, Witten, Hastie, Tibshirani (2021) ISLR (2e) - Taddy (2019) Business Data Science (BDS) - Varian (2014) “Big Data: New Tricks for Econometrics”, JEP\nUseful references: DSB, ESL, AEA ML/Big Data workshops (2018/2023)."
  },
  {
    "objectID": "docs/syllabus.html#course-description",
    "href": "docs/syllabus.html#course-description",
    "title": "Machine Learning, Big Data & AI in Economics (MS)",
    "section": "",
    "text": "This course introduces statistical learning and high-dimensional methods for prediction and decision support in economics and public policy. We will emphasize practical pipelines, responsible use of models, and the distinction between prediction and causal inference.\nPrerequisites: Econometrics or an equivalent quantitative course.\nTexts (core): - James, Witten, Hastie, Tibshirani (2021) ISLR (2e) - Taddy (2019) Business Data Science (BDS) - Varian (2014) “Big Data: New Tricks for Econometrics”, JEP\nUseful references: DSB, ESL, AEA ML/Big Data workshops (2018/2023)."
  },
  {
    "objectID": "docs/syllabus.html#learning-outcomes",
    "href": "docs/syllabus.html#learning-outcomes",
    "title": "Machine Learning, Big Data & AI in Economics (MS)",
    "section": "2 Learning Outcomes",
    "text": "2 Learning Outcomes\n\nBuild, validate, and compare predictive models in R and Python.\nUnderstand bias–variance tradeoffs, regularization, and model selection.\nHandle text data and high-dimensional tabular data.\nCommunicate model results to non-technical audiences (policy memos).\nRecognize scope, limitations, fairness, and reproducibility concerns."
  },
  {
    "objectID": "docs/syllabus.html#assessment",
    "href": "docs/syllabus.html#assessment",
    "title": "Machine Learning, Big Data & AI in Economics (MS)",
    "section": "3 Assessment",
    "text": "3 Assessment\n\nWeekly labs (best 5 of 7) — 15%\nProblem Sets (2) — 10%\nMidterm check (Week 8 quiz) — 20%\nCapstone project —5%\nFinal — 50%"
  },
  {
    "objectID": "docs/syllabus.html#software",
    "href": "docs/syllabus.html#software",
    "title": "Machine Learning, Big Data & AI in Economics (MS)",
    "section": "4 Software",
    "text": "4 Software\n\nPython: pandas, scikit-learn, imbalanced-learn, SHAP, Dask/Spark (demo)\nR: tidyverse, tidymodels, textrecipes, glmnet, ranger, xgboost\nQuarto for reproducible documents and slides"
  },
  {
    "objectID": "docs/syllabus.html#weekly-plan-15-weeks",
    "href": "docs/syllabus.html#weekly-plan-15-weeks",
    "title": "Machine Learning, Big Data & AI in Economics (MS)",
    "section": "5 Weekly Plan (15 Weeks)",
    "text": "5 Weekly Plan (15 Weeks)\n\n5.1 Week 1 — Orientation, Prediction vs Causation, OLS as ERM\n\nLoss functions, risk vs empirical risk, train/validation/test.\nOLS baseline as MSE minimizer; evaluation metrics (MAE/MSE/R²).\nLab: Load CPI/WDI; split data; baseline OLS; reproducible folder structure.\nReading: ISLR Ch. 2–3; Varian (2014) selections.\n\n\n\n5.2 Week 2 — Data Pipelines & “Big(ger) Data” Hygiene\n\nTidy data, joins, leakage, missingness; intro to SQL thinking.\nLab: Join CPI + fuel prices + holidays; sklearn/tidymodels Pipeline.\nReading: DSB Ch. 2–3; BDS Ch. 1–2.\n\n\n\n5.3 Week 3 — Model Selection & Resampling\n\nBias–variance; cross-validation (k-fold, time-series); AIC/BIC intuition.\nLab: K-fold CV on linear baselines; pick a model defensibly.\nReading: ISLR Ch. 5.\n\n\n\n5.4 Week 4 — Ridge Regression (ℓ2)\n\nOverfitting; shrinkage; choosing λ; interpretability.\nLab: Ridge path + CV; macro nowcasting demo.\nReading: ISLR Ch. 6 (Ridge).\n\n\n\n5.5 Week 5 — LASSO (ℓ1) & Feature Selection\n\nSparsity vs shrinkage; correlated regressors.\nLab: LASSO path; stability selection; compare to ridge.\nReading: ISLR Ch. 6 (LASSO).\n\n\n\n5.6 Week 6 — Nonlinear Regression (Splines, k-NN, Polynomials)\n\nControlled flexibility; local vs global fits.\nLab: Splines vs linear/ridge/LASSO; select degrees of freedom.\nReading: ISLR Ch. 7–8.\n\n\n\n5.7 Week 7 — Trees & Random Forests\n\nCART; impurity; RF bagging, OOB error; pitfalls in variable importance.\nLab: Household classification (poverty proxy); PDP/ICE plots.\nReading: ISLR Ch. 8.\n\n\n\n5.8 Week 8 — Boosting (GBM/XGBoost/LightGBM)\n\nAdditive modeling; learning rate, depth, early stopping; class imbalance.\nLab: Tune boosting; PR-AUC; class weights.\nReading: BDS Ch. 6–7.\n\n\n\n5.9 Week 9 — Classification Foundations & Decision Thresholds\n\nLogistic regression; calibration; cost-sensitive decisions.\nLab: Calibrate probabilities; decision curves; threshold memo.\nReading: ISLR Ch. 4; DSB Ch. 7–9.\n\n\n\n5.10 Week 10 — Dimensionality Reduction & Clustering\n\nPCA; k-means; hierarchical clustering; nearest neighbors.\nLab: PCA on firm/sector features; cluster districts; stability.\nReading: ISLR Ch. 10.\n\n\n\n5.11 Week 11 — Text as Data I (Classical NLP)\n\nBag-of-words, TF-IDF, n-grams; sentiment; risks with media text.\nLab: Classify policy statements (English/Urdu); linear classifier.\nReading: BDS Ch. 9.\n\n\n\n5.12 Week 12 — Text as Data II (Embeddings & Intro to LLMs)\n\nEmbeddings; using transformer features; ethics & privacy.\nLab: Embedding features to improve Week-11 classifier; SHAP on text.\nReading: Selected survey/tutorial excerpts.\n\n\n\n5.13 Week 13 — Big Data at Scale\n\nWhen to scale; partitioning, parallelism; Spark/Dask; Parquet.\nLab: Re-run a pipeline on Spark/Dask subset; profile runtime/memory.\nReading: BDS Ch. 10–12.\n\n\n\n5.14 Week 14 — Interpretability, Fairness, Reproducibility, MLOps Lite\n\nDiagnostics beyond accuracy; SHAP; fairness metrics; data/model cards; CI basics.\nLab: SHAP on boosted model; fairness audit; Quarto report.\nReading: Varian (2014) revisit; practice notes on interpretability.\n\n\n\n5.15 Week 15 — Capstone Symposium\n\n8-minute presentations + Q&A; final repo and memo due."
  },
  {
    "objectID": "docs/syllabus.html#capstone-project",
    "href": "docs/syllabus.html#capstone-project",
    "title": "Machine Learning, Big Data & AI in Economics (MS)",
    "section": "6 Capstone Project",
    "text": "6 Capstone Project\nTeams of 2–3 choose one: - Nowcasting inflation with high-frequency proxies. - Targeting aid/risk scoring under class imbalance + fairness. - Energy demand forecasting for load management. - Text classification of monetary/fiscal communication. - Revenue anomaly/outlier detection with explanations.\nDeliverables: Reproducible repo, data card, model card, fairness & interpretability analysis, policy memo (6–8 pages) with actionable recommendations."
  },
  {
    "objectID": "docs/syllabus.html#academic-integrity-ai-tooling-policy",
    "href": "docs/syllabus.html#academic-integrity-ai-tooling-policy",
    "title": "Machine Learning, Big Data & AI in Economics (MS)",
    "section": "7 Academic Integrity & AI-Tooling Policy",
    "text": "7 Academic Integrity & AI-Tooling Policy\nUse AI tools for boilerplate and debugging only with attribution. You are responsible for understanding and explaining any code you submit."
  },
  {
    "objectID": "docs/lec1.html#todays-agenda",
    "href": "docs/lec1.html#todays-agenda",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n\nThe AI Revolution in Economics\nMachine Learning Fundamentals\n\nBig Data & Alternative Data Sources\nCourse Technology Stack & Roadmap"
  },
  {
    "objectID": "docs/lec1.html#from-econometrics-to-ai",
    "href": "docs/lec1.html#from-econometrics-to-ai",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "From Econometrics to AI",
    "text": "From Econometrics to AI\n\n\n🏛️ Traditional Economics\n\nTheory-driven hypotheses\nSmall, clean datasets\nLinear relationships\nCausal identification\nStatistical significance\n\n\n🤖 AI-Powered Economics\n\nData-driven pattern discovery\nMassive, complex datasets\nNon-linear relationships\nPredictive optimization\nCross-validation performance\n\n\n\n\n\n\n\n\nNote\n\n\nWe’re training intelligent systems that discover patterns and optimize decisions at scale"
  },
  {
    "objectID": "docs/lec1.html#core-definition",
    "href": "docs/lec1.html#core-definition",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "Core Definition",
    "text": "Core Definition\n\nMachine Learning: Algorithms that automatically improve performance through experience, learning patterns from data without explicit programming"
  },
  {
    "objectID": "docs/lec1.html#the-learning-process",
    "href": "docs/lec1.html#the-learning-process",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "The Learning Process",
    "text": "The Learning Process\n\n\n\n\n\nflowchart LR\n    A[Training Data] --&gt; B[Algorithm]\n    B --&gt; C[Trained Model] \n    C --&gt; D[Predictions]\n    D --&gt; E[Performance Feedback]\n    E --&gt; B\n\n\n\n\n\n\n\n\n🧠 Intelligence\nAdaptive algorithms\n\n🔍 Discovery\nPattern recognition\n\n⚡ Scale\nReal-time processing"
  },
  {
    "objectID": "docs/lec1.html#the-5-vs-of-economic-big-data",
    "href": "docs/lec1.html#the-5-vs-of-economic-big-data",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "The 5 V’s of Economic Big Data",
    "text": "The 5 V’s of Economic Big Data\n\n\n📊 Volume\nPetabytes of transaction, satellite, social media data\n🚀 Velocity\nHigh-frequency trading, real-time payments, streaming indicators\n🌍 Variety\nStructured, unstructured, semi-structured data\n\n✅ Veracity\nData quality, noise, missing values\n💰 Value\nCompetitive advantages from data insights\n\n\n\n\n\n\n\nTip\n\n\nExample: Google Flu Trends uses search queries to predict disease outbreaks faster than traditional health reporting systems"
  },
  {
    "objectID": "docs/lec1.html#narrow-ai-applications",
    "href": "docs/lec1.html#narrow-ai-applications",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "Narrow AI Applications",
    "text": "Narrow AI Applications\n\n\n💹 Algorithmic Trading\nHigh-frequency decision making\n🏪 Dynamic Pricing\nReal-time price optimization\n🎯 Personalization\nCustomized recommendations\n\n🏛️ Policy Optimization\nAI simulations for government\n🔍 Risk Assessment\nIntelligent credit scoring\n📊 Nowcasting\nReal-time economic indicators\n\n\n\n\n\n\n\nImportant\n\n\nReal Impact: Amazon adjusts 2.5 million prices daily using AI algorithms"
  },
  {
    "objectID": "docs/lec1.html#end-to-end-process",
    "href": "docs/lec1.html#end-to-end-process",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "End-to-End Process",
    "text": "End-to-End Process\n\n\n\n\n\nflowchart TD\n    A[Data Collection] --&gt; B[Data Cleaning]\n    B --&gt; C[Feature Engineering] \n    C --&gt; D[Model Training]\n    D --&gt; E[Validation]\n    E --&gt; F[Deployment]\n    F --&gt; G[Monitoring]\n    G --&gt; H[Retraining]\n    H --&gt; D"
  },
  {
    "objectID": "docs/lec1.html#key-components",
    "href": "docs/lec1.html#key-components",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "Key Components",
    "text": "Key Components\n\nData Sources: APIs, databases, web scraping, sensors\nFeature Engineering: Transform raw data into model inputs\nModel Training: Algorithm learns from historical data\nDeployment: Production systems serving real-time predictions\nMLOps: Continuous monitoring and model updates"
  },
  {
    "objectID": "docs/lec1.html#learning-from-examples",
    "href": "docs/lec1.html#learning-from-examples",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "Learning from Examples",
    "text": "Learning from Examples\n\n\n\n\n\n\nNote\n\n\nDefinition: Training algorithms on labeled input-output pairs to predict outcomes for new data\n\n\n\n\n\n📈 Regression Problems\nPredicting continuous values\n\nGDP growth forecasting\nStock price prediction\n\nHouse price estimation\nEnergy demand modeling\n\n\n🎯 Classification Problems\nPredicting categories\n\nRecession prediction\nCredit default detection\nMarket regime identification\nFraud detection\n\n\nKey Insight: Success measured by out-of-sample prediction accuracy"
  },
  {
    "objectID": "docs/lec1.html#discovering-hidden-patterns",
    "href": "docs/lec1.html#discovering-hidden-patterns",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "Discovering Hidden Patterns",
    "text": "Discovering Hidden Patterns\n\n\n\n\n\n\nNote\n\n\nDefinition: Finding structures in data without labeled examples - autonomous pattern discovery\n\n\n\n\n\n🔍 Clustering\nGroup similar observations\n\nCustomer segmentation\nMarket regimes\n\nEconomic regions\n\n\n📉 Dimensionality Reduction\nSimplify complex data\n\nFactor models\nData visualization\nFeature compression\n\n\n🚨 Anomaly Detection\nIdentify unusual patterns\n\nFinancial fraud\nMarket manipulation\nEconomic outliers"
  },
  {
    "objectID": "docs/lec1.html#complementary-approaches",
    "href": "docs/lec1.html#complementary-approaches",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "Complementary Approaches",
    "text": "Complementary Approaches\n\n\n\nAspect\nTraditional Econometrics\nMachine Learning\n\n\n\n\nGoal\nCausal inference\nPrediction accuracy\n\n\nData\nSmall, curated\nLarge, complex\n\n\nModels\nLinear, interpretable\nFlexible, black-box\n\n\nValidation\nStatistical tests\nCross-validation\n\n\nFocus\nUnderstanding\nPerformance\n\n\n\n\n\n\n\n\n\nImportant\n\n\nModern Synthesis: Causal ML combines econometric rigor with ML prediction power"
  },
  {
    "objectID": "docs/lec1.html#the-fundamental-challenge",
    "href": "docs/lec1.html#the-fundamental-challenge",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "The Fundamental Challenge",
    "text": "The Fundamental Challenge\n\\[\\text{Expected Test Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\\]\n\n\n🎯 Bias\nSystematic error\nOversimplified models miss important patterns\n\n📊 Variance\nRandom error\nOvercomplex models memorize noise\n\n⚖️ Sweet Spot\nOptimal complexity\nBalance bias and variance\n\nStrategy: Use cross-validation to find optimal model complexity"
  },
  {
    "objectID": "docs/lec1.html#from-simple-to-complex",
    "href": "docs/lec1.html#from-simple-to-complex",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "From Simple to Complex",
    "text": "From Simple to Complex\n\n\n🟢 Simple\nLinear Models\n✅ Interpretable, fast\n❌ May underfit\nUse for: Policy analysis, baselines\n\n🟡 Moderate\nEnsemble Methods\n✅ Good balance\n❌ Less interpretable\nUse for: Most economic problems\n\n🔴 Complex\nDeep Learning\n✅ Captures complexity\n❌ Black box, needs big data\nUse for: Image, text, time series"
  },
  {
    "objectID": "docs/lec1.html#beyond-traditional-economic-data",
    "href": "docs/lec1.html#beyond-traditional-economic-data",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "Beyond Traditional Economic Data",
    "text": "Beyond Traditional Economic Data\n\n\n🛰️ Satellite Imagery - Economic activity tracking - Crop yield estimation - Urban development - Night lights → GDP\n📱 Mobile & Location\n- Consumer mobility patterns - Real-time sentiment - Economic activity hotspots\n\n🌐 Web & Social Media - Search trends - News sentiment analysis\n- Job postings data - Social media indicators\n💳 Transaction Data - Credit card spending - Digital payments - E-commerce patterns - Supply chain tracking"
  },
  {
    "objectID": "docs/lec1.html#real-world-applications",
    "href": "docs/lec1.html#real-world-applications",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "Real-World Applications",
    "text": "Real-World Applications\n\n\n💹 Financial Markets - HFT algorithms process millions of trades/second - Robo-advisors manage $1+ trillion in assets - AI detects market manipulation in real-time\n🏪 E-Commerce\n- Amazon’s dynamic pricing (2.5M daily changes) - Personalized recommendations drive 35% of sales - Supply chain optimization with ML\n\n🏛️ Government & Policy - Tax compliance algorithms - Social benefit fraud detection - Traffic optimization systems - Public health monitoring\n🌍 Global Economics - Central bank policy simulations\n- Trade flow predictions - Currency intervention algorithms"
  },
  {
    "objectID": "docs/lec1.html#tools-of-the-trade",
    "href": "docs/lec1.html#tools-of-the-trade",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "Tools of the Trade",
    "text": "Tools of the Trade\n\n\n🐍 Core Languages - Python (pandas, scikit-learn, TensorFlow) - R (tidyverse, caret, quantmod, tidymodels)\n- SQL (data querying) - Scala (big data processing)\n☁️ Cloud Platforms - AWS (SageMaker, EC2, S3) - Google Cloud (AI Platform, BigQuery) - Azure (Machine Learning Studio)\n\n🚀 MLOps Tools - Docker (containerization) - Kubernetes (orchestration) - MLflow (experiment tracking) - Git (version control)\n📊 Visualization - Plotly (interactive charts) - Tableau (business intelligence) - D3.js (web visualizations)"
  },
  {
    "objectID": "docs/lec1.html#week-journey",
    "href": "docs/lec1.html#week-journey",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "16-Week Journey",
    "text": "16-Week Journey\n\n\n🔨 Weeks 1-4: Foundations - Python and R ML ecosystem setup - Data preprocessing & EDA - Linear models & regularization - Cross-validation strategies\n🚀 Weeks 5-8: Advanced ML - Tree-based models (XGBoost) - Neural networks & deep learning - Unsupervised learning techniques - Time series forecasting\n\n🏛️ Weeks 9-12: Economic Applications - Causal inference with ML - NLP for economic text analysis - Nowcasting with alternative data - Algorithmic trading strategies\n⚡ Weeks 13-16: Production - Big data with Spark - MLOps & deployment - Real-time systems - Capstone project"
  },
  {
    "objectID": "docs/lec1.html#hands-on-learning",
    "href": "docs/lec1.html#hands-on-learning",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "Hands-On Learning",
    "text": "Hands-On Learning\n📝 Assignments (40%) - Weekly R/Python programming exercises - Data preprocessing challenges - Model implementation from scratch - Economic case studies\n🏗️ Projects (40%) - Mid-term: Economic forecasting with alternative data - Final: End-to-end ML system for economic problem - Presentations: Industry-style model deployments\n📊 Participation (20%) - Kaggle competitions - Discussion forums - Peer code reviews"
  },
  {
    "objectID": "docs/lec1.html#python-ml-ecosystem",
    "href": "docs/lec1.html#python-ml-ecosystem",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "Python ML Ecosystem",
    "text": "Python ML Ecosystem\n🎯 Learning Objectives: - Set up complete ML development environment - Master pandas for economic data manipulation\n- Build your first predictive model - Version control with Git\n📋 Pre-Class Setup: - Install Anaconda Python 3.9+ - Create GitHub account - Access Google Colab - Download course datasets\n🔬 Lab Session: - Quarto/Jupyter notebook introduction - Economic data exploration - Simple linear regression implementation - Model evaluation basics"
  },
  {
    "objectID": "docs/lec1.html#key-takeaways",
    "href": "docs/lec1.html#key-takeaways",
    "title": "Machine Learning, Big Data & AI in Economics",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n✅ ML revolutionizes economic analysis and decision-making\n✅ Big data provides unprecedented insights into economic behavior\n✅ AI systems increasingly drive real-world economic outcomes\n✅ This course bridges theory and practical implementation\nReady to start your ML journey in economics? 🚀"
  },
  {
    "objectID": "docs/lec2.html#what-is-machine-learning",
    "href": "docs/lec2.html#what-is-machine-learning",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\n🤖 Machine Learning\n\nA subset of Artificial Intelligence that enables systems to learn from data without explicit programming.\n\nKey Economic Insight 💡\nML focuses on self-learning algorithms that derive knowledge from data to predict economic outcomes and inform policy decisions.\n\n\nIn economics, ML helps us understand complex relationships in economic data that traditional econometric methods might miss."
  },
  {
    "objectID": "docs/lec2.html#the-ai-hierarchy-in-economics",
    "href": "docs/lec2.html#the-ai-hierarchy-in-economics",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "The AI Hierarchy in Economics",
    "text": "The AI Hierarchy in Economics\n\n\n🧠 Artificial Intelligence\nBroadest concept\n\nMimicking human economic decision-making\nAutomated trading systems\nPolicy recommendation engines\n\n\nEconomic Example: Central bank decision support systems\n\n\n⚙️ Machine Learning\nSubset of AI\n\nLearning from economic data\nPredicting market outcomes\nPattern recognition in behavior\n\n\nEconomic Example: Credit scoring models, GDP forecasting\n\n\n🔗 Deep Learning\nSubset of ML\n\nScalable ML for big economic data\nAutomated feature extraction\nComplex economic relationships\n\n\nEconomic Example: Analyzing satellite data for economic activity\n\n\n\nThis hierarchy is particularly important in economics where we deal with increasingly complex and high-dimensional data."
  },
  {
    "objectID": "docs/lec2.html#types-of-ml-in-economics",
    "href": "docs/lec2.html#types-of-ml-in-economics",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Types of ML in Economics",
    "text": "Types of ML in Economics\n\n\n📊 Three Main Approaches\n\nSupervised Learning ✅ Learning from labeled economic data\nUnsupervised Learning 🧩 Discovering hidden economic patterns\n\nReinforcement Learning 🏆 Learning optimal economic policies\n\n\n\n🎯 Economic Applications Preview\n\nSupervised: Recession prediction, stock price forecasting\nUnsupervised: Market segmentation, economic clustering\nReinforcement: Optimal monetary policy, algorithmic trading"
  },
  {
    "objectID": "docs/lec2.html#supervised-learning-in-economics",
    "href": "docs/lec2.html#supervised-learning-in-economics",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Supervised Learning in Economics",
    "text": "Supervised Learning in Economics\n\n\n\n\n\n\nDefinition\n\n\nUses labeled economic datasets to train algorithms for classification or prediction\n\n\n\n\n\n📋 Classification\nGroups economic entities into predefined categories\nEconomic Examples: - 🏦 Bank loan approval (approve/deny) - 📈 Market regime classification (bull/bear) - 🏢 Firm bankruptcy prediction - 👥 Consumer credit scoring\n\nCode Example:\n# Predicting recession probability\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_economic_indicators, y_recession_labels)\n\n\n📈 Regression\nPredicts continuous economic values\nEconomic Examples: - 💰 House price prediction - 📊 GDP growth forecasting\n- 💱 Exchange rate modeling - 🛢️ Commodity price prediction\n\nCode Example:\n# Predicting housing prices\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_housing_features, y_prices)\n\n\n\nSupervised learning is the most common type in economics because we often have historical labeled data."
  },
  {
    "objectID": "docs/lec2.html#economic-data-in-supervised-learning",
    "href": "docs/lec2.html#economic-data-in-supervised-learning",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Economic Data in Supervised Learning",
    "text": "Economic Data in Supervised Learning\n\n🏦 Banking Example📈 Market Prediction🏛️ Policy Analysis\n\n\n\n\n\n\n\nflowchart LR\n    A[📊 Customer Data] --&gt; B[🤖 ML Model]\n    B --&gt; C[✅ Approve/❌ Deny]\n    \n    A1[Income] --&gt; A\n    A2[Credit History] --&gt; A  \n    A3[Employment] --&gt; A\n    A4[Debt Ratio] --&gt; A\n\n\n\n\n\n\nFeatures: Income, credit score, employment history, debt-to-income ratio\nTarget: Loan approval decision (binary classification)\n\n\n\n\n\n\n\nflowchart LR\n    A[📊 Market Data] --&gt; B[🤖 ML Model]\n    B --&gt; C[📈 Price Forecast]\n    \n    A1[Technical Indicators] --&gt; A\n    A2[Economic News] --&gt; A  \n    A3[Trading Volume] --&gt; A\n    A4[Sentiment Data] --&gt; A\n\n\n\n\n\n\nFeatures: Technical indicators, news sentiment, trading volume, economic indicators\nTarget: Future stock price (regression)\n\n\n\n\n\n\n\nflowchart LR\n    A[📊 Policy Data] --&gt; B[🤖 ML Model]\n    B --&gt; C[🎯 Policy Impact]\n    \n    A1[GDP Growth] --&gt; A\n    A2[Inflation Rate] --&gt; A  \n    A3[Interest Rates] --&gt; A\n    A4[Employment] --&gt; A\n\n\n\n\n\n\nFeatures: Macroeconomic indicators, policy variables\nTarget: Policy effectiveness score (regression/classification)"
  },
  {
    "objectID": "docs/lec2.html#interactive-demo-supervised-learning",
    "href": "docs/lec2.html#interactive-demo-supervised-learning",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Interactive Demo: Supervised Learning",
    "text": "Interactive Demo: Supervised Learning\n\n🎮 Try It Yourself📊 Understanding the Model\n\n\n\n\n🏠 House Price Predictor\n\nAdjust the sliders to see how features affect predicted price:\n\nSquare Feet: 2000 \nBedrooms: 3 \nLocation Score: 7 \n\n\n\nPredicted Price: $450,000\n\n\n\n\n\nSimple Linear Model:\nPrice = 150 × sqft + 25,000 × bedrooms + 10,000 × location + base_price\nEconomic Interpretation: - Each additional square foot adds ~$150 to value - Each bedroom adds ~$25,000 to value\n- Location premium can add up to $100,000 - This demonstrates marginal effects in economics"
  },
  {
    "objectID": "docs/lec2.html#unsupervised-learning-in-economics",
    "href": "docs/lec2.html#unsupervised-learning-in-economics",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Unsupervised Learning in Economics",
    "text": "Unsupervised Learning in Economics\n\n\n\n\n\n\nDefinition\n\n\nAnalyzes unlabeled economic data to discover hidden patterns without human intervention\n\n\n\n\n\n🔍 Clustering\nGroups similar economic entities into natural clusters\nEconomic Applications: - 🏪 Customer market segmentation - 🌍 Country economic development clusters\n- 📊 Industry classification - 🏙️ Regional economic similarity\n\nReal Example: World Bank uses clustering to group countries by development indicators\n\n\n↘️ Dimensionality Reduction\nReduces complexity while preserving economic information\nEconomic Applications: - 📈 Financial risk factor identification - 🏢 Principal components of economic indicators - 📊 Data visualization for policy makers - 🔄 Feature selection for economic models\n\nReal Example: Federal Reserve uses PCA to create financial stress indices\n\n\n\nUnsupervised learning is particularly valuable in economics for exploratory data analysis and discovering unknown economic relationships."
  },
  {
    "objectID": "docs/lec2.html#economic-clustering-visualization",
    "href": "docs/lec2.html#economic-clustering-visualization",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Economic Clustering Visualization",
    "text": "Economic Clustering Visualization\n\n🌍 Country Development Clusters🏦 Bank Customer Segments📊 Economic Indicators\n\n\n\n\n\n\n\ngraph TB\n    subgraph \"🏆 Developed Countries\"\n        A1[USA 🇺🇸]\n        A2[Germany 🇩🇪] \n        A3[Japan 🇯🇵]\n    end\n    \n    subgraph \"📈 Emerging Markets\"\n        B1[China 🇨🇳]\n        B2[India 🇮🇳]\n        B3[Brazil 🇧🇷]\n    end\n    \n    subgraph \"🌱 Developing Countries\"\n        C1[Kenya 🇰🇪]\n        C2[Bangladesh 🇧🇩]\n        C3[Vietnam 🇻🇳]\n    end\n\n\n\n\n\n\nClustering Features: GDP per capita, HDI, education index, infrastructure\n\n\n\n\n\n\n\ngraph LR\n    subgraph \"💎 Premium Customers\"\n        P1[High Income]\n        P2[Multiple Products]\n        P3[Low Risk]\n    end\n    \n    subgraph \"🎯 Growing Customers\"  \n        G1[Medium Income]\n        G2[Expanding Needs]\n        G3[Medium Risk]\n    end\n    \n    subgraph \"🌱 Entry Level\"\n        E1[Lower Income]\n        E2[Basic Products] \n        E3[Higher Risk]\n    end\n\n\n\n\n\n\nBusiness Value: Targeted marketing, personalized products, risk management\n\n\nPrincipal Component Analysis Example:\n\n\n\nComponent\nInterpretation\nVariables\n\n\n\n\nPC1 (40%)\nEconomic Development\nGDP, Education, Health\n\n\nPC2 (25%)\nInnovation Capacity\nR&D, Patents, Tech\n\n\nPC3 (15%)\nFinancial Stability\nDebt, Inflation, Currency\n\n\n\nReduces 20+ indicators to 3 meaningful components"
  },
  {
    "objectID": "docs/lec2.html#reinforcement-learning-in-economics",
    "href": "docs/lec2.html#reinforcement-learning-in-economics",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Reinforcement Learning in Economics",
    "text": "Reinforcement Learning in Economics\n\n\n\n\n\n\nDefinition\n\n\nAn agent learns optimal economic policies through trial and error, receiving rewards for good decisions and penalties for bad ones\n\n\n\n\n\n🎯 Key Economic Concepts\n\nAgent: Central bank, trader, firm\nEnvironment: Economic system, market\nActions: Policy decisions, trades, investments\n\nRewards: Economic outcomes, profits, welfare\n\n\n🔄 Learning Process\n\nTake economic action\nObserve market response\n\nReceive reward/penalty\nUpdate strategy\nRepeat until optimal\n\n\n\n💼 Economic Applications\n\n🏛️ Monetary Policy: Optimal interest rate setting\n📈 Algorithmic Trading: Automated investment strategies\n🏢 Supply Chain: Inventory and pricing optimization\n🎮 Auction Design: Optimal bidding strategies\n🌱 Climate Policy: Carbon pricing mechanisms\n\n\nReal Example: JPMorgan uses RL for optimal trade execution to minimize market impact\n\n\n\nReinforcement learning is particularly powerful for dynamic economic decision-making where the environment responds to actions."
  },
  {
    "objectID": "docs/lec2.html#rl-in-monetary-policy",
    "href": "docs/lec2.html#rl-in-monetary-policy",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "RL in Monetary Policy",
    "text": "RL in Monetary Policy\n\n🏛️ Central Bank Agent📊 Policy Learning Process🎯 Economic Outcomes\n\n\n\n\n\n\n\nflowchart TD\n    A[🏛️ Central Bank&lt;br/&gt;Agent] --&gt; B[📊 Economic State&lt;br/&gt;Inflation, GDP, Employment]\n    B --&gt; C[🎯 Policy Action&lt;br/&gt;Interest Rate Decision]\n    C --&gt; D[📈 Economic Response&lt;br/&gt;Market Reaction]\n    D --&gt; E[🏆 Reward&lt;br/&gt;Economic Stability]\n    E --&gt; A\n    \n    style A fill:#e1f5fe\n    style E fill:#e8f5e8\n\n\n\n\n\n\n\n\nState Variables: - Current inflation rate - GDP growth rate\n- Unemployment rate - Market volatility\nActions: - Raise interest rates (+0.25%) - Lower interest rates (-0.25%)\n- Keep rates unchanged\nReward Function:\nReward = -|inflation - target|² - |unemployment - natural_rate|² - |gdp_volatility|\n\n\n\n\nTraditional Rules: - Fixed policy responses - Limited adaptability - May miss complex interactions\nExample: Taylor Rule\nRate = neutral + 1.5×(inflation - target) + 0.5×GDP_gap\n\nRL-Based Policy: - Adaptive responses - Learns from experience - Captures non-linear relationships\nAdvantage: Can handle regime changes and structural breaks automatically"
  },
  {
    "objectID": "docs/lec2.html#comparing-ml-approaches-in-economics",
    "href": "docs/lec2.html#comparing-ml-approaches-in-economics",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Comparing ML Approaches in Economics",
    "text": "Comparing ML Approaches in Economics\n\n\n\n\n\n\n\n\n\n\n\nApproach\nData Type\nEconomic Use Case\nExample\nPros\nCons\n\n\n\n\nSupervised ✅\nLabeled historical\nPrediction & Classification\nCredit scoring, recession forecasting\nHigh accuracy, interpretable\nNeeds labeled data, may not adapt\n\n\nUnsupervised 🧩\nUnlabeled current\nPattern discovery\nMarket segmentation, anomaly detection\nNo labels needed, finds hidden patterns\nHard to validate, less precise\n\n\nReinforcement 🏆\nInteractive feedback\nDynamic optimization\nTrading algorithms, policy design\nAdapts to changes, optimal decisions\nComplex to implement, needs simulation\n\n\n\n\n🎯 Economic Insight\n\nSupervised: Best for prediction problems with historical precedent\nUnsupervised: Best for exploration of new economic phenomena\n\nReinforcement: Best for optimization in dynamic economic environments\n\n\n\nIn practice, economists often combine these approaches - using unsupervised learning for exploration, supervised learning for prediction, and reinforcement learning for policy optimization."
  },
  {
    "objectID": "docs/lec2.html#real-world-economic-ml-applications",
    "href": "docs/lec2.html#real-world-economic-ml-applications",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Real-World Economic ML Applications",
    "text": "Real-World Economic ML Applications\n\n🏦 Financial Services🏛️ Government & Policy🏢 Business Applications\n\n\nCredit Scoring & Risk Management\n\nBanks: Wells Fargo, JPMorgan use ML for loan decisions\nFeatures: Income, credit history, spending patterns, social data\nImpact: Reduced default rates by 15-20%\n\nAlgorithmic Trading\n\nFirms: Renaissance Technologies, Two Sigma\nTechniques: Deep RL, NLP for news analysis\nAssets: Over $1 trillion managed by ML algorithms\n\n\n\nEconomic Forecasting\n\nFed: Uses ML for GDP, inflation predictions\nIMF: Nowcasting economic growth using satellite data\nWorld Bank: Poverty mapping with mobile phone data\n\nTax Compliance\n\nIRS: ML for fraud detection (98% accuracy)\nEU: VAT gap prediction using transaction data\n\n\n\nDynamic Pricing\n\nAirlines: Real-time price optimization\nRide-sharing: Uber’s surge pricing algorithm\n\nE-commerce: Amazon’s recommendation engine\n\nSupply Chain\n\nWalmart: Demand forecasting with weather data\nManufacturing: Predictive maintenance\nLogistics: Route optimization"
  },
  {
    "objectID": "docs/lec2.html#economic-data-challenges-in-ml",
    "href": "docs/lec2.html#economic-data-challenges-in-ml",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Economic Data Challenges in ML",
    "text": "Economic Data Challenges in ML\n\n\n⚠️ Unique Challenges\n🌊 Non-stationarity - Economic relationships change over time - Structural breaks and regime changes - Models need constant updating\n📊 Limited Data\n- Macroeconomic data: quarterly/annual - Financial crises are rare events - Privacy constraints on personal data\n🔗 Endogeneity - Variables influence each other - Causation vs. correlation issues - Policy affects the data generation process\n\n✅ ML Solutions\n🔄 Online Learning - Models update with new data - Adaptive algorithms for regime changes - Ensemble methods for robustness\n📈 Transfer Learning - Use patterns from similar economies - Cross-country knowledge transfer - Synthetic data generation\n🎯 Causal ML - Treatment effect estimation - Instrumental variables in ML - Randomized controlled trials\n\n\n💡 Key Takeaway\nEconomic ML requires careful consideration of causality, temporal dynamics, and policy implications beyond standard ML metrics."
  },
  {
    "objectID": "docs/lec2.html#interactive-economic-scenario",
    "href": "docs/lec2.html#interactive-economic-scenario",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Interactive Economic Scenario",
    "text": "Interactive Economic Scenario\n\n🎮 Central Bank Simulation📊 Learning from Decisions\n\n\n\n\n🏛️ You are the Central Bank Governor\n\n\nCurrent Economic Conditions:\n\n\n\n📈 Inflation: 4.2% Above Target (2%)\n\n\n👥 Unemployment: 6.1% Moderate\n\n\n📊 GDP Growth: 2.8% Stable\n\n\n\n\n🎯 Your Policy Decision:\n\n\n📈 Raise Rates (+0.5%)\n\n\n➡️ Maintain Rates\n\n\n📉 Lower Rates (-0.5%)\n\n\n\n\n📊 Economic Outcome:\n\n\n\n\n🔄 Try Another Scenario\n\n\n\n\n\nEconomic Trade-offs:\n\n\n\n\n\n\n\n\n\n\nDecision\nInflation Impact\nEmployment Impact\nGrowth Impact\nRisk\n\n\n\n\nRaise Rates\n⬇️ Decreases\n⬇️ May increase unemployment\n⬇️ Slows growth\nRecession risk\n\n\nMaintain\n➡️ Stable\n➡️ Stable\n➡️ Stable\nStatus quo\n\n\nLower Rates\n⬆️ May increase\n⬆️ Decreases unemployment\n⬆️ Stimulates\nBubble risk\n\n\n\nML Advantage: Can learn optimal policies across different economic scenarios and time periods."
  },
  {
    "objectID": "docs/lec2.html#future-of-ml-in-economics",
    "href": "docs/lec2.html#future-of-ml-in-economics",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Future of ML in Economics",
    "text": "Future of ML in Economics\n\n\n🚀 Emerging Trends\n🤖 Large Language Models - Economic text analysis at scale - Policy document understanding - Automated economic research\n🛰️ Alternative Data - Satellite imagery for economic activity - Social media sentiment analysis\n- Mobile phone data for migration\n⚡ Real-time Economics - Nowcasting with high-frequency data - Flash GDP estimates - Real-time inflation tracking\n\n🎯 Key Opportunities\n🌍 Development Economics - Financial inclusion prediction - Poverty mapping and targeting - Agricultural productivity optimization\n🏛️ Policy Evaluation - Causal inference at scale - Natural experiment identification - Personalized policy recommendations\n💼 Business Intelligence - Market timing and entry strategies - Customer lifetime value prediction - Supply chain resilience\n\n\n💡 Economic Research Revolution\nML is transforming economics from a data-scarce to a data-rich field, enabling new research questions and policy insights previously impossible."
  },
  {
    "objectID": "docs/lec2.html#best-practices-for-economic-ml",
    "href": "docs/lec2.html#best-practices-for-economic-ml",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Best Practices for Economic ML",
    "text": "Best Practices for Economic ML\n\n\n\n\n\n\nCritical Considerations\n\n\n\n\n\n\n\n\n🔍 Model Validation\n\nOut-of-sample testing with temporal splits\nCross-validation respecting time structure\n\nStress testing across economic cycles\nA/B testing for policy interventions\n\n📊 Interpretability\n\nSHAP values for feature importance\nLIME for local explanations\nPartial dependence plots for marginal effects\nEconomic theory validation of results\n\n\n⚖️ Ethical Considerations\n\nBias detection in algorithmic decisions\nFairness across demographic groups\nPrivacy protection in personal data\nTransparency in public policy applications\n\n🎯 Implementation\n\nStart simple before complex models\nDomain expertise integration essential\nContinuous monitoring of model performance\nRegulatory compliance considerations\n\n\n\n🏆 Success Factors\n\nEconomic theory + Machine learning expertise\nQuality data + Proper validation\n\nInterpretability + Actionable insights"
  },
  {
    "objectID": "docs/lec2.html#conclusion-next-steps",
    "href": "docs/lec2.html#conclusion-next-steps",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Conclusion & Next Steps",
    "text": "Conclusion & Next Steps\n\n🎯 Key Takeaways\n\n\n🧠 Understanding ML Types\n\nSupervised: Prediction with historical economic data\nUnsupervised: Discovery of hidden economic patterns\n\nReinforcement: Optimization of economic policies\n\n\n\n💼 Economic Applications\nML is transforming finance, policy-making, and business strategy across all sectors of the economy.\n\n\n🚀 Getting Started\n\nLearn fundamentals: Statistics, programming (Python/R)\nPractice projects: Start with public economic datasets\nDomain knowledge: Combine ML with economic theory\nStay updated: Follow latest research and applications\n\n\n\n📚 Resources\n\nVideo Source: IBM Technology - What is Machine Learning?\nAdvanced: Stanford’s CS229 Machine Learning Course\nEconomics: “Big Data and Economics” by Liran Einav and Jonathan Levin"
  },
  {
    "objectID": "docs/lec2.html#thank-you",
    "href": "docs/lec2.html#thank-you",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Thank You!",
    "text": "Thank You!\n\n🎓 Questions & Discussion\n\n\n🤝 Let’s Connect\n\nExplore economic datasets together\nDiscuss ML applications in your field\nShare implementation experiences\n\n\n\n💡 Remember\n“The best machine learning model is only as good as the economic theory and data quality behind it.”\n\n\nHappy Learning! 🚀"
  },
  {
    "objectID": "docs/lec2.html#conclusion-next-steps-.center",
    "href": "docs/lec2.html#conclusion-next-steps-.center",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Conclusion & Next Steps {.center}",
    "text": "Conclusion & Next Steps {.center}\n\n🎯 Key Takeaways\n\n\n🧠 Understanding ML Types\n\nSupervised: Prediction with historical economic data\nUnsupervised: Discovery of hidden economic patterns\n\nReinforcement: Optimization of economic policies\n\n\n\n💼 Economic Applications\nML is transforming finance, policy-making, and business strategy across all sectors of the economy.\n\n\n🚀 Getting Started\n\nLearn fundamentals: Statistics, programming (Python/R)\nPractice projects: Start with public economic datasets\nDomain knowledge: Combine ML with economic theory\nStay updated: Follow latest research and applications\n\n\n\n📚 Resources\n\nVideo Source: IBM Technology - What is Machine Learning?\nAdvanced: Stanford’s CS229 Machine Learning Course\nEconomics: “Big Data and Economics” by Liran Einav and Jonathan Levin"
  },
  {
    "objectID": "docs/lec2.html#comparing-ml-approaches-in-economics-.smaller",
    "href": "docs/lec2.html#comparing-ml-approaches-in-economics-.smaller",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Comparing ML Approaches in Economics {.smaller}",
    "text": "Comparing ML Approaches in Economics {.smaller}\n\n\n\n\n\n\n\n\n\n\n\nApproach\nData Type\nEconomic Use Case\nExample\nPros\nCons\n\n\n\n\nSupervised ✅\nLabeled historical\nPrediction & Classification\nCredit scoring, recession forecasting\nHigh accuracy, interpretable\nNeeds labeled data, may not adapt\n\n\nUnsupervised 🧩\nUnlabeled current\nPattern discovery\nMarket segmentation, anomaly detection\nNo labels needed, finds hidden patterns\nHard to validate, less precise\n\n\nReinforcement 🏆\nInteractive feedback\nDynamic optimization\nTrading algorithms, policy design\nAdapts to changes, optimal decisions\nComplex to implement, needs simulation\n\n\n\n\n🎯 Economic Insight\n\nSupervised: Best for prediction problems with historical precedent\nUnsupervised: Best for exploration of new economic phenomena\n\nReinforcement: Best for optimization in dynamic economic environments\n\n\n\nIn practice, economists often combine these approaches - using unsupervised learning for exploration, supervised learning for prediction, and reinforcement learning for policy optimization."
  },
  {
    "objectID": "docs/ch02.html#outline",
    "href": "docs/ch02.html#outline",
    "title": "What is Statistical Learning?",
    "section": "Outline",
    "text": "Outline\n\nWhat is Statistical Learning?\nWhy estimate \\(f\\)?\nHow do we estimate \\(f\\)?\nThe trade-off: prediction accuracy vs. model interpretability\nSupervised vs. unsupervised learning\nRegression vs. classification"
  },
  {
    "objectID": "docs/ch02.html#what-is-statistical-learning",
    "href": "docs/ch02.html#what-is-statistical-learning",
    "title": "What is Statistical Learning?",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\nThink of statistical learning like learning to predict based on patterns in data.\nSimple Example: Predicting house prices based on size, location, age.\nData: \\((X_1, X_2, \\dots, X_p, Y)\\) for \\(i = 1, \\dots, n\\). - \\(X_j\\) are predictors (size, location, age of house) - \\(Y\\) is the response (house price)\nKey Assumption: House price depends on at least one feature (size matters!)\nModel: \\(Y = f(X) + \\varepsilon\\) - \\(f\\) = the “true relationship” we want to learn - \\(\\varepsilon\\) = random noise (things we can’t predict)\n\nStatistical learning finds the best \\(f\\) using data"
  },
  {
    "objectID": "docs/ch02.html#example-wage-data",
    "href": "docs/ch02.html#example-wage-data",
    "title": "What is Statistical Learning?",
    "section": "Example: Wage Data",
    "text": "Example: Wage Data\n\nR CodePython CodeInterpretation\n\n\n\n\nCode\nlibrary(ISLR2)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata(Wage)\nWage %&gt;%\n  ggplot(aes(x = age, y = wage)) +\n  geom_point(alpha = 0.4, color = \"steelblue\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\") +\n  labs(title = \"Wage vs. Age\", x = \"Age\", y = \"Wage ($1000s)\") +\n  theme_minimal()\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import fetch_openml\n\n# Load wage data (or similar dataset)\nwage_data = pd.read_csv('wage_data.csv')\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=wage_data, x='age', y='wage', alpha=0.4)\nsns.loess(data=wage_data, x='age', y='wage', color='red')\nplt.title('Wage vs. Age')\nplt.xlabel('Age')\nplt.ylabel('Wage ($1000s)')\nplt.show()\n\n\nWhat we see: - Wages increase with age until around 40-50 - Then wages level off or slightly decrease - Lots of variation at each age (the “noise”)\nBusiness insight: Companies should expect to pay higher wages to experienced workers, but the premium levels off"
  },
  {
    "objectID": "docs/ch02.html#different-standard-deviations",
    "href": "docs/ch02.html#different-standard-deviations",
    "title": "What is Statistical Learning?",
    "section": "Different Standard Deviations",
    "text": "Different Standard Deviations\n\nThe difficulty of estimating \\(f\\) depends on the noise level."
  },
  {
    "objectID": "docs/ch02.html#different-estimates-for-f",
    "href": "docs/ch02.html#different-estimates-for-f",
    "title": "What is Statistical Learning?",
    "section": "Different Estimates for \\(f\\)",
    "text": "Different Estimates for \\(f\\)"
  },
  {
    "objectID": "docs/ch02.html#example-housing-prices-boston",
    "href": "docs/ch02.html#example-housing-prices-boston",
    "title": "What is Statistical Learning?",
    "section": "Example: Housing Prices (Boston)",
    "text": "Example: Housing Prices (Boston)\n\n\nCode\nlibrary(MASS)\ndata(Boston)\n\n# Fit linear model using modern approach\nlm_fit &lt;- Boston %&gt;%\n  lm(medv ~ lstat + rm, data = .)\n\nsummary(lm_fit)\n\n\n\nCall:\nlm(formula = medv ~ lstat + rm, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.076  -3.516  -1.010   1.909  28.131 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.35827    3.17283  -0.428    0.669    \nlstat       -0.64236    0.04373 -14.689   &lt;2e-16 ***\nrm           5.09479    0.44447  11.463   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.54 on 503 degrees of freedom\nMultiple R-squared:  0.6386,    Adjusted R-squared:  0.6371 \nF-statistic: 444.3 on 2 and 503 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "docs/ch02.html#housing-prices-visualization",
    "href": "docs/ch02.html#housing-prices-visualization",
    "title": "What is Statistical Learning?",
    "section": "Housing Prices: Visualization",
    "text": "Housing Prices: Visualization\n\nPredictors: % lower status (lstat), average rooms (rm).\nResponse: median home value (medv)."
  },
  {
    "objectID": "docs/ch02.html#supervised-vs.-unsupervised-example",
    "href": "docs/ch02.html#supervised-vs.-unsupervised-example",
    "title": "What is Statistical Learning?",
    "section": "Supervised vs. Unsupervised Example",
    "text": "Supervised vs. Unsupervised Example"
  },
  {
    "objectID": "docs/ch02.html#regression-vs.-classification-example",
    "href": "docs/ch02.html#regression-vs.-classification-example",
    "title": "What is Statistical Learning?",
    "section": "Regression vs. Classification Example",
    "text": "Regression vs. Classification Example\n\n\nCode\n# Create binary outcome using dplyr\nWage &lt;- Wage %&gt;%\n  mutate(highwage = ifelse(wage &gt; 250, 1, 0))\n\n# Fit logistic regression\nglm_fit &lt;- Wage %&gt;%\n  glm(highwage ~ age + education, data = ., family = binomial)\n\nsummary(glm_fit)\n\n\n\nCall:\nglm(formula = highwage ~ age + education, family = binomial, \n    data = .)\n\nCoefficients:\n                             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)                 -20.73275  652.23660  -0.032   0.9746  \nage                           0.02686    0.01078   2.491   0.0128 *\neducation2. HS Grad          14.28330  652.23657   0.022   0.9825  \neducation3. Some College     15.06687  652.23652   0.023   0.9816  \neducation4. College Grad     16.13787  652.23645   0.025   0.9803  \neducation5. Advanced Degree  17.35777  652.23643   0.027   0.9788  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 730.53  on 2999  degrees of freedom\nResidual deviance: 615.78  on 2994  degrees of freedom\nAIC: 627.78\n\nNumber of Fisher Scoring iterations: 18"
  },
  {
    "objectID": "docs/ch02.html#classification-predicted-probabilities",
    "href": "docs/ch02.html#classification-predicted-probabilities",
    "title": "What is Statistical Learning?",
    "section": "Classification: Predicted Probabilities",
    "text": "Classification: Predicted Probabilities"
  },
  {
    "objectID": "docs/ch02.html#end",
    "href": "docs/ch02.html#end",
    "title": "What is Statistical Learning?",
    "section": "End",
    "text": "End\nRemember: Machine learning is a tool, not magic. Success comes from:\n\nUnderstanding your business problem\nChoosing the right approach\nValidating your results\nCommunicating findings clearly\n\nQuestions? 🤔"
  },
  {
    "objectID": "docs/ch02.html#why-do-we-want-to-estimate-f",
    "href": "docs/ch02.html#why-do-we-want-to-estimate-f",
    "title": "What is Statistical Learning?",
    "section": "Why Do We Want to Estimate \\(f\\)?",
    "text": "Why Do We Want to Estimate \\(f\\)?\nTwo main reasons:\n\nPrediction 📈\n\n“What will this house sell for?”\n“Will this customer buy our product?”\nWe don’t care HOW the prediction works, just that it’s accurate\n\nUnderstanding 🔍\n\n“How does education affect wages?”\n“Which marketing channels work best?”\nWe want to understand the relationship itself\n\n\nReal-world example: A bank wants to predict loan defaults (prediction) AND understand what factors cause defaults (understanding)"
  },
  {
    "objectID": "docs/ch02.html#how-noise-affects-learning",
    "href": "docs/ch02.html#how-noise-affects-learning",
    "title": "What is Statistical Learning?",
    "section": "How Noise Affects Learning",
    "text": "How Noise Affects Learning\nKey Insight: More noise = harder to learn the pattern\nThink of it like listening to music with background static - more static makes it harder to hear the song.\n\nR CodePython CodeBusiness Application\n\n\n\n\nCode\nset.seed(123)\nx &lt;- seq(-2, 2, length.out = 100)\ny_true &lt;- sin(pi * x)  # True relationship\ny_low &lt;- y_true + rnorm(100, 0, 0.2)   # Low noise\ny_high &lt;- y_true + rnorm(100, 0, 1)    # High noise\n\n# Create comparison plots\nlibrary(patchwork)\np1 &lt;- ggplot(data.frame(x=x, y=y_low), aes(x, y)) + \n      geom_point(color=\"steelblue\") + \n      geom_line(aes(y=y_true), color=\"red\") +\n      ggtitle(\"Low Noise - Easy to Learn\")\n\np2 &lt;- ggplot(data.frame(x=x, y=y_high), aes(x, y)) + \n      geom_point(color=\"steelblue\") + \n      geom_line(aes(y=y_true), color=\"red\") +\n      ggtitle(\"High Noise - Hard to Learn\")\n\np1 + p2\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(123)\nx = np.linspace(-2, 2, 100)\ny_true = np.sin(np.pi * x)\ny_low = y_true + np.random.normal(0, 0.2, 100)\ny_high = y_true + np.random.normal(0, 1, 100)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nax1.scatter(x, y_low, alpha=0.7, color='steelblue')\nax1.plot(x, y_true, color='red', linewidth=2)\nax1.set_title('Low Noise - Easy to Learn')\n\nax2.scatter(x, y_high, alpha=0.7, color='steelblue')  \nax2.plot(x, y_true, color='red', linewidth=2)\nax2.set_title('High Noise - Hard to Learn')\n\nplt.tight_layout()\nplt.show()\n\n\nReal Example: Predicting sales\n\nLow noise: Luxury cars (few, predictable buyers)\nHigh noise: Fast food (many random factors)\n\nStrategy: With high noise, collect MORE data or use simpler models"
  },
  {
    "objectID": "docs/ch02.html#different-ways-to-estimate-f",
    "href": "docs/ch02.html#different-ways-to-estimate-f",
    "title": "What is Statistical Learning?",
    "section": "Different Ways to Estimate \\(f\\)",
    "text": "Different Ways to Estimate \\(f\\)\nQuestion: Given data points, how do we draw the “best” line?\nAnswer: Different methods give different results!\n\nR CodePython CodeKey Tradeoff\n\n\n\n\nCode\n# Method 1: Linear regression (straight line)\nlinear_fit &lt;- lm(y ~ x, data = df)\n\n# Method 2: Polynomial (curved line) \npoly_fit &lt;- lm(y ~ poly(x, 3), data = df)\n\n# Method 3: LOESS (flexible curve)\ndf %&gt;%\n  ggplot(aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"red\") +      # Linear\n  geom_smooth(method = \"loess\", color = \"blue\") +  # Flexible\n  labs(title = \"Different Methods, Different Results\")\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\n\n# Linear regression\nlinear = LinearRegression()\nlinear.fit(x.reshape(-1,1), y)\n\n# Polynomial regression  \npoly = Pipeline([\n    ('poly', PolynomialFeatures(degree=3)),\n    ('linear', LinearRegression())\n])\npoly.fit(x.reshape(-1,1), y)\n\n# Plot results\nplt.scatter(x, y)\nplt.plot(x, linear.predict(x.reshape(-1,1)), 'r-', label='Linear')\nplt.plot(x, poly.predict(x.reshape(-1,1)), 'b-', label='Polynomial')\nplt.legend()\nplt.show()\n\n\nSimple models (straight lines): - ✅ Easy to understand and explain - ❌ Might miss important patterns\nComplex models (wiggly curves): - ✅ Capture complex patterns\n- ❌ Hard to interpret, might overfit\nRule of thumb: Start simple, add complexity only if needed"
  },
  {
    "objectID": "docs/ch02.html#real-example-boston-housing-prices",
    "href": "docs/ch02.html#real-example-boston-housing-prices",
    "title": "What is Statistical Learning?",
    "section": "Real Example: Boston Housing Prices",
    "text": "Real Example: Boston Housing Prices\nBusiness Question: How much should we price this house?\n\nR CodePython CodeBusiness Insights\n\n\n\n\nCode\nlibrary(MASS)\ndata(Boston)\n\n# Simple model: Price depends on poverty level and room count\nlm_fit &lt;- Boston %&gt;%\n  lm(medv ~ lstat + rm, data = .)\n\n# Look at results\nsummary(lm_fit)\n\n# Interpretation:\n# - Each extra room adds ~$9,000 to house value\n# - 1% increase in poverty rate reduces value by ~$950\n\n\n\n\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_boston\n\n# Load Boston housing data\nboston = load_boston()\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf['price'] = boston.target\n\n# Fit model: Price = f(poverty_rate, avg_rooms)\nmodel = LinearRegression()\nX = df[['LSTAT', 'RM']]  # poverty %, avg rooms\ny = df['price']\n\nmodel.fit(X, y)\n\n# Print coefficients\nprint(f\"Room effect: ${model.coef_[1]:.2f}k per room\")\nprint(f\"Poverty effect: -${abs(model.coef_[0]):.2f}k per 1% increase\")\n\n\nActionable findings:\n\nRoom count matters: Each extra room = ~$9K more value\n\nStrategy: Highlight room count in listings\n\nLocation matters: High-poverty areas have lower prices\n\nStrategy: Different pricing strategies by neighborhood\n\nModel fit: R² ≈ 0.67 means we explain 67% of price variation\n\nStill missing some important factors (schools, transportation?)\n\n\n\n\n\n\n\n\nWhat Affects Boston Housing Prices?\n\n\n\n\n\n\n\n\n\nFactor\nEffect ($1000s)\nStd Error\nt-stat\np-value\n\n\n\n\nBaseline Price\n-1.36\n3.1728278\n-0.4280953\n0.669\n\n\nPoverty Rate Effect (%)\n-0.64\n0.0437315\n-14.6886992\n&lt; 0.001\n\n\nAdditional Room Effect\n5.09\n0.4444655\n11.4627299\n&lt; 0.001"
  },
  {
    "objectID": "docs/ch02.html#visualizing-housing-price-relationships",
    "href": "docs/ch02.html#visualizing-housing-price-relationships",
    "title": "What is Statistical Learning?",
    "section": "Visualizing Housing Price Relationships",
    "text": "Visualizing Housing Price Relationships\nKey insight: Relationships aren’t always linear!\n\nBusiness Implication: The poverty-price relationship is clearly non-linear! Linear models miss this pattern."
  },
  {
    "objectID": "docs/ch02.html#supervised-vs.-unsupervised-learning",
    "href": "docs/ch02.html#supervised-vs.-unsupervised-learning",
    "title": "What is Statistical Learning?",
    "section": "Supervised vs. Unsupervised Learning",
    "text": "Supervised vs. Unsupervised Learning\n\n\nSupervised 👨‍🏫\nYou have the “answer key”\nExamples: - Predict house prices (have past sales) - Classify emails as spam (have labeled examples) - Forecast sales (have historical data)\nGoal: Learn from examples to predict new cases\n\nUnsupervised 🔍\nNo “answer key” - find hidden patterns\nExamples: - Group customers by behavior - Find market segments - Detect unusual transactions\nGoal: Discover structure in data\n\n\nR CodePython CodeBusiness Value\n\n\n\n\nCode\n# Unsupervised: Find customer segments\nset.seed(2)\ncustomer_data &lt;- tibble(\n  spending = c(rnorm(50, 30, 10), rnorm(50, 80, 15)),\n  frequency = c(rnorm(50, 2, 1), rnorm(50, 8, 2))\n)\n\n# K-means clustering (finds groups automatically)\nsegments &lt;- kmeans(customer_data, centers = 2)\ncustomer_data$segment &lt;- segments$cluster\n\n# Plot results  \nggplot(customer_data, aes(spending, frequency, color = factor(segment))) +\n  geom_point(size = 3) +\n  labs(title = \"Customer Segments Found Automatically\")\n\n\n\n\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Generate customer data\nspending = np.concatenate([np.random.normal(30, 10, 50), \n                          np.random.normal(80, 15, 50)])\nfrequency = np.concatenate([np.random.normal(2, 1, 50),\n                           np.random.normal(8, 2, 50)])\n\n# Find segments\nkmeans = KMeans(n_clusters=2)\nsegments = kmeans.fit_predict(np.column_stack([spending, frequency]))\n\n# Plot\nplt.scatter(spending, frequency, c=segments, cmap='viridis')\nplt.xlabel('Average Spending')\nplt.ylabel('Purchase Frequency') \nplt.title('Customer Segments Found Automatically')\nplt.show()\n\n\nUnsupervised learning reveals:\n\nCustomer segments → Targeted marketing\nFraud patterns → Risk management\n\nMarket structure → Competitive strategy\n\nKey insight: Sometimes the most valuable discoveries come from data exploration, not prediction!"
  },
  {
    "objectID": "docs/ch02.html#regression-vs.-classification",
    "href": "docs/ch02.html#regression-vs.-classification",
    "title": "What is Statistical Learning?",
    "section": "Regression vs. Classification",
    "text": "Regression vs. Classification\nThe key difference: What type of outcome are you predicting?\n\n\nRegression 📈\nPredicting numbers - House prices: $150K, $200K, $175K - Sales volume: 1,000 units, 1,500 units\n- Temperature: 25°C, 30°C, 22°C - Stock returns: +5%, -2%, +8%\n\nClassification 🏷️\nPredicting categories - Email: Spam or Not Spam - Loan decision: Approve or Reject\n- Customer: High/Medium/Low Value - Medical diagnosis: Disease A, B, or Healthy\n\n\nR CodePython CodeBusiness Applications\n\n\n\n\nCode\n# Classification example: High wage prediction\nWage &lt;- Wage %&gt;%\n  mutate(high_wage = ifelse(wage &gt; 250, \"High\", \"Low\"))\n\n# Logistic regression for classification\nglm_fit &lt;- glm(high_wage == \"High\" ~ age + education, \n               data = Wage, family = binomial)\n\n# Get probabilities instead of just yes/no\npred_prob &lt;- predict(glm_fit, type = \"response\")\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create binary outcome\nwage_data['high_wage'] = (wage_data['wage'] &gt; 250).astype(int)\n\n# Prepare data\nle = LabelEncoder() \nX = pd.DataFrame({\n    'age': wage_data['age'],\n    'education': le.fit_transform(wage_data['education'])\n})\ny = wage_data['high_wage']\n\n# Fit classification model\nclf = LogisticRegression()\nclf.fit(X, y)\n\n# Get probabilities\nprobabilities = clf.predict_proba(X)[:, 1]  # Prob of high wage\n\n\nClassification is everywhere:\n\nBanking: Loan approval (binary)\nMarketing: Customer value (low/medium/high)\n\nHR: Resume screening (hire/reject)\nE-commerce: Product categories\n\nKey insight: Classification gives you actionable categories, regression gives you precise numbers"
  },
  {
    "objectID": "docs/ch02.html#the-big-picture-prediction-vs.-interpretation-trade-off",
    "href": "docs/ch02.html#the-big-picture-prediction-vs.-interpretation-trade-off",
    "title": "What is Statistical Learning?",
    "section": "The Big Picture: Prediction vs. Interpretation Trade-off",
    "text": "The Big Picture: Prediction vs. Interpretation Trade-off\nCentral tension in ML: Accurate models are often hard to explain\n\n\nHigh Interpretability 🔍\nSimple models you can explain - Linear regression - Decision trees (small ones) - Simple rules\nWhen to use: - Regulatory requirements - Medical decisions\n- Policy recommendations - Building trust\n\nHigh Prediction Accuracy 📈\nComplex “black box” models - Deep neural networks - Random forests (large) - Ensemble methods\nWhen to use: - Image recognition - Recommendation systems - High-stakes prediction - When accuracy is paramount\n\nReal-world examples: - Medical diagnosis: Need interpretable models (doctor must explain) - Netflix recommendations: Black box is fine (just works) - Credit scoring: Regulated industry needs interpretable models - Fraud detection: Accuracy matters more than explanation"
  },
  {
    "objectID": "docs/ch02.html#key-takeaways-for-economics-business-students",
    "href": "docs/ch02.html#key-takeaways-for-economics-business-students",
    "title": "What is Statistical Learning?",
    "section": "Key Takeaways for Economics & Business Students",
    "text": "Key Takeaways for Economics & Business Students\n\nStart with the business question: Do you need prediction or understanding?\nSimple models first: Linear regression beats fancy algorithms if you need interpretation\nData quality matters more than algorithm choice: Clean data + simple model &gt; messy data + complex model\nContext is king: A 90% accurate model is useless if you can’t act on it\nValidate everything: Your model is only as good as its performance on NEW data\n\nNext steps: Learn to evaluate model performance and avoid common pitfalls"
  },
  {
    "objectID": "docs/ch02.html#whats-coming-next",
    "href": "docs/ch02.html#whats-coming-next",
    "title": "What is Statistical Learning?",
    "section": "What’s Coming Next",
    "text": "What’s Coming Next\nIn future lectures, we’ll cover:\n\nModel Assessment: How do we know if our model is good?\nBias-Variance Trade-off: Why simple models sometimes win\nCross-validation: Testing models the right way\n\nOverfitting: When models memorize instead of learn\nResampling Methods: Making the most of limited data\n\nHomework preview: Apply these concepts to a real dataset from your country/region"
  },
  {
    "objectID": "docs/lec02.html#outline",
    "href": "docs/lec02.html#outline",
    "title": "Machine Learning?",
    "section": "Outline",
    "text": "Outline\n\nWhat is Machine Learning?\nWhy estimate \\(f\\)?\nHow do we estimate \\(f\\)?\nThe trade-off: prediction accuracy vs. model interpretability\nSupervised vs. unsupervised learning\nRegression vs. classification"
  },
  {
    "objectID": "docs/lec02.html#what-is-statistical-learning",
    "href": "docs/lec02.html#what-is-statistical-learning",
    "title": "Machine Learning?",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\nThink of statistical learning like learning to predict based on patterns in data.\nSimple Example: Predicting house prices based on size, location, age.\nData: \\((X_1, X_2, \\dots, X_p, Y)\\) for \\(i = 1, \\dots, n\\). - \\(X_j\\) are predictors (size, location, age of house) - \\(Y\\) is the response (house price)\nKey Assumption: House price depends on at least one feature (size matters!)\nModel: \\(Y = f(X) + \\varepsilon\\)\n\n\\(f\\) = the “true relationship” we want to learn\n\\(\\varepsilon\\) = random noise (things we can’t predict)\n\n\nMachine learning finds the best \\(f\\) using data"
  },
  {
    "objectID": "docs/lec02.html#why-do-we-want-to-estimate-f",
    "href": "docs/lec02.html#why-do-we-want-to-estimate-f",
    "title": "Machine Learning?",
    "section": "Why Do We Want to Estimate \\(f\\)?",
    "text": "Why Do We Want to Estimate \\(f\\)?\nTwo main reasons:\n\nPrediction 📈\n\n“What will this house sell for?”\n“Will this customer buy our product?”\nWe don’t care HOW the prediction works, just that it’s accurate\n\nUnderstanding 🔍\n\n“How does education affect wages?”\n“Which marketing channels work best?”\nWe want to understand the relationship itself\n\n\nReal-world example: A bank wants to predict loan defaults (prediction) AND understand what factors cause defaults (understanding)"
  },
  {
    "objectID": "docs/lec02.html#example-wage-data",
    "href": "docs/lec02.html#example-wage-data",
    "title": "Machine Learning?",
    "section": "Example: Wage Data",
    "text": "Example: Wage Data\n\nR CodePython CodeInterpretation\n\n\n\n\nCode\nlibrary(ISLR2)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata(Wage)\nWage %&gt;%\n  ggplot(aes(x = age, y = wage)) +\n  geom_point(alpha = 0.4, color = \"steelblue\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\") +\n  labs(title = \"Wage vs. Age\", x = \"Age\", y = \"Wage ($1000s)\") +\n  theme_minimal()\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import fetch_openml\n\n# Load wage data (or similar dataset)\nwage_data = pd.read_csv('wage_data.csv')\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=wage_data, x='age', y='wage', alpha=0.4)\nsns.loess(data=wage_data, x='age', y='wage', color='red')\nplt.title('Wage vs. Age')\nplt.xlabel('Age')\nplt.ylabel('Wage ($1000s)')\nplt.show()\n\n\nWhat we see: - Wages increase with age until around 40-50 - Then wages level off or slightly decrease - Lots of variation at each age (the “noise”)\nBusiness insight: Companies should expect to pay higher wages to experienced workers, but the premium levels off"
  },
  {
    "objectID": "docs/lec02.html#how-noise-affects-learning",
    "href": "docs/lec02.html#how-noise-affects-learning",
    "title": "Machine Learning?",
    "section": "How Noise Affects Learning",
    "text": "How Noise Affects Learning\nKey Insight: More noise = harder to learn the pattern\nThink of it like listening to music with background static - more static makes it harder to hear the song.\n\nR CodePython CodeBusiness Application\n\n\n\n\nCode\nset.seed(123)\nx &lt;- seq(-2, 2, length.out = 100)\ny_true &lt;- sin(pi * x)  # True relationship\ny_low &lt;- y_true + rnorm(100, 0, 0.2)   # Low noise\ny_high &lt;- y_true + rnorm(100, 0, 1)    # High noise\n\n# Create comparison plots\nlibrary(patchwork)\np1 &lt;- ggplot(data.frame(x=x, y=y_low), aes(x, y)) + \n      geom_point(color=\"steelblue\") + \n      geom_line(aes(y=y_true), color=\"red\") +\n      ggtitle(\"Low Noise - Easy to Learn\")\n\np2 &lt;- ggplot(data.frame(x=x, y=y_high), aes(x, y)) + \n      geom_point(color=\"steelblue\") + \n      geom_line(aes(y=y_true), color=\"red\") +\n      ggtitle(\"High Noise - Hard to Learn\")\n\np1 + p2\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(123)\nx = np.linspace(-2, 2, 100)\ny_true = np.sin(np.pi * x)\ny_low = y_true + np.random.normal(0, 0.2, 100)\ny_high = y_true + np.random.normal(0, 1, 100)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nax1.scatter(x, y_low, alpha=0.7, color='steelblue')\nax1.plot(x, y_true, color='red', linewidth=2)\nax1.set_title('Low Noise - Easy to Learn')\n\nax2.scatter(x, y_high, alpha=0.7, color='steelblue')  \nax2.plot(x, y_true, color='red', linewidth=2)\nax2.set_title('High Noise - Hard to Learn')\n\nplt.tight_layout()\nplt.show()\n\n\nReal Example: Predicting sales\n\nLow noise: Luxury cars (few, predictable buyers)\nHigh noise: Fast food (many random factors)\n\nStrategy: With high noise, collect MORE data or use simpler models"
  },
  {
    "objectID": "docs/lec02.html#different-ways-to-estimate-f",
    "href": "docs/lec02.html#different-ways-to-estimate-f",
    "title": "Machine Learning?",
    "section": "Different Ways to Estimate \\(f\\)",
    "text": "Different Ways to Estimate \\(f\\)\nQuestion: Given data points, how do we draw the “best” line?\nAnswer: Different methods give different results!\n\nR CodePython CodeKey Tradeoff\n\n\n\n\nCode\n# Method 1: Linear regression (straight line)\nlinear_fit &lt;- lm(y ~ x, data = df)\n\n# Method 2: Polynomial (curved line) \npoly_fit &lt;- lm(y ~ poly(x, 3), data = df)\n\n# Method 3: LOESS (flexible curve)\ndf %&gt;%\n  ggplot(aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"red\") +      # Linear\n  geom_smooth(method = \"loess\", color = \"blue\") +  # Flexible\n  labs(title = \"Different Methods, Different Results\")\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\n\n# Linear regression\nlinear = LinearRegression()\nlinear.fit(x.reshape(-1,1), y)\n\n# Polynomial regression  \npoly = Pipeline([\n    ('poly', PolynomialFeatures(degree=3)),\n    ('linear', LinearRegression())\n])\npoly.fit(x.reshape(-1,1), y)\n\n# Plot results\nplt.scatter(x, y)\nplt.plot(x, linear.predict(x.reshape(-1,1)), 'r-', label='Linear')\nplt.plot(x, poly.predict(x.reshape(-1,1)), 'b-', label='Polynomial')\nplt.legend()\nplt.show()\n\n\nSimple models (straight lines): - ✅ Easy to understand and explain - ❌ Might miss important patterns\nComplex models (wiggly curves): - ✅ Capture complex patterns\n- ❌ Hard to interpret, might overfit\nRule of thumb: Start simple, add complexity only if needed"
  },
  {
    "objectID": "docs/lec02.html#real-example-boston-housing-prices",
    "href": "docs/lec02.html#real-example-boston-housing-prices",
    "title": "Machine Learning?",
    "section": "Real Example: Boston Housing Prices",
    "text": "Real Example: Boston Housing Prices\nBusiness Question: How much should we price this house?\n\nR CodePython CodeBusiness Insights\n\n\n\n\nCode\nlibrary(MASS)\ndata(Boston)\n\n# Simple model: Price depends on poverty level and room count\nlm_fit &lt;- Boston %&gt;%\n  lm(medv ~ lstat + rm, data = .)\n\n# Look at results\nsummary(lm_fit)\n\n# Interpretation:\n# - Each extra room adds ~$9,000 to house value\n# - 1% increase in poverty rate reduces value by ~$950\n\n\n\n\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_boston\n\n# Load Boston housing data\nboston = load_boston()\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf['price'] = boston.target\n\n# Fit model: Price = f(poverty_rate, avg_rooms)\nmodel = LinearRegression()\nX = df[['LSTAT', 'RM']]  # poverty %, avg rooms\ny = df['price']\n\nmodel.fit(X, y)\n\n# Print coefficients\nprint(f\"Room effect: ${model.coef_[1]:.2f}k per room\")\nprint(f\"Poverty effect: -${abs(model.coef_[0]):.2f}k per 1% increase\")\n\n\nActionable findings:\n\nRoom count matters: Each extra room = ~$9K more value\n\nStrategy: Highlight room count in listings\n\nLocation matters: High-poverty areas have lower prices\n\nStrategy: Different pricing strategies by neighborhood\n\nModel fit: R² ≈ 0.67 means we explain 67% of price variation\n\nStill missing some important factors (schools, transportation?)\n\n\n\n\n\n\n\n\nWhat Affects Boston Housing Prices?\n\n\n\n\n\n\n\n\n\nFactor\nEffect ($1000s)\nStd Error\nt-stat\np-value\n\n\n\n\nBaseline Price\n-1.36\n3.1728278\n-0.4280953\n0.669\n\n\nPoverty Rate Effect (%)\n-0.64\n0.0437315\n-14.6886992\n&lt; 0.001\n\n\nAdditional Room Effect\n5.09\n0.4444655\n11.4627299\n&lt; 0.001"
  },
  {
    "objectID": "docs/lec02.html#visualizing-housing-price-relationships",
    "href": "docs/lec02.html#visualizing-housing-price-relationships",
    "title": "Machine Learning?",
    "section": "Visualizing Housing Price Relationships",
    "text": "Visualizing Housing Price Relationships\nKey insight: Relationships aren’t always linear!\n\nBusiness Implication: The poverty-price relationship is clearly non-linear! Linear models miss this pattern."
  },
  {
    "objectID": "docs/lec02.html#supervised-vs.-unsupervised-learning",
    "href": "docs/lec02.html#supervised-vs.-unsupervised-learning",
    "title": "Machine Learning?",
    "section": "Supervised vs. Unsupervised Learning",
    "text": "Supervised vs. Unsupervised Learning\n\n\nSupervised 👨‍🏫\nYou have the “answer key”\nExamples: - Predict house prices (have past sales) - Classify emails as spam (have labeled examples) - Forecast sales (have historical data)\nGoal: Learn from examples to predict new cases\n\nUnsupervised 🔍\nNo “answer key” - find hidden patterns\nExamples: - Group customers by behavior - Find market segments - Detect unusual transactions\nGoal: Discover structure in data\n\n\nR CodePython CodeBusiness Value\n\n\n\n\nCode\n# Unsupervised: Find customer segments\nset.seed(2)\ncustomer_data &lt;- tibble(\n  spending = c(rnorm(50, 30, 10), rnorm(50, 80, 15)),\n  frequency = c(rnorm(50, 2, 1), rnorm(50, 8, 2))\n)\n\n# K-means clustering (finds groups automatically)\nsegments &lt;- kmeans(customer_data, centers = 2)\ncustomer_data$segment &lt;- segments$cluster\n\n# Plot results  \nggplot(customer_data, aes(spending, frequency, color = factor(segment))) +\n  geom_point(size = 3) +\n  labs(title = \"Customer Segments Found Automatically\")\n\n\n\n\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Generate customer data\nspending = np.concatenate([np.random.normal(30, 10, 50), \n                          np.random.normal(80, 15, 50)])\nfrequency = np.concatenate([np.random.normal(2, 1, 50),\n                           np.random.normal(8, 2, 50)])\n\n# Find segments\nkmeans = KMeans(n_clusters=2)\nsegments = kmeans.fit_predict(np.column_stack([spending, frequency]))\n\n# Plot\nplt.scatter(spending, frequency, c=segments, cmap='viridis')\nplt.xlabel('Average Spending')\nplt.ylabel('Purchase Frequency') \nplt.title('Customer Segments Found Automatically')\nplt.show()\n\n\nUnsupervised learning reveals:\n\nCustomer segments → Targeted marketing\nFraud patterns → Risk management\n\nMarket structure → Competitive strategy\n\nKey insight: Sometimes the most valuable discoveries come from data exploration, not prediction!"
  },
  {
    "objectID": "docs/lec02.html#regression-vs.-classification",
    "href": "docs/lec02.html#regression-vs.-classification",
    "title": "Machine Learning?",
    "section": "Regression vs. Classification",
    "text": "Regression vs. Classification\nThe key difference: What type of outcome are you predicting?\n\n\nRegression 📈\nPredicting numbers - House prices: $150K, $200K, $175K - Sales volume: 1,000 units, 1,500 units\n- Temperature: 25°C, 30°C, 22°C - Stock returns: +5%, -2%, +8%\n\nClassification 🏷️\nPredicting categories - Email: Spam or Not Spam - Loan decision: Approve or Reject\n- Customer: High/Medium/Low Value - Medical diagnosis: Disease A, B, or Healthy\n\n\nR CodePython CodeBusiness Applications\n\n\n\n\nCode\n# Classification example: High wage prediction\nWage &lt;- Wage %&gt;%\n  mutate(high_wage = ifelse(wage &gt; 250, \"High\", \"Low\"))\n\n# Logistic regression for classification\nglm_fit &lt;- glm(high_wage == \"High\" ~ age + education, \n               data = Wage, family = binomial)\n\n# Get probabilities instead of just yes/no\npred_prob &lt;- predict(glm_fit, type = \"response\")\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create binary outcome\nwage_data['high_wage'] = (wage_data['wage'] &gt; 250).astype(int)\n\n# Prepare data\nle = LabelEncoder() \nX = pd.DataFrame({\n    'age': wage_data['age'],\n    'education': le.fit_transform(wage_data['education'])\n})\ny = wage_data['high_wage']\n\n# Fit classification model\nclf = LogisticRegression()\nclf.fit(X, y)\n\n# Get probabilities\nprobabilities = clf.predict_proba(X)[:, 1]  # Prob of high wage\n\n\nClassification is everywhere:\n\nBanking: Loan approval (binary)\nMarketing: Customer value (low/medium/high)\n\nHR: Resume screening (hire/reject)\nE-commerce: Product categories\n\nKey insight: Classification gives you actionable categories, regression gives you precise numbers"
  },
  {
    "objectID": "docs/lec02.html#the-big-picture-prediction-vs.-interpretation-trade-off",
    "href": "docs/lec02.html#the-big-picture-prediction-vs.-interpretation-trade-off",
    "title": "Machine Learning?",
    "section": "The Big Picture: Prediction vs. Interpretation Trade-off",
    "text": "The Big Picture: Prediction vs. Interpretation Trade-off\nCentral tension in ML: Accurate models are often hard to explain\n\n\nHigh Interpretability 🔍\nSimple models you can explain - Linear regression - Decision trees (small ones) - Simple rules\nWhen to use: - Regulatory requirements - Medical decisions\n- Policy recommendations - Building trust\n\nHigh Prediction Accuracy 📈\nComplex “black box” models - Deep neural networks - Random forests (large) - Ensemble methods\nWhen to use: - Image recognition - Recommendation systems - High-stakes prediction - When accuracy is paramount\n\nReal-world examples: - Medical diagnosis: Need interpretable models (doctor must explain) - Netflix recommendations: Black box is fine (just works) - Credit scoring: Regulated industry needs interpretable models - Fraud detection: Accuracy matters more than explanation"
  },
  {
    "objectID": "docs/lec02.html#key-takeaways-for-economics-business-students",
    "href": "docs/lec02.html#key-takeaways-for-economics-business-students",
    "title": "Machine Learning?",
    "section": "Key Takeaways for Economics & Business Students",
    "text": "Key Takeaways for Economics & Business Students\n\nStart with the business question: Do you need prediction or understanding?\nSimple models first: Linear regression beats fancy algorithms if you need interpretation\nData quality matters more than algorithm choice: Clean data + simple model &gt; messy data + complex model\nContext is king: A 90% accurate model is useless if you can’t act on it\nValidate everything: Your model is only as good as its performance on NEW data\n\nNext steps: Learn to evaluate model performance and avoid common pitfalls"
  },
  {
    "objectID": "docs/lec02.html#whats-coming-next",
    "href": "docs/lec02.html#whats-coming-next",
    "title": "Machine Learning?",
    "section": "What’s Coming Next",
    "text": "What’s Coming Next\nIn future lectures, we’ll cover:\n\nModel Assessment: How do we know if our model is good?\nBias-Variance Trade-off: Why simple models sometimes win\nCross-validation: Testing models the right way\n\nOverfitting: When models memorize instead of learn\nResampling Methods: Making the most of limited data\n\nHomework preview: Apply these concepts to a real dataset from your country/region"
  },
  {
    "objectID": "docs/lec02.html#end",
    "href": "docs/lec02.html#end",
    "title": "Machine Learning?",
    "section": "End",
    "text": "End\nRemember: Machine learning is a tool, not magic. Success comes from:\n\nUnderstanding your business problem\nChoosing the right approach\nValidating your results\nCommunicating findings clearly\n\nQuestions? 🤔"
  },
  {
    "objectID": "docs/lec2.html#reinforcement-learning-in-economics-.smaller",
    "href": "docs/lec2.html#reinforcement-learning-in-economics-.smaller",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Reinforcement Learning in Economics {.smaller}",
    "text": "Reinforcement Learning in Economics {.smaller}\n\n\n\n\n\n\nDefinition\n\n\nAn agent learns optimal economic policies through trial and error, receiving rewards for good decisions and penalties for bad ones\n\n\n\n\n\n🎯 Key Economic Concepts\n\nAgent: Central bank, trader, firm\nEnvironment: Economic system, market\nActions: Policy decisions, trades, investments\n\nRewards: Economic outcomes, profits, welfare\n\n\n🔄 Learning Process\n\nTake economic action\nObserve market response\n\nReceive reward/penalty\nUpdate strategy\nRepeat until optimal\n\n\n\n💼 Economic Applications\n\n🏛️ Monetary Policy: Optimal interest rate setting\n📈 Algorithmic Trading: Automated investment strategies\n🏢 Supply Chain: Inventory and pricing optimization\n🎮 Auction Design: Optimal bidding strategies\n🌱 Climate Policy: Carbon pricing mechanisms\n\n\nReal Example: JPMorgan uses RL for optimal trade execution to minimize market impact\n\n\n\nReinforcement learning is particularly powerful for dynamic economic decision-making where the environment responds to actions."
  },
  {
    "objectID": "docs/lec2.html#rl-in-monetary-policy-.smaller",
    "href": "docs/lec2.html#rl-in-monetary-policy-.smaller",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "RL in Monetary Policy {.smaller}",
    "text": "RL in Monetary Policy {.smaller}\n\n🏛️ Central Bank Agent📊 Policy Learning Process🎯 Economic Outcomes\n\n\n\n\n\n\n\nflowchart TD\n    A[🏛️ Central Bank&lt;br/&gt;Agent] --&gt; B[📊 Economic State&lt;br/&gt;Inflation, GDP, Employment]\n    B --&gt; C[🎯 Policy Action&lt;br/&gt;Interest Rate Decision]\n    C --&gt; D[📈 Economic Response&lt;br/&gt;Market Reaction]\n    D --&gt; E[🏆 Reward&lt;br/&gt;Economic Stability]\n    E --&gt; A\n    \n    style A fill:#e1f5fe\n    style E fill:#e8f5e8\n\n\n\n\n\n\n\n\nState Variables: - Current inflation rate - GDP growth rate\n- Unemployment rate - Market volatility\nActions: - Raise interest rates (+0.25%) - Lower interest rates (-0.25%)\n- Keep rates unchanged\nReward Function:\nReward = -|inflation - target|² - |unemployment - natural_rate|² - |gdp_volatility|\n\n\n\n\nTraditional Rules: - Fixed policy responses - Limited adaptability - May miss complex interactions\nExample: Taylor Rule\nRate = neutral + 1.5×(inflation - target) + 0.5×GDP_gap\n\nRL-Based Policy: - Adaptive responses - Learns from experience - Captures non-linear relationships\nAdvantage: Can handle regime changes and structural breaks automatically"
  },
  {
    "objectID": "docs/lec2.html#real-world-economic-ml-applications-.smaller",
    "href": "docs/lec2.html#real-world-economic-ml-applications-.smaller",
    "title": "Machine Learning in Economics & Social Sciences",
    "section": "Real-World Economic ML Applications {.smaller}",
    "text": "Real-World Economic ML Applications {.smaller}\n\n🏦 Financial Services🏛️ Government & Policy🏢 Business Applications\n\n\nCredit Scoring & Risk Management\n\nBanks: Wells Fargo, JPMorgan use ML for loan decisions\nFeatures: Income, credit history, spending patterns, social data\nImpact: Reduced default rates by 15-20%\n\nAlgorithmic Trading\n\nFirms: Renaissance Technologies, Two Sigma\nTechniques: Deep RL, NLP for news analysis\nAssets: Over $1 trillion managed by ML algorithms\n\n\n\nEconomic Forecasting\n\nFed: Uses ML for GDP, inflation predictions\nIMF: Nowcasting economic growth using satellite data\nWorld Bank: Poverty mapping with mobile phone data\n\nTax Compliance\n\nIRS: ML for fraud detection (98% accuracy)\nEU: VAT gap prediction using transaction data\n\n\n\nDynamic Pricing\n\nAirlines: Real-time price optimization\nRide-sharing: Uber’s surge pricing algorithm\n\nE-commerce: Amazon’s recommendation engine\n\nSupply Chain\n\nWalmart: Demand forecasting with weather data\nManufacturing: Predictive maintenance\nLogistics: Route optimization"
  }
]