{
  "hash": "897a68104220bb3562e517d55f050ebd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Stock Market Prediction using Machine Learning\"\ntitle-banner-block: true\nauthor: \"Prof. Dr. Zahid Asghar\"\n\nformat: html\n\nexecute:\n  freeze: auto\n  warnings: false\n  message: false\n---\n\n\n\n## Introduction\n\nIn this tutorial, we will explore several machine learning classification models to predict the direction of stock movement for the **Pakistan Stock Exchange (PSX)** using historical data. We will use models such as **Logistic Regression**, **Linear Discriminant Analysis (LDA)**, **Quadratic Discriminant Analysis (QDA)**, **K-Nearest Neighbors (KNN)**, and **Decision Trees** (CART).\n\nThe goal is to understand some machine learning techniques and how they can be applied to real-world financial data. We will use historical stock data from the PSX. We can understand that predicting movements in stock prices is a challenging task due to the complex nature of financial markets. This is a simplified example to demonstrate how machine learning models can be used in real financial applications. We have created confusion matrix and calculated accuracy, precision, recall, and F1 score to evaluate the performance of each model. We\npredict whether the stock price will go \"Up\" or \"Down\" based on historical changes in stock price and trading volume. \n\n\n## 1. Loading and Preparing the Data\n\nThe first step is to load the PSX data, clean it, and perform necessary transformations for feature engineering.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)      # For model evaluation and cross-validation\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: ggplot2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: lattice\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(rpart)      # For Decision Trees\nlibrary(tidyverse)  # For data manipulation and visualization\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ purrr::lift()   masks caret::lift()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(Metrics)    # For additional evaluation metrics\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'Metrics' was built under R version 4.4.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'Metrics'\n\nThe following objects are masked from 'package:caret':\n\n    precision, recall\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(janitor)    # For cleaning column names\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(MASS)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(class)    # For KNN\n```\n:::\n\n\n\n\n## Load data\n\nWe have psx data in csv format. We will load the data and inspect its structure.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npsx <- read_csv(\"data/psx.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 1827 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Date, Vol., Change %\ndbl (4): Price, Open, High, Low\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Inspect the data structure (rows, columns, and types)\ndim(psx)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1827    7\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(psx)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,827\nColumns: 7\n$ Date       <chr> \"11/15/2024\", \"11/14/2024\", \"11/13/2024\", \"11/12/2024\", \"11…\n$ Price      <dbl> 17.42, 17.19, 16.78, 16.86, 17.02, 17.22, 17.14, 17.31, 16.…\n$ Open       <dbl> 17.40, 16.70, 16.95, 17.00, 17.46, 17.14, 17.24, 17.24, 17.…\n$ High       <dbl> 17.60, 17.60, 16.95, 17.20, 17.46, 17.49, 17.60, 17.50, 17.…\n$ Low        <dbl> 16.90, 16.55, 16.50, 16.70, 16.97, 17.01, 17.00, 16.60, 16.…\n$ Vol.       <chr> \"896.83K\", \"1.01M\", \"239.44K\", \"179.95K\", \"364.92K\", \"208.0…\n$ `Change %` <chr> \"1.34%\", \"2.44%\", \"-0.47%\", \"-0.94%\", \"-1.16%\", \"0.47%\", \"-…\n```\n\n\n:::\n:::\n\n\n\n## Data cleaning and feature engineering\n\nWe will clean the data, convert columns to appropriate types, create new features, and handle missing values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert the `Date` column to a proper Date format\npsx <- psx %>% \n  mutate(date = as.Date(Date, format = \"%m/%d/%Y\")) |> dplyr::select(-Date)\n# Clean column names to make them consistent and readable\npsx <- psx %>% clean_names()\n\n# Convert `change_percent` to numeric and create a new column `direction` for the stock movement (Up/Down)\npsx <- psx %>% \n  mutate(\n    change_percent = as.numeric(str_remove(change_percent, \"%\")),\n    direction = if_else(change_percent > 0, \"Up\", \"Down\")\n  )\n\n# Arrange data by ascending date to maintain temporal order\npsx <- psx %>% arrange(date)\n\n# Create lag variables for `change_percent` (lags of 1 to 5 days)\npsx <- psx %>% \n  mutate(\n    lag1 = lag(change_percent, 1),\n    lag2 = lag(change_percent, 2),\n    lag3 = lag(change_percent, 3),\n    lag4 = lag(change_percent, 4),\n    lag5 = lag(change_percent, 5)\n  )\n\n# Convert `vol` (volume) to numeric, handling the M and K suffixes\npsx <- psx %>% mutate(vol = as.numeric(str_replace_all(vol, c(\"M\" = \"e6\", \"K\" = \"e3\"))))\n\n# Drop rows with missing values (NA)\npsx_clean <- psx %>% drop_na()\n\n# Convert `direction` to numeric for binary classification (1 for \"Up\", 0 for \"Down\")\npsx_clean <- psx_clean %>%\n  mutate(direction = if_else(direction == \"Up\", 1, 0))\n```\n:::\n\n\n\nThere are 1822 rows in the cleaned dataset after data cleaning and feature engineering.\n\n\n**Explanation:** \n\n1. We loaded the dataset and cleaned column names. \n\n2. We transformed the `change_percent` column from character to numeric. \n\n3. We created a `direction` column to classify stock movements as \"Up\" or \"Down\". \n\n4. Lag features are created to provide past data (1-day lag to 5-day lag) as predictors for stock movement. \n\n5. We converted the volume (`vol`) column to numeric format for use in the model. \n\n6. Rows with missing values were dropped, and the target variable `direction` was converted to binary numeric values for classification.\n\n\n\n## 2. Logistic Regression (GLM)\n\nLogistic regression will model the probability that the stock will go \"Up\" based on the lag features and volume.\n\n\n\n::: {.cell evaluate='false'}\n\n```{.r .cell-code}\n# Fit logistic regression model\n# glm_fit <- glm(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol, \n#                data = psx_clean, family = binomial)\n# \n# # Model summary\n# summary(glm_fit)\n# \n# # Add predicted probabilities to the dataset\n# psx_clean <- psx_clean %>%\n#   mutate(\n#     glm_probs = predict(glm_fit, type = \"response\"),\n#     glm_pred = if_else(glm_probs > 0.5, 1, 0)\n#   )\n# \n# # Evaluate logistic regression performance using confusion matrix and accuracy\n# confusion_matrix <- table(glm_pred = psx_clean$glm_pred, actual = psx_clean$direction)\n# accuracy <- mean(psx_clean$glm_pred == psx_clean$direction)\n\n# Display results\n#print(confusion_matrix)\n#print(paste(\"Overall Accuracy:\", round(accuracy, 3)))\n```\n:::\n\n\n\n**Explanation:**\n1. The logistic regression model is fit with the lag features and volume as predictors.\n2. Predictions are made for each observation using `predict()`, and the predicted probabilities are added to the dataset.\n3. We then classify the probability as \"Up\" or \"Down\" based on a threshold of 0.5.\n4. The performance is evaluated by comparing predicted values against actual values using a confusion matrix and accuracy.\n\n---\n\n## 3. Train-Test Split\n\nWe split the data into **training** and **testing** datasets to evaluate the performance of our models.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train-test split based on the date\ntrain_data <- psx_clean %>%\n  filter(date < as.Date(\"2024-01-01\"))\n\ntest_data <- psx_clean %>%\n  filter(date >= as.Date(\"2024-01-01\"))\n# \n# # Ensure direction is numeric (0 for Down, 1 for Up)\n# train_data <- train_data %>%\n#   mutate(direction = as.factor(direction))\n# \n# test_data <- test_data %>%\n#   mutate(direction = as.factor(direction))\n# \n# # Ensure consistent factor levels for direction in both training and test data\n# train_data$direction <- factor(train_data$direction, levels = c(0, 1))  # 0 = Down, 1 = Up\n# test_data$direction <- factor(test_data$direction, levels = c(0, 1))\n```\n:::\n\n\n\n\n\n## Linear Discriminant Analysis (LDA)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlda_fit <- lda(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol,\n               data = train_data)\n\n# Predict on test data\n\nlda_pred <- predict(lda_fit, newdata = test_data)\n\n# Evaluate LDA model\n\nlda_accuracy <- mean(lda_pred$class == test_data$direction)\n```\n:::\n\n\n\n**Explanation:**\n1. We split the data based on the date (all data before January 1, 2024 for training, and the rest for testing).\n2. The target variable `direction` is converted into a factor to comply with classification model requirements.\n\n---\n\n## 4. Linear Discriminant Analysis (LDA)\n\nLDA is another classification technique that assumes normality in the predictor variables and tries to separate the classes (Up/Down) by maximizing the ratio of between-class variance to within-class variance.\n\n\n\nError in Ops.factor(lda_pred, test_data$direction) : \n  level sets of factors are different\n> \n\n\n\n**Explanation:**\n1. We fit the LDA model using the training data.\n2. Predictions are made on the test set using the fitted model.\n3. The model accuracy is evaluated by comparing predicted and actual values.\n\n---\n\n## 5. Quadratic Discriminant Analysis (QDA)\n\nQDA is similar to LDA but allows for each class to have its own covariance matrix, making it more flexible when the data distribution is not the same across classes.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit QDA model\nqda_fit <- qda(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol, data = train_data)\n\n# Predict on test data\nqda_pred <- predict(qda_fit, newdata = test_data)$class\n\n# Evaluate QDA model performance\nqda_accuracy <- mean(qda_pred == test_data$direction)\n\n# Display QDA results\nprint(paste(\"QDA Accuracy:\", round(qda_accuracy, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"QDA Accuracy: 0.605\"\n```\n\n\n:::\n:::\n\n\n\n**Explanation:**\n1. We fit the QDA model using the training data.\n2. Predictions are made on the test set, and the model’s accuracy is evaluated.\n\n---\n\n## 6. K-Nearest Neighbors (KNN)\n\nKNN is a non-parametric method that assigns the class of an observation based on the majority class of its k nearest neighbors in the feature space.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit KNN model\n\nknn_fit <- knn(train = train_data[, c(\"lag1\", \"lag2\", \"lag3\", \"lag4\", \"lag5\", \"vol\")],\n               test = test_data[, c(\"lag1\", \"lag2\", \"lag3\", \"lag4\", \"lag5\", \"vol\")],\n               cl = train_data$direction,\n               k = 5)\n\n# Evaluate KNN model\n\nknn_accuracy <- mean(knn_fit == test_data$direction)\n```\n:::\n\n\n\n**Explanation:**\n1. Data is normalized to ensure all features have the same scale for KNN.\n2. We fit the KNN model with `k = 5` and evaluate its accuracy.\n\n---\n\n## 7. Decision Trees (CART)\n\nThe Decision Tree algorithm creates a tree-like model of decisions and their possible consequences.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit Decision Tree model (CART)\ndt_fit <- rpart(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol,\n                 data = train_data,\n                 method = \"class\")\n\n# Predict on test data\n\ndt_pred <- predict(dt_fit, newdata = test_data, type = \"class\")\n\n\n# Evaluate Decision Tree model\n\ndt_accuracy <- mean(dt_pred == test_data$direction)\n```\n:::\n\n\n\n**Explanation:**\n1. We fit the Decision Tree (CART) model using the training data.\n2. Predictions are made on the test set, and the accuracy is calculated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train logistic regression on train data\nglm_train <- glm(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol,\n                 data = train_data, family = binomial)\n\n# Predict on train data\ntrain_data <- train_data %>%\n  mutate(\n    glm_probs = predict(glm_train, newdata = train_data, type = \"response\"),\n    glm_pred = if_else(glm_probs > 0.5, \"Up\", \"Down\")\n  )\n\n# Calculate confusion matrix and accuracy for train data\ntrain_confusion_matrix <- table(glm_pred = train_data$glm_pred, actual = train_data$direction)\nglm_train_accuracy <- mean(train_data$glm_pred == train_data$direction)\n\n# Predict on test data\ntest_data <- test_data %>%\n  mutate(\n    glm_probs = predict(glm_train, newdata = test_data, type = \"response\"),\n    glm_pred = if_else(glm_probs > 0.5, \"Up\", \"Down\")\n  )\n\n# Calculate confusion matrix and accuracy for test data\ntest_confusion_matrix <- table(glm_pred = test_data$glm_pred, actual = test_data$direction)\nglm_test_accuracy <- mean(test_data$glm_pred == test_data$direction)\n\n# Display results\nlist(\n  train_confusion_matrix = train_confusion_matrix,\n  glm_train_accuracy = glm_train_accuracy,\n  test_confusion_matrix = test_confusion_matrix,\n  glm_test_accuracy = glm_test_accuracy\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$train_confusion_matrix\n        actual\nglm_pred   0   1\n    Down 756 510\n    Up   119 222\n\n$glm_train_accuracy\n[1] 0\n\n$test_confusion_matrix\n        actual\nglm_pred  0  1\n    Down 69 37\n    Up   46 63\n\n$glm_test_accuracy\n[1] 0\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Logistic regression model\nglm_fit <- glm(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol,\n               data = psx_clean, family = binomial)\n\n# Model summary\nsummary(glm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + \n    vol, family = binomial, data = psx_clean)\n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -5.237e-01  6.095e-02  -8.592  < 2e-16 ***\nlag1        -6.414e-02  1.678e-02  -3.822 0.000132 ***\nlag2        -1.150e-02  1.549e-02  -0.742 0.458036    \nlag3        -4.223e-03  1.573e-02  -0.268 0.788338    \nlag4        -2.171e-02  1.515e-02  -1.433 0.151943    \nlag5        -4.423e-02  1.526e-02  -2.898 0.003758 ** \nvol          4.184e-07  4.901e-08   8.536  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2512.1  on 1821  degrees of freedom\nResidual deviance: 2396.3  on 1815  degrees of freedom\nAIC: 2410.3\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n\n```{.r .cell-code}\n# Predict probabilities and classify directions\npsx_clean1 <- psx_clean %>%\n  mutate(\n    glm_probs = predict(glm_fit, type = \"response\"),\n    glm_pred = if_else(glm_probs > 0.5, \"Up\", \"Down\")\n  )\n\n# Evaluate model accuracy\nconfusion_matrix <- table(glm_pred = psx_clean1$glm_pred, actual = psx_clean1$direction)\naccuracy <- mean(psx_clean1$glm_pred == psx_clean1$direction)\n\n# Split data into train and test sets (based on date < 2024)\ntrain <- psx_clean1 %>%\n  filter(date < as.Date(\"2024-01-01\"))\n\ntest <- psx_clean1 %>%\n  filter(date >= as.Date(\"2024-01-01\"))\n\n# Train logistic regression on train data\nglm_train <- glm(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + vol,\n                 data = train, family = binomial)\n\n# Predict on test data\ntest <- test %>%\n  mutate(\n    glm_probs = predict(glm_train, newdata = ., type = \"response\"),\n    glm_pred = if_else(glm_probs > 0.5, \"Up\", \"Down\")\n  )\n\n# Convert predictions and true labels to factors with the same levels\ntest$glm_pred <- factor(test$glm_pred, levels = c(\"Down\", \"Up\"))\ntest$direction <- factor(test$direction, levels = c(\"Down\", \"Up\"))\n\n# Evaluate test accuracy\nconfusion_matrix_test <- table(glm_pred = test$glm_pred, actual = test$direction)\ntest_accuracy <- mean(test$glm_pred == test$direction)\n\n# Output results\nlist(\n  train_accuracy = accuracy,\n  test_accuracy = test_accuracy,\n  train_confusion_matrix = confusion_matrix,\n  test_confusion_matrix = confusion_matrix_test\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$train_accuracy\n[1] 0\n\n$test_accuracy\n[1] NA\n\n$train_confusion_matrix\n        actual\nglm_pred   0   1\n    Down 848 562\n    Up   142 270\n\n$test_confusion_matrix\n        actual\nglm_pred Down Up\n    Down    0  0\n    Up      0  0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Confusion Matrix and Accuracy using caret\nglm_cm <- confusionMatrix(test$glm_pred, test$direction)\n\n# Extract Accuracy\nglm_accuracy <- glm_cm$overall['Accuracy']\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Confusion Matrix and Accuracy using caret\nglm_cm <- confusionMatrix(test$glm_pred, test$direction)\n\n# Extract Accuracy\nglm_accuracy <- glm_cm$overall['Accuracy']\n\nglm_accuracy\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAccuracy \n     NaN \n```\n\n\n:::\n:::\n\n\n\n\n\n## 8. Model Evaluation\n\nWe evaluate the performance of each model using accuracy, which is the proportion of correct predictions over the total number of predictions.\n\n\n## Output results\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist( glm_accuracy = glm_accuracy,\n  lda_accuracy = lda_accuracy,\n  qda_accuracy = qda_accuracy,\n  knn_accuracy = knn_accuracy,\n  dt_accuracy = dt_accuracy\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$glm_accuracy\nAccuracy \n     NaN \n\n$lda_accuracy\n[1] 0.6325581\n\n$qda_accuracy\n[1] 0.6046512\n\n$knn_accuracy\n[1] 0.5488372\n\n$dt_accuracy\n[1] 0.5813953\n```\n\n\n:::\n:::\n\n\n\n\n## Conclusion\n\nWe have successfully implemented various machine learning classification models to predict stock movement direction. The models were evaluated using accuracy, and further improvements can be made by tuning hyperparameters, adding more features, or using ensemble methods. \n\nEach model has its strengths and weaknesses, and the results from this tutorial can be used to guide further research or model enhancement.\n\n",
    "supporting": [
      "psx_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}