{
  "hash": "34de3d2f5186b246c44e369f91fbe4f3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"What is Statistical Learning?\"\nsubtitle: \"Chapter 02 ‚Äì Part I\"\ndate: today\nformat:\n  revealjs:\n    theme: [default, custom.scss]\n    slide-number: true\n    chalkboard: true\n    #multiplex: true\n    preview-links: auto\n    logo: \"\"\n    footer: \"ML in Economics | [Zahid Asghar](https://www.zahid.quarto.pub)\"\n    transition: slide\n    background-transition: fade\n    highlight-style: github\n    code-fold: true\n    code-tools: true\n    menu:\n      side: left\n      width: normal\n    css: custom.scss\nexecute:\n  echo: false\n  warning: false\n  freeze: auto \n  \n---\n\n## Outline\n* What is Statistical Learning?\n* Why estimate $f$?\n* How do we estimate $f$?\n* The trade-off: prediction accuracy vs. model interpretability\n* Supervised vs. unsupervised learning\n* Regression vs. classification\n\n---\n\n## What is Statistical Learning?\n\nThink of statistical learning like **learning to predict** based on patterns in data.\n\n**Simple Example**: Predicting house prices based on size, location, age.\n\nData: $(X_1, X_2, \\dots, X_p, Y)$ for $i = 1, \\dots, n$. \n  - $X_j$ are predictors (size, location, age of house)\n  - $Y$ is the response (house price)\n\n**Key Assumption**: House price depends on at least one feature (size matters!)\n\n**Model**: $Y = f(X) + \\varepsilon$ \n- $f$ = the \"true relationship\" we want to learn\n- $\\varepsilon$ = random noise (things we can't predict)\n\n> Statistical learning finds the best $f$ using data\n\n---\n\n## Why Do We Want to Estimate $f$?\n\n**Two main reasons:**\n\n::: {.incremental}\n1. **Prediction** üìà\n   - \"What will this house sell for?\"\n   - \"Will this customer buy our product?\"\n   - We don't care HOW the prediction works, just that it's accurate\n\n2. **Understanding** üîç  \n   - \"How does education affect wages?\"\n   - \"Which marketing channels work best?\"\n   - We want to understand the relationship itself\n:::\n\n**Real-world example**: A bank wants to predict loan defaults (prediction) AND understand what factors cause defaults (understanding)\n\n---\n\n## Example: Wage Data\n\n::: {.panel-tabset}\n\n## R Code\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ISLR2)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata(Wage)\nWage %>%\n  ggplot(aes(x = age, y = wage)) +\n  geom_point(alpha = 0.4, color = \"steelblue\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\") +\n  labs(title = \"Wage vs. Age\", x = \"Age\", y = \"Wage ($1000s)\") +\n  theme_minimal()\n```\n:::\n\n\n## Python Code\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import fetch_openml\n\n# Load wage data (or similar dataset)\nwage_data = pd.read_csv('wage_data.csv')\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=wage_data, x='age', y='wage', alpha=0.4)\nsns.loess(data=wage_data, x='age', y='wage', color='red')\nplt.title('Wage vs. Age')\nplt.xlabel('Age')\nplt.ylabel('Wage ($1000s)')\nplt.show()\n```\n\n## Interpretation\n**What we see**: \n- Wages increase with age until around 40-50\n- Then wages level off or slightly decrease\n- Lots of variation at each age (the \"noise\")\n\n**Business insight**: Companies should expect to pay higher wages to experienced workers, but the premium levels off\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ch02_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n---\n\n## How Noise Affects Learning\n\n**Key Insight**: More noise = harder to learn the pattern\n\nThink of it like listening to music with background static - more static makes it harder to hear the song.\n\n::: {.panel-tabset}\n\n## R Code\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nx <- seq(-2, 2, length.out = 100)\ny_true <- sin(pi * x)  # True relationship\ny_low <- y_true + rnorm(100, 0, 0.2)   # Low noise\ny_high <- y_true + rnorm(100, 0, 1)    # High noise\n\n# Create comparison plots\nlibrary(patchwork)\np1 <- ggplot(data.frame(x=x, y=y_low), aes(x, y)) + \n      geom_point(color=\"steelblue\") + \n      geom_line(aes(y=y_true), color=\"red\") +\n      ggtitle(\"Low Noise - Easy to Learn\")\n\np2 <- ggplot(data.frame(x=x, y=y_high), aes(x, y)) + \n      geom_point(color=\"steelblue\") + \n      geom_line(aes(y=y_true), color=\"red\") +\n      ggtitle(\"High Noise - Hard to Learn\")\n\np1 + p2\n```\n:::\n\n\n## Python Code\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(123)\nx = np.linspace(-2, 2, 100)\ny_true = np.sin(np.pi * x)\ny_low = y_true + np.random.normal(0, 0.2, 100)\ny_high = y_true + np.random.normal(0, 1, 100)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nax1.scatter(x, y_low, alpha=0.7, color='steelblue')\nax1.plot(x, y_true, color='red', linewidth=2)\nax1.set_title('Low Noise - Easy to Learn')\n\nax2.scatter(x, y_high, alpha=0.7, color='steelblue')  \nax2.plot(x, y_true, color='red', linewidth=2)\nax2.set_title('High Noise - Hard to Learn')\n\nplt.tight_layout()\nplt.show()\n```\n\n## Business Application\n**Real Example**: Predicting sales\n\n- **Low noise**: Luxury cars (few, predictable buyers)\n- **High noise**: Fast food (many random factors)\n\n**Strategy**: With high noise, collect MORE data or use simpler models\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ch02_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n---\n\n## Different Ways to Estimate $f$\n\n**Question**: Given data points, how do we draw the \"best\" line?\n\n**Answer**: Different methods give different results!\n\n::: {.panel-tabset}\n\n## R Code  \n\n::: {.cell}\n\n```{.r .cell-code}\n# Method 1: Linear regression (straight line)\nlinear_fit <- lm(y ~ x, data = df)\n\n# Method 2: Polynomial (curved line) \npoly_fit <- lm(y ~ poly(x, 3), data = df)\n\n# Method 3: LOESS (flexible curve)\ndf %>%\n  ggplot(aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"red\") +      # Linear\n  geom_smooth(method = \"loess\", color = \"blue\") +  # Flexible\n  labs(title = \"Different Methods, Different Results\")\n```\n:::\n\n\n## Python Code\n```python\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\n\n# Linear regression\nlinear = LinearRegression()\nlinear.fit(x.reshape(-1,1), y)\n\n# Polynomial regression  \npoly = Pipeline([\n    ('poly', PolynomialFeatures(degree=3)),\n    ('linear', LinearRegression())\n])\npoly.fit(x.reshape(-1,1), y)\n\n# Plot results\nplt.scatter(x, y)\nplt.plot(x, linear.predict(x.reshape(-1,1)), 'r-', label='Linear')\nplt.plot(x, poly.predict(x.reshape(-1,1)), 'b-', label='Polynomial')\nplt.legend()\nplt.show()\n```\n\n## Key Tradeoff\n**Simple models** (straight lines):\n- ‚úÖ Easy to understand and explain\n- ‚ùå Might miss important patterns\n\n**Complex models** (wiggly curves):\n- ‚úÖ Capture complex patterns  \n- ‚ùå Hard to interpret, might overfit\n\n**Rule of thumb**: Start simple, add complexity only if needed\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ch02_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n---\n\n## Real Example: Boston Housing Prices\n\n**Business Question**: How much should we price this house?\n\n::: {.panel-tabset}\n\n## R Code\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\ndata(Boston)\n\n# Simple model: Price depends on poverty level and room count\nlm_fit <- Boston %>%\n  lm(medv ~ lstat + rm, data = .)\n\n# Look at results\nsummary(lm_fit)\n\n# Interpretation:\n# - Each extra room adds ~$9,000 to house value\n# - 1% increase in poverty rate reduces value by ~$950\n```\n:::\n\n\n## Python Code  \n```python\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_boston\n\n# Load Boston housing data\nboston = load_boston()\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf['price'] = boston.target\n\n# Fit model: Price = f(poverty_rate, avg_rooms)\nmodel = LinearRegression()\nX = df[['LSTAT', 'RM']]  # poverty %, avg rooms\ny = df['price']\n\nmodel.fit(X, y)\n\n# Print coefficients\nprint(f\"Room effect: ${model.coef_[1]:.2f}k per room\")\nprint(f\"Poverty effect: -${abs(model.coef_[0]):.2f}k per 1% increase\")\n```\n\n## Business Insights\n**Actionable findings**:\n\n1. **Room count matters**: Each extra room = ~$9K more value\n   - Strategy: Highlight room count in listings\n   \n2. **Location matters**: High-poverty areas have lower prices  \n   - Strategy: Different pricing strategies by neighborhood\n   \n3. **Model fit**: R¬≤ ‚âà 0.67 means we explain 67% of price variation\n   - Still missing some important factors (schools, transportation?)\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: What Affects Boston Housing Prices?\n\n|Factor                  | Effect ($1000s)| Std Error|      t-stat|p-value |\n|:-----------------------|---------------:|---------:|-----------:|:-------|\n|Baseline Price          |           -1.36| 3.1728278|  -0.4280953|0.669   |\n|Poverty Rate Effect (%) |           -0.64| 0.0437315| -14.6886992|< 0.001 |\n|Additional Room Effect  |            5.09| 0.4444655|  11.4627299|< 0.001 |\n\n\n:::\n:::\n\n\n---\n\n## Visualizing Housing Price Relationships\n\n**Key insight**: Relationships aren't always linear!\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ch02_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n**Business Implication**: The poverty-price relationship is clearly non-linear! Linear models miss this pattern.\n\n---\n\n## Supervised vs. Unsupervised Learning {.scrollable}\n\n::: {.columns}\n\n::: {.column width=\"50%\"}\n### Supervised üë®‚Äçüè´\n**You have the \"answer key\"**\n\nExamples:\n- Predict house prices (have past sales)\n- Classify emails as spam (have labeled examples)\n- Forecast sales (have historical data)\n\n**Goal**: Learn from examples to predict new cases\n:::\n\n::: {.column width=\"50%\"}\n### Unsupervised üîç  \n**No \"answer key\" - find hidden patterns**\n\nExamples:\n- Group customers by behavior\n- Find market segments\n- Detect unusual transactions\n\n**Goal**: Discover structure in data\n:::\n\n:::\n\n::: {.panel-tabset}\n\n## R Code\n\n::: {.cell}\n\n```{.r .cell-code}\n# Unsupervised: Find customer segments\nset.seed(2)\ncustomer_data <- tibble(\n  spending = c(rnorm(50, 30, 10), rnorm(50, 80, 15)),\n  frequency = c(rnorm(50, 2, 1), rnorm(50, 8, 2))\n)\n\n# K-means clustering (finds groups automatically)\nsegments <- kmeans(customer_data, centers = 2)\ncustomer_data$segment <- segments$cluster\n\n# Plot results  \nggplot(customer_data, aes(spending, frequency, color = factor(segment))) +\n  geom_point(size = 3) +\n  labs(title = \"Customer Segments Found Automatically\")\n```\n:::\n\n\n## Python Code\n```python\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Generate customer data\nspending = np.concatenate([np.random.normal(30, 10, 50), \n                          np.random.normal(80, 15, 50)])\nfrequency = np.concatenate([np.random.normal(2, 1, 50),\n                           np.random.normal(8, 2, 50)])\n\n# Find segments\nkmeans = KMeans(n_clusters=2)\nsegments = kmeans.fit_predict(np.column_stack([spending, frequency]))\n\n# Plot\nplt.scatter(spending, frequency, c=segments, cmap='viridis')\nplt.xlabel('Average Spending')\nplt.ylabel('Purchase Frequency') \nplt.title('Customer Segments Found Automatically')\nplt.show()\n```\n\n## Business Value\n**Unsupervised learning reveals**:\n\n1. **Customer segments** ‚Üí Targeted marketing\n2. **Fraud patterns** ‚Üí Risk management  \n3. **Market structure** ‚Üí Competitive strategy\n\n**Key insight**: Sometimes the most valuable discoveries come from data exploration, not prediction!\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ch02_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n---\n\n## Regression vs. Classification {.scrollable}\n\n**The key difference**: What type of outcome are you predicting?\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### Regression üìà\n**Predicting numbers**\n- House prices: $150K, $200K, $175K\n- Sales volume: 1,000 units, 1,500 units  \n- Temperature: 25¬∞C, 30¬∞C, 22¬∞C\n- Stock returns: +5%, -2%, +8%\n:::\n\n::: {.column width=\"50%\"}\n### Classification üè∑Ô∏è\n**Predicting categories**\n- Email: Spam or Not Spam\n- Loan decision: Approve or Reject  \n- Customer: High/Medium/Low Value\n- Medical diagnosis: Disease A, B, or Healthy\n:::\n:::\n\n::: {.panel-tabset}\n\n## R Code\n\n::: {.cell}\n\n```{.r .cell-code}\n# Classification example: High wage prediction\nWage <- Wage %>%\n  mutate(high_wage = ifelse(wage > 250, \"High\", \"Low\"))\n\n# Logistic regression for classification\nglm_fit <- glm(high_wage == \"High\" ~ age + education, \n               data = Wage, family = binomial)\n\n# Get probabilities instead of just yes/no\npred_prob <- predict(glm_fit, type = \"response\")\n```\n:::\n\n\n## Python Code  \n```python\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create binary outcome\nwage_data['high_wage'] = (wage_data['wage'] > 250).astype(int)\n\n# Prepare data\nle = LabelEncoder() \nX = pd.DataFrame({\n    'age': wage_data['age'],\n    'education': le.fit_transform(wage_data['education'])\n})\ny = wage_data['high_wage']\n\n# Fit classification model\nclf = LogisticRegression()\nclf.fit(X, y)\n\n# Get probabilities\nprobabilities = clf.predict_proba(X)[:, 1]  # Prob of high wage\n```\n\n## Business Applications\n**Classification is everywhere**:\n\n- **Banking**: Loan approval (binary)\n- **Marketing**: Customer value (low/medium/high)  \n- **HR**: Resume screening (hire/reject)\n- **E-commerce**: Product categories\n\n**Key insight**: Classification gives you actionable categories, regression gives you precise numbers\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ch02_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n---\n\n## The Big Picture: Prediction vs. Interpretation Trade-off {.scrollable}\n\n**Central tension in ML**: Accurate models are often hard to explain\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### High Interpretability üîç\n**Simple models you can explain**\n- Linear regression\n- Decision trees (small ones)\n- Simple rules\n\n**When to use**:\n- Regulatory requirements\n- Medical decisions  \n- Policy recommendations\n- Building trust\n:::\n\n::: {.column width=\"50%\"}\n### High Prediction Accuracy üìà\n**Complex \"black box\" models**\n- Deep neural networks\n- Random forests (large)\n- Ensemble methods\n\n**When to use**:\n- Image recognition\n- Recommendation systems\n- High-stakes prediction\n- When accuracy is paramount\n:::\n:::\n\n**Real-world examples**:\n- **Medical diagnosis**: Need interpretable models (doctor must explain)\n- **Netflix recommendations**: Black box is fine (just works)\n- **Credit scoring**: Regulated industry needs interpretable models\n- **Fraud detection**: Accuracy matters more than explanation\n\n---\n\n## Key Takeaways for Economics & Business Students\n\n::: {.incremental}\n1. **Start with the business question**: Do you need prediction or understanding?\n\n2. **Simple models first**: Linear regression beats fancy algorithms if you need interpretation\n\n3. **Data quality matters more than algorithm choice**: Clean data + simple model > messy data + complex model\n\n4. **Context is king**: A 90% accurate model is useless if you can't act on it\n\n5. **Validate everything**: Your model is only as good as its performance on NEW data\n:::\n\n**Next steps**: Learn to evaluate model performance and avoid common pitfalls\n\n---\n\n## What's Coming Next\n\nIn future lectures, we'll cover:\n\n::: {.incremental}\n- **Model Assessment**: How do we know if our model is good?\n- **Bias-Variance Trade-off**: Why simple models sometimes win\n- **Cross-validation**: Testing models the right way  \n- **Overfitting**: When models memorize instead of learn\n- **Resampling Methods**: Making the most of limited data\n:::\n\n**Homework preview**: Apply these concepts to a real dataset from your country/region\n\n---\n\n## End\n\n**Remember**: Machine learning is a tool, not magic. Success comes from: \n\n- Understanding your business problem \n\n- Choosing the right approach  \n\n- Validating your results \n\n- Communicating findings clearly\n\n*Questions?* ü§î\n",
    "supporting": [
      "ch02_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}