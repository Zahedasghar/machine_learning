# Combine cross-validation results into a summary data frame
results_train <- data.frame(
Method = c("LOOCV", "5-Fold CV", "10-Fold CV"),
Training_Accuracy = c(
model_loocv$results$Accuracy[1],
model_5fold$results$Accuracy[1],
model_10fold$results$Accuracy[1]
)
)
print(results_train)
# Train-test split based on date
train_data <- psx_clean %>% filter(date < as.Date("2024-01-01"))
test_data <- psx_clean %>% filter(date >= as.Date("2024-01-01"))
# Predictions and accuracy on the test set
loocv_preds <- predict(model_loocv, newdata = test_data)
loocv_test_accuracy <- mean(loocv_preds == test_data$direction)
fold5_preds <- predict(model_5fold, newdata = test_data)
fold5_test_accuracy <- mean(fold5_preds == test_data$direction)
fold10_preds <- predict(model_10fold, newdata = test_data)
fold10_test_accuracy <- mean(fold10_preds == test_data$direction)
# Summarize training and testing accuracies
results <- data.frame(
Method = c("LOOCV", "5-Fold CV", "10-Fold CV"),
Training_Accuracy = results_train$Training_Accuracy,
Test_Accuracy = c(loocv_test_accuracy, fold5_test_accuracy, fold10_test_accuracy)
)
print(results)
# Confusion matrices for the test set
confusionMatrix(data = loocv_preds, reference = test_data$direction, positive = "Up")
confusionMatrix(data = fold5_preds, reference = test_data$direction, positive = "Up")
confusionMatrix(data = fold10_preds, reference = test_data$direction, positive = "Up")
#install.packages("ISLR")  # Contains datasets used in "Introduction to Statistical Learning"
#install.packages("boot")  # Bootstrapping functions
library(ISLR2)
library(boot)
data("Auto")
mean_mpg <- function(data, indices) {
sampled_data <- data[indices]  # Resample using indices
return(mean(sampled_data))
}
mean_mpg
boot_result <- boot(data = Auto$mpg, statistic = mean_mpg, R = 10)  # 1000 bootstrap samples
print(boot_result)
mean(mpg)
mean(Auto$mpg)  # Original mean
boot_result <- boot(data = Auto$mpg, statistic = mean_mpg, R = 1)  # 1000 bootstrap samples
print(boot_result)
mean(Auto$mpg)  # Original mean
print(boot_result)
boot_result <- boot(data = Auto$mpg, statistic = mean_mpg, R = 2)  # 1000 bootstrap samples
print(boot_result)
boot(data = Auto$mpg, R = 10)
boot_result <- boot(data = Auto$mpg, statistic = mean_mpg, R = 20)  # 1000 bootstrap samples
print(boot_result)
boot_result <- boot(data = Auto$mpg, statistic = mean_mpg, R = 200)  # 1000 bootstrap samples
print(boot_result)
boot_result <- boot(data = Auto$mpg, statistic = mean_mpg, R = 1000)  # 1000 bootstrap samples
print(boot_result)
sample(Auto$mpg, 10, replace = TRUE)  # Example of resampling (with replacement)
sample(Auto$mpg, 10, replace = TRUE)  # Example of resampling (with replacement)
sample(Auto$mpg, 10, replace = TRUE)  # Example of resampling (with replacement)
sample(Auto$mpg, 10, replace = TRUE)  # Example of resampling (with replacement)
boot_result <- boot(data = Auto$mpg, statistic = mean_mpg, R = 1000)  # 1000 bootstrap samples
print(boot_result)
plot(boot_result)
sample(c(Auto$mpg,Auto$horsepower) , 10, replace = TRUE)
sample(c(Auto$mpg,Auto$horsepower) , 10, replace = TRUE)
fit <- lm(mpg ~ horsepower, data = sampled_data)
# Define a statistic to bootstrap coefficients of a linear model
boot_fn <- function(data, indices) {
sampled_data <- data[indices, ]
fit <- lm(mpg ~ horsepower, data = sampled_data)
return(coef(fit))
}
# Define a statistic to bootstrap coefficients of a linear model
boot_fn <- function(data, indices) {
sampled_data <- data[indices, ]
fit <- lm(mpg ~ horsepower, data = sampled_data)
return(coef(fit))
}
# Perform bootstrapping
set.seed(123)
boot_reg <- boot(data = Auto, statistic = boot_fn, R = 1000)
print(boot_reg)
lm(mpg ~ horsepower, data = Auto)  # Original regression coefficients
print(boot_reg)
boot.ci(boot.out = boot_result, type = c("norm", "basic", "perc", "bca"))
# Example dataset
data <- mtcars$mpg  # Using the `mpg` column from the mtcars dataset
# Bootstrap sampling
set.seed(123)  # For reproducibility
bootstrap_sample <- sample(data, size = 10, replace = TRUE)
# Print the bootstrap sample
print(bootstrap_sample)
print(boot_reg)
bootstrap_sample <- sample(data, size = 10, replace = TRUE)
# Print the bootstrap sample
print(bootstrap_sample)
bootstrap_sample <- sample(data, size = 10, replace = TRUE)
# Print the bootstrap sample
print(bootstrap_sample)
# Example dataset
data <- mtcars$mpg  # Using the `mpg` column from the mtcars dataset
# Bootstrap sampling
# For reproducibility
bootstrap_sample <- sample(data, size = 10, replace = TRUE)
# Print the bootstrap sample
print(bootstrap_sample)
# Bootstrap sampling
# For reproducibility
bootstrap_sample <- sample(data, size = 10, replace = TRUE)
# Print the bootstrap sample
print(bootstrap_sample)
library(ISLR)
set.seed(1)
train=sample(392,196)
lm.fit=lm(mpg~horsepower,data=Auto,subset=train)
attach(Auto)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
glm.fit=glm(mpg~horsepower,data=Auto)
cv.err=cv.glm(Auto,glm.fit)
cv.err$delta
cv.err=rep(0,5)
library(boot)
glm.fit=glm(mpg~horsepower,data=Auto)
cv.err=cv.glm(Auto,glm.fit)
cv.err$delta
cv.err=rep(0,5)
for(i in 1:5)
{glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error[i]=cv.glm(Auto,glm.fit$delta[1]}
cv.error[i]=cv.glm(Auto,glm.fit$delta[1]}
cv.err=rep(0,5)
for(i in 1:5)
for(i in 1:5){glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error[i]=cv.glm(Auto,glm.fit$delta[1]}
for(i in 1:5){glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error
library(boot)  # Ensure the boot library is loaded
cv.err <- rep(0, 5)  # Initialize a vector to store CV errors for degrees 1 to 5
cv.err <- rep(0, 5)  # Initialize a vector to store CV errors for degrees 1 to 5
for (i in 1:5) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)  # Fit polynomial model
cv.err[i] <- cv.glm(Auto, glm.fit)$delta[1]  # Store the CV error for degree i
}
print(cv.err)  # Display the cross-validation errors
1+2
# Load necessary libraries
library(tidyverse)
library(leaps)
library(glmnet)
library(pls)
# Load the dataset
data(Hitters, package = "ISLR")
# Data inspection
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
# Remove missing values using tidyverse approach
Hitters <- Hitters %>% drop_na(Salary)
dim(Hitters)
sum(is.na(Hitters))
# Best Subset Selection using leaps
regfit_full <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
reg_summary <- summary(regfit_full)
# Access and plot model results
reg_summary$rsq
ggplot(data.frame(NumVars = 1:19, RSS = reg_summary$rss), aes(x = NumVars, y = RSS)) +
geom_line() +
labs(x = "Number of Variables", y = "RSS")
# Plot Adjusted R^2 and highlight the best model
best_adj_r2 <- which.max(reg_summary$adjr2)
ggplot(data.frame(NumVars = 1:19, AdjR2 = reg_summary$adjr2), aes(x = NumVars, y = AdjR2)) +
geom_line() +
geom_point(aes(x = best_adj_r2, y = reg_summary$adjr2[best_adj_r2]), color = "red", size = 3)
# Forward and Backward Stepwise Selection
regfit_fwd <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = "forward")
regfit_bwd <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = "backward")
# Comparing coefficients of models with 7 variables
coef(regfit_full, 7)
coef(regfit_fwd, 7)
coef(regfit_bwd, 7)
# Model Selection with Cross-Validation
set.seed(1)
train_index <- sample(seq_len(nrow(Hitters)), size = 0.7 * nrow(Hitters))
train_data <- Hitters[train_index, ]
test_data <- Hitters[-train_index, ]
regfit_best <- regsubsets(Salary ~ ., data = train_data, nvmax = 19)
test_mat <- model.matrix(Salary ~ ., data = test_data)
val_errors <- map_dbl(1:19, ~{
coefi <- coef(regfit_best, id = .x)
pred <- test_mat[, names(coefi)] %*% coefi
mean((test_data$Salary - pred)^2)
})
best_model <- which.min(val_errors)
best_model
coef(regfit_best, best_model)
# Ridge Regression
x <- model.matrix(Salary ~ ., Hitters)[, -1]
y <- Hitters$Salary
ridge_mod <- glmnet(x, y, alpha = 0, lambda = 10^seq(10, -2, length = 100))
cv_ridge <- cv.glmnet(x[train_index,], y[train_index], alpha = 0)
best_lambda_ridge <- cv_ridge$lambda.min
ridge_pred <- predict(ridge_mod, s = best_lambda_ridge, newx = x[-train_index,])
mean((ridge_pred - y[-train_index])^2)
# The Lasso
lasso_mod <- glmnet(x[train_index,], y[train_index], alpha = 1)
cv_lasso <- cv.glmnet(x[train_index,], y[train_index], alpha = 1)
best_lambda_lasso <- cv_lasso$lambda.min
lasso_pred <- predict(lasso_mod, s = best_lambda_lasso, newx = x[-train_index,])
mean((lasso_pred - y[-train_index])^2)
# Principal Components Regression
set.seed(2)
pcr_fit <- pcr(Salary ~ ., data = train_data, scale = TRUE, validation = "CV")
summary(pcr_fit)
validationplot(pcr_fit, val.type = "MSEP")
# Partial Least Squares
set.seed(1)
pls_fit <- plsr(Salary ~ ., data = train_data, scale = TRUE, validation = "CV")
summary(pls_fit)
validationplot(pls_fit, val.type = "MSEP")
# Load necessary libraries
library(tidyverse)
library(leaps)
??leaps
library(glmnet)
library(pls)
# Load the dataset
data(Hitters, package = "ISLR")
# Data inspection
names(Hitters)
# Data inspection
names(Hitters)
Hitters |> select(1:4)
Hitters |> select(Division, Errors, Assists)
dim(Hitters)
sum(is.na(Hitters$Salary))
# Remove missing values using tidyverse approach
Hitters <- Hitters %>% drop_na(Salary)
dim(Hitters)
sum(is.na(Hitters))
# Best Subset Selection using leaps
regfit_full <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
reg_summary <- summary(regfit_full)
# Access and plot model results
reg_summary$rsq
ggplot(data.frame(NumVars = 1:19, RSS = reg_summary$rss), aes(x = NumVars, y = RSS)) +
geom_line() +
labs(x = "Number of Variables", y = "RSS")
# Plot Adjusted R^2 and highlight the best model
best_adj_r2 <- which.max(reg_summary$adjr2)
ggplot(data.frame(NumVars = 1:19, AdjR2 = reg_summary$adjr2), aes(x = NumVars, y = AdjR2)) +
geom_line() +
geom_point(aes(x = best_adj_r2, y = reg_summary$adjr2[best_adj_r2]), color = "red", size = 3)
# Forward and Backward Stepwise Selection
regfit_fwd <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = "forward")
regfit_bwd <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = "backward")
# Comparing coefficients of models with 7 variables
coef(regfit_full, 7)
coef(regfit_fwd, 7)
coef(regfit_bwd, 7)
coef(regfit_fwd, 2)
coef(regfit_fwd, 7)
# Model Selection with Cross-Validation
set.seed(1)
train_index <- sample(seq_len(nrow(Hitters)), size = 0.7 * nrow(Hitters))
train_data <- Hitters[train_index, ]
test_data <- Hitters[-train_index, ]
regfit_best <- regsubsets(Salary ~ ., data = train_data, nvmax = 19)
test_mat <- model.matrix(Salary ~ ., data = test_data)
val_errors <- map_dbl(1:19, ~{
coefi <- coef(regfit_best, id = .x)
pred <- test_mat[, names(coefi)] %*% coefi
mean((test_data$Salary - pred)^2)
})
best_model <- which.min(val_errors)
best_model
coef(regfit_best, best_model)
# Ridge Regression
x <- model.matrix(Salary ~ ., Hitters)[, -1]
y <- Hitters$Salary
ridge_mod <- glmnet(x, y, alpha = 0, lambda = 10^seq(10, -2, length = 100))
cv_ridge <- cv.glmnet(x[train_index,], y[train_index], alpha = 0)
best_lambda_ridge <- cv_ridge$lambda.min
ridge_pred <- predict(ridge_mod, s = best_lambda_ridge, newx = x[-train_index,])
mean((ridge_pred - y[-train_index])^2)
# The Lasso
lasso_mod <- glmnet(x[train_index,], y[train_index], alpha = 1)
cv_lasso <- cv.glmnet(x[train_index,], y[train_index], alpha = 1)
best_lambda_lasso <- cv_lasso$lambda.min
lasso_pred <- predict(lasso_mod, s = best_lambda_lasso, newx = x[-train_index,])
mean((lasso_pred - y[-train_index])^2)
# Principal Components Regression
set.seed(2)
pcr_fit <- pcr(Salary ~ ., data = train_data, scale = TRUE, validation = "CV")
summary(pcr_fit)
validationplot(pcr_fit, val.type = "MSEP")
library(ISLR2)
data("College")
college<-College
attach(college)
ggplot(college)+aes(Private,Outstate,color="red")+geom_boxplot()
library(tidyverse)
ggplot(college)+aes(Private,Outstate,color="red")+geom_boxplot()
Elite<-rep("No",nrow(college))
Elite[college$Top10perc>50]="Yes"
Elite=as.factor(Elite)
college<-data.frame(college,Elite)
summary(Elite)
ggplot(college)+aes(Elite,Outstate,col="red")+geom_boxplot()
par(mfrow=c(2,2))
ggplot(college,aes(Top10perc,col="red"))+geom_histogram(bins = 10,fill="red")
ggplot(college,aes(Apps,col="red"))+geom_histogram(bins = 10,fill="green")
ggplot(college,aes(Personal,col="red"))+geom_histogram(bins = 10,fill="orange")
ggplot(college,aes(PhD,col="red"))+geom_histogram(bins = 10,fill="blue")
summary(PhD)
row.names(college[PhD>100,])
summary(Apps)
row.names(college[Apps>25000,])
detach(college)
# Load necessary libraries
library(rpart)
library(rpart.plot)
library(caret)
library(dplyr)
# Replace with your actual file path
hmda_data <- read_dta("data/hmda_aer.dta")
# Step 1: Load the HMDA data
library(haven)
hmdata <- read_dta("data/hmda_aer.dta")
hmda_data <- read_dta("data/hmda_aer.dta")
# Step 2: Explore the dataset
str(hmda_data)
summary(hmda_data)
head(hmda_data)
# Step 3: Data Cleaning and Preprocessing
# Convert target variable and relevant columns to appropriate types
hmda_data$loan_approved <- as.factor(hmda_data$loan_approved)
colnames(hmda_data)
hmda_data <- read_dta("data/hmda_sw.dta")
# Step 2: Explore the dataset
str(hmda_data)
summary(hmda_data)
head(hmda_data)
colnames(hmda_data)
library(tidyverse)
library(ISLR2)
library(rpart)
library(rpart.plot)
hd <- read_csv("https://github.com/Willschulman/Heart-Disease-Prediction-Using-a-Random-Forrest-Model-in-R/blob/main/heart.csv")
View(hd)
knitr::opts_chunk$set(echo = TRUE)
#remove(list = ls())
rticles::journals()
library(tidyr)
library(tidyverse)
library(rpart)
#install.packages("rpart.plot")
library(rpart.plot)
library(haven)
#pdhs_kr<- read_dta("C:/Users/hp/OneDrive - Higher Education Commission/Research Students/Afia/final variables dta.dta")
#pdhs_kr<-pdhs_kr %>% na.omit()
#tree <- rpart(stunting~., data = pdhs_kr, method = 'class')
#rpart.plot(fit, extra = 110)
#printcp(tree)
#rpart.plot(tree, cex=0.6,extra = 106)
#prp(tree, faclen = 0, cex = 0.8, extra = 1)
dt_ict<-read_dta("C:/Users/92300/Dropbox/Amna Noor/ICT_women.dta")
hies<-dt_ict %>% na.omit()
hies<-as_tibble(hies) %>% filter(province!=7|province!=8)
hies$educated<-as.integer(hies$educated)
hies$region<-as.integer(hies$region)
hies$employed<-as.integer(hies$employed)
hies
hies$province<-as.integer(hies$province)
summary(hies$province)
hies_kp<-hies%>% filter(province==1)
hies_punjab<- hies%>% filter(province==2)
hies_sind<- hies%>% filter(province==3)
hies_balochistan<- hies%>% filter(province==4)
tree<-rpart(cons_index~ict+region+educated+
employed+Boys+Girls+age_interval,data=hies,method='class',
cp=0.002)
prp(tree)
tree_balochistan<-rpart(cons_index~ict+region+educated+
employed+Boys+Girls+age_interval,data=hies_balochistan,method='class',
cp=0.002)
prp(tree_balochistan)
bal_cons<-rpart.plot(tree_balochistan,main= "Classification Tree for Balochistan Province",cex=0.5 ,extra=101)
install.packages("mall")
ollamar::pull("llama3.2")
library(mall)
??ollamar
library(ollamar)
ollamar::test_connection()
llm_use(
backend = "ollama",
model = "llama3.1:8b",
seed = 1337,
temperature = 0
)
library(mall)
llm_use(
backend = "ollama",
model = "llama3.1:8b",
seed = 1337,
temperature = 0
)
tib <- tibble::tibble(
id = 1:4,
adjective = c('painful', 'hungry', 'cheerful', 'exciting')
)
tib |>
llm_sentiment(col = adjective)
#install.packages("mall")
library(mall)
library(ollamar)
ollmar::pull_ollama('llama3.1:8b')
ollmar::pull_ollama('llama3.1')
ollmar::pull('llama3.1')
library(ollamar)
ollmar::pull('llama3.1')
ollamar::pull('llama3.1')
ollamar::pull('llama3.1')
llm_use(
backend = "ollama",
model = "llama3.1:8b",
seed = 1337,
temperature = 0
)
tib <- tibble::tibble(
id = 1:4,
adjective = c('painful', 'hungry', 'cheerful', 'exciting')
)
tib |>
llm_sentiment(col = adjective)
#install.packages("mall")
library(mall)
library(ollamar)
ollamar::pull('llama3.1')
ollamar::pull('llama3.1')
ollamar::pull('llama3.1')
Function to check if Ollama server is running
is_ollama_running <- function() {
con <- try(socketConnection("127.0.0.1", port = 11434, open = "r", timeout = 2), silent = TRUE)
if (inherits(con, "try-error")) {
return(FALSE)  # Server is not running
} else {
close(con)
return(TRUE)  # Server is running
}
}
#install.packages("mall")
library(mall)
library(ollamar)
ollamar::pull('llama3.1')
tib <- tibble::tibble(
id = 1:4,
adjective = c('painful', 'hungry', 'cheerful', 'exciting')
)
tib |>
llm_sentiment(col = adjective)
# Install required packages if not already installed
required_packages <- c("mall", "ollamar", "tibble")
for (pkg in required_packages) {
if (!requireNamespace(pkg, quietly = TRUE)) {
install.packages(pkg)
}
}
# Load libraries
library(mall)
library(ollamar)
library(tibble)
# Function to check if Ollama server is running
is_ollama_running <- function() {
con <- try(socketConnection("127.0.0.1", port = 11434, open = "r", timeout = 2), silent = TRUE)
if (inherits(con, "try-error")) {
return(FALSE)  # Server is not running
} else {
close(con)
return(TRUE)  # Server is running
}
}
# Start Ollama server if not running
if (!is_ollama_running()) {
message("Ollama server is not running. Trying to start it...")
# Start server (Unix-based systems)
if (.Platform$OS.type == "unix") {
system("ollama serve &", wait = FALSE)
}
# Start server (Windows)
else {
shell("start /B ollama serve", wait = FALSE)
}
Sys.sleep(5)  # Allow time for server to initialize
if (!is_ollama_running()) {
stop("Failed to start Ollama server. Please check installation and network settings.")
}
}
reviews |>
llm_sentiment(review)
library(mall)
data("reviews")
reviews |>
llm_sentiment(review)
library(ollamar)
ollamar::test_connection()
ollama version
